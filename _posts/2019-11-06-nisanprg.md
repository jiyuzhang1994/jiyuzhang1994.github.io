---
layout: default
permalink: /nisanprg/
title: Nisan's Pseudorandom Generator for Space-Bounded Computation
tags: Study
---

* [ROBP and Matrix Multiplication](#robp)
* [Preliminaries](#pre)  
* [Nisan's Pseudorandom Generator](#nisan)  
* [An Algorithm for Pseudorandom Matrix Approximation](#algo)

In this post we present Nisan's pseudorandom generator (PRG). In the next post, we will see how Saks and Zhou use this PRG to prove that $BPL\subseteq L^{3/2}$.  

## <a name="robp"></a> 1. ROBP and Matrix Multiplication

Recall that we are interested in the ROBP where each layer of it has the same set of nodes, and the set is of size $2^s$. Each node in a layer has $2^m$ out edges each labeled with a binary string of length $m$ and maps to a node in the next layer. We can visualize the computation on a ROBP as following:  we associate the transition from the $i$th layer to the $i+1$th layer with a $2^s\times 2^s$ transition matrix $M$ which is substochastic. Therefore each time the ROBP makes a transition, the previous product is multiplied with the same $M$. After $r$ times transition the resulting matrix is denoted as $M^r$. The entry $M^r_{i, j}$ is the probability that the ROBP starts at node $i$ and ends at node $j$ after $r$ transitions when we choose input uniformly at random. The randomness is taken over the possible inputs to this ROBP.  

Let's examine some more properties about the matrix $M$. For each entry in $M$, say $M_{i,j}$, we can associate it with a subset of strings in $$\{0,1\}^m$$, which means that the the $i$th node given one of this set of strings will transit to the $j$th node in the next layer. Moreover, the same string does not appear twice in the a row. and the set of all strings in a row of $M$ is exactly the set of all strings $$\{0,1\}^m$$.    

Recall our goal is to construct a pseudorandom generator for the above ROBP. This is equivalent to we want that the final matrix $M_{prg}^r$ obtained by feeding the ROBP with pseudorandom bits is "close" to the final matrix $M^r$ obtained by feeding with uniformly random bits.

With this in mind we now present some techniques we need in order to analyze Nisan's PRG (which we will present later).  


## 2. <a name="pre"></a> Preliminaries  

For convenience, we set $d = 2^s$. We then introduce more notations. Let $[d]$ be the set of all nodes in a layer and $$Q: [d]\times \{0,1\}^m\rightarrow [d]$$ be the transition function for two consecutive layers's computation. $$Q^r:[d]\times \{0,1\}^{m\cdot r}\rightarrow [d]$$ is the transition function for $r$ consecutive layers. Then we use $Q[x]$ to represent the behavior of $Q$ on $$x \in \{0,1\}^m$$, and the corresponding matrix is denoted as $M[x]$.  

Let $Q_{i,j}$ be the subset of strings in $$\{0,1\}^m$$ that maps node $i$ to node $j$ in the next layer. Similarly, let $Q^r_{i,j}$ be the subset of strings in $$\{0,1\}^{m\cdot r}$$ which can be viewed as paths from node $i$ in the first layer to node $j$ in the last layer. Clearly, $M^r_{i,j}=\frac{\lvert Q^r_{i,j} \rvert}{2^{mr}}$.  


**Definition 2.1** (Matrix Norm) Let $M$ be a $d \times d$ matrix, we use the matrix norm $$\lvert \lvert M\rvert \rvert$$ which is the largest row sum of $M$:  

$$\lvert \lvert M\rvert \rvert = \max_{i} \sum_j \lvert M_{i, j} \rvert$$  

The following are some standard facts:

**Proposition 2.2**  

For matrices $M, N \in R^{d}\times R^d$,

$$ \lvert \lvert M+N\rvert \rvert \leq \lvert \lvert M\rvert \rvert  + \lvert \lvert N\rvert \rvert$$  

$$ \lvert \lvert MN\rvert \rvert \leq \lvert \lvert M\rvert \rvert  \cdot  \lvert \lvert N\rvert \rvert$$  


**Proposition 2.3**  

If $M, M^\prime, N, N^\prime$ are substochastic matrices, then  

$$\lvert\lvert MM^\prime - NN^\prime \rvert\rvert \leq \lvert\lvert M-N\rvert\rvert + \lvert\lvert M^\prime- N^\prime\rvert\rvert$$  


**Definition 2.4** (Pairwise Independent Hashing Family) We call a family of hashing functions $H$ is pairwise independent if for any $x_1, x_2 \in A$ and $x_1 \neq x_2$, $y_1, y_2\in B$ where $$A, B\subseteq \{0,1\}^m$$, when $$h:\{0,1\}^m\rightarrow \{0,1\}^m$$ is chosen randomly from $H$, then  

$$\Pr_{h\in H}\left [\ h(x_1) = y_1 \land h(x_2) = y_2 \right ] = \frac{1}{\lvert B\rvert ^2}$$  

An explicit construction of such hash family is known to exist and can be encoded using $2m$ bits. Moreover, it can be computed using $O(m)$ space.

**Definition 2.5** Let $A, B\subseteq \{0,1\}^m$, $h:\{0,1\}^m \rightarrow \{0,1\}^m$. We say a hash function is *$\epsilon$-independent for $A$, $B$* if   

$$ \left \lvert \Pr_x \left[ x\in A \land h(x)\in B \right] - \frac{\lvert A\rvert \lvert B \rvert}{2^{2m}} \right \rvert < \epsilon$$

From now on we denote $\frac{\lvert A\rvert}{2^m}$ as $\alpha$ and $\frac{\lvert B\rvert}{2^m}$ as $\beta$.  

We now prove a mixing lemma which roughtly says that the functions from a pairwise independent hashing family are almost all $\epsilon$-independent.  

**Theorem 2.6** (Mixing Lemma) Fix any $A$ ,$B$, if $$h:\{0,1\}^m\rightarrow \{0,1\}^m$$ is chosen randomly from a pairwise independent hashing family $H$, then  

$$\Pr_{h\in H} \left[ \text{h is not } \epsilon \text{-independent} \right]\leq \frac{\alpha\beta/\epsilon^2}{2^m}$$  

**Proof.** 

For any $h$, let $X_i$ be the indicator variable that $h(x_i)\in B$, and $X= \sum_{i\in A}X_i$. $h$ is  $\epsilon$-independent iff $$\lvert\frac{X}{2^m}-\alpha\beta\rvert < \epsilon$$. Consider the expectation $E[\frac{X}{2^m}] = \frac{1}{2^m}E[X]$.  By linearity of expectation, $ E[\frac{X}{2^m}] = \frac{1}{2^m}\sum_{i\in A} E[X_i] =  \frac{1}{2^m}\cdot \beta \lvert A\rvert = \alpha\beta$. Since $X_i$'s are pairwise independent, by Chebyshev inequality we have:  

$$\Pr\left[ \lvert\frac{X}{2^m}-\alpha\beta\rvert \geq \epsilon \right ]\leq \frac{Var[X]}{2^{2m}\cdot \epsilon^2} \leq\frac{\sum_{i\in A}Var[X_i]}{2^{2m}\cdot \epsilon^2} \leq \frac{\lvert A\rvert\cdot \beta(1-\beta)}{2^{2m}\cdot \epsilon^2}\leq \frac{\alpha\beta}{2^m\cdot \epsilon^2}$$  


## <a name="prg"></a> 3. Nisan's Pseudorandom Generator  

Nisan's generator is constructed by using random hash functions from pairwise independent hash family.   
Consider the matrix $M^2[x_1,x_2]$ of $Q^2[x_1, x_2]$ where $x_1x_2$ is chosen uniformly at random, we first show that if we choose $h_1$ randomly from $H$, then $M^2(x_1, h_1(x_1)$ of is close to $M^2(x_1,x_2)$.  

**Lemma 3.1**   

$$\Pr \left[ \lvert\lvert M^2[x_1, h_1(x_1)] -M^2[x_1,x_2]\rvert\rvert \geq \epsilon \right ] \leq \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}$$  

**Proof.**  

Consider the $(i, j)$-th entry of $M^2[x_1, h_1(x_1)] -M^2(x_1,x_2)$,   

$$\begin{align*}
  \left\lvert M^2[x_1, h_1(x_1)]_{i,j} -M^2[x_1,x_2]_{i,j}\right\rvert &= \lvert \sum_p (M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - M[x_1]_{i,p}\cdot M[x_2]_{p, j})\rvert\\
  &\leq \sum_p \left\lvert M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - \alpha\beta \right\rvert\\
 \end{align*}$$

Inparticular, if $h$ is $\epsilon/d^2$-independent for all $Q_{i,p}$ and $Q_{p, j}$, then  

$$\sum_p \left\lvert M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - \alpha\beta \right\rvert < \epsilon/d$$  

and 


$$\lvert\lvert M^2[x_1, h_1(x_1)] -M^2[x_1,x_2]\rvert\rvert <\epsilon$$  

With Lemma 2.6, if we want this holds for all triples $(i, p, j)$, a union bound gives us that  

$$\Pr \left[ \lvert\lvert M^2[x_1, h_1(x_1)] -M^2(x_1,x_2)\rvert\rvert \geq \epsilon \right ] \leq \frac{\alpha\beta\cdot d^4 \cdot d^3}{2^m \epsilon^2} = \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}$$  

Now we have a lemma for the case $r=2$, we can do it by repeatedly squaring to prove a lemma for the case $r$. For the next steps, every time we pick a new hash function from the above family, we will generate our pseudorandom bits by compositing these picked hash functions. For example, when we pick $h_2$, then new pseudorandom bits are generated by concatenating $h_2(x)$ and $h_2(h_1(x))$. From now on we will denote the new matrix as $M^{2^r}[x; h_1,..\ldots h_r]$ and the matrix we feed with uniformly random bits $x_1, \ldots x_{2^r}$ is denoted simply as $M^{2^r}$. Let's do some more steps:

when $r=4$,  

$$\begin{align*}
  \lvert\lvert M^4[x, h_1, h_2]_{i,j} -M^4_{i,j}\rvert\rvert &= \lvert\lvert M^4[x, h_1, h_2]_{i,j}-(M^2[x, h_1])^2 + (M^2[x, h_1])^2 - M^4\rvert\rvert\\
  &\leq \lvert\lvert M^4[x, h_1, h_2]_{i,j}-(M^2[x, h_1])^2\rvert\rvert + \lvert\lvert (M^2[x, h_1])^2 - M^4\rvert\rvert\\
  &\leq \epsilon + 2\cdot \epsilon\\
  &\leq 3\epsilon
 \end{align*}$$

when $r=8$,  

$$\begin{align*}
  \lvert\lvert M^8[x, h_1, h_2, h_3]_{i,j} -M^8_{i,j}\rvert\rvert &= \lvert\lvert M^8[x, h_1, h_2, h_3]_{i,j}-(M^4[x, h_1, h_2])^2 + (M^4[x, h_1, h_2])^2 - M^8\rvert\rvert\\
  &\leq \epsilon + 2\cdot 3\epsilon\\
  &\leq 7\epsilon
 \end{align*}$$
 

The recursive relation for error is $\epsilon(r)=\epsilon + 2\cdot \epsilon(\log r)$.  In general, we have

$$\epsilon(r) = (r-1)\epsilon $$  

**Theorem 3.2** If we pick $h_1,\ldots ,h_{\log r}$ uniformly random from the pairwise independent hash family, then  

$$\Pr_{h_1,\ldots ,h_{\log r}}\left[\lvert\lvert M^r[h_1,\ldots ,h_{\log r}] - M^r\rvert\rvert \geq (r-1)\epsilon \right]\leq (\log r) \cdot \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}$$

With these we're ready to present Nisan's pseudorandom generator.  

**Definition 3.3** (*Pseudorandom Generator for Matirx Multiplication*) An $(m,r)$-generator $$G:\{0,1\}^m\rightarrow \{0,1\}^{mr}$$ is $\epsilon$-pseudorandom for $M^r$ if  

$$\lvert M^r[G(x)]_{i,j} - M^r[x_1 \ldots x_r]_{i,j}\rvert \leq \epsilon$$  

Nisan's generator is thus constructed by compositing these randomly chosen hash functions as we described above. For $$x\in \{0,1\}^m$$,  

$$G_{h_1,\ldots ,h_{\log r}}(x) = x,h_1(x), h_2(x), h_2(h_1(x)), \ldots, h_{\log r}(x),\ldots h_{\log r}(h_{\log r-1}(\ldots h_1(x)))$$  

**Theorem 3.4** (Nisan) For $d, m$ and $r<d$, there is an $(m,r)$-generator $$G:\{0,1\}^m\rightarrow \{0,1\}^{mr}$$ that is $1/d$-pseudorandom for $M^r$. Moreover, the generator takes a seed of length $O(m\log r)$ and can be computed in space $O(m\log r)$.  

**Proof.**  

In Theorem 3.2, let's set $\epsilon = 1/2dr$ and $m = \Theta(\log d) = C\log d$ for some large $C$. Therefore  

$$\lvert\lvert M^r[h_1,\ldots ,h_{\log r}] - M^r\rvert\rvert \leq 1/2d$$   

except with probability at most $\frac{d^9 r^2 \log r}{d^C} = O(1/d)$ for large $C$. While in the later case, the difference is bounded to be at most $1\cdot O(1/d) = 1/2d$.

Finally, we have that  

$$\left \lvert\Pr_{x, h_1, \ldots ,h_{\log r}}\left[ Q(i; G(x)) = j\right ]- \Pr_{x_1, \ldots , x_{2^r}}\left[ Q(i; x_1, \ldots , x_{2^r}) = j\right ]\right \lvert \leq 1/d$$


## <a name="algo"></a> 4. An Algorithm for Pseudorandom Matrix Approximation 

