---
layout: default
permalink: /nisanprg/
title: Nisan's Generator for Space-Bounded Computation
tags: Study
---

* [ROBP and Matrix Multiplication](#robp)
* [Techniques](#tech)

In this post we present Nisan's pseudorandom generator (PRG). In the next post, we will see how Saks and Zhou use this PRG to prove that $BPL\subseteq L^{3/2}$.  

## <a name="robp"></a> 1. ROBP and Matrix Multiplication

Recall that we are interested in the ROBP where each layer of it has the same set of nodes, and the set is of size $2^s$. Each node in a layer has $2^m$ out edges each labeled with a binary string of length $m$ and maps to a node in the next layer. We can visualize the computing process as following:  we associate the transition from the $i$th layer to the $j$th layer with a transition $2^s\times 2^s$ matrix $M$ which is substochastic. Therefore each time the ROBP makes a transition, the previous product is multiplied with a new $M$. After $r$ times transition the resulting matrix is denoted as $M^r$. The entry $M^r_{i, j}$ is the probability that the ROBP starts at node $i$ and ends at node $j$ after $r$ transitions. The randomness is taken over the possible input to this ROBP.  

Let's examine some more details about the matrix $M$. For each entry in $M$, say $M_{i,j}$, we can associate it with a subset of strings in $$\{0,1\}^m$$, which means that the the $i$th node given these strings will transit to the $j$th node in the next layer. The same string does not appear twice in the a row. The set of all strings in a row of $M$ is exactly the set of strings $$\{0,1\}^m$$.  

Recall our goal is to construct a pseudorandom generator for the above ROBP. This is equivalent to we want that the final matrix $M_{prg}^r$ obtained by feeding the ROBP with pseudorandom bits is close to the final matrix $M^r$ obtained by feeding with uniformly random bits.

With this in mind we now present some techniques we need in order to analyze Nisan's PRG (which we will present later).  


## 2. <a name="tech"></a> Techniques  

**Definition 2.1** (Matrix Norm) Let $M$ be a $2^s \times 2^s$ matrix, we use the matrix norm $$\lvert \lvert M\rvert \rvert$$ which is the largest row sum of $M$:  

$$\lvert \lvert M\rvert \rvert = \max_{i} \sum_j \lvert M_{i, j} \rvert$$  

The following are some standard facts:

**Proposition 2.2**  

$$ \lvert \lvert M+N\rvert \rvert \leq \lvert \lvert M\rvert \rvert  + \lvert \lvert N\rvert \rvert$$  

$$ \lvert \lvert MN\rvert \rvert \leq \lvert \lvert M\rvert \rvert  \cdot  \lvert \lvert N\rvert \rvert$$  


**Definition 2.3** (Pairwise Independent Hashing Family) We call a family of hashing functions $H$ is pairwise independent if for any $x_1, x_2 \in A$ and $x_1 \neq x_2$, $y_1, y_2\in B$  

$$\Pr_{h\in H}\left [\ h(x_1) = y_1 \land h(x_2) = y_2 \right ] = \frac{1}{\lvert B\rvert ^2}$$

**Definition 2.4** Let $A, B\subseteq \{0,1\}^m$, $h:\{0,1\}^m \rightarrow \{0,1\}^m$. We say a hash function is *$\epsilon$-independent for $A$, $B$* if   

$$ \left \lvert \Pr_x \left[ x\in A \land h(x)\in B \right] - \frac{\lvert A\rvert \lvert B \rvert}{2^{2m}} \right \rvert < \epsilon$$

From now on we denote $\frac{\lvert A\rvert}{2^m}$ as $\alpha$ and $\frac{\lvert B\rvert}{2^m}$ as $\beta$.  

We now prove a mixing lemma which roughtly says that the functions from a pairwise independent hashing family are almost all $\epsilon$-independent.  

**Thoerem 2.5** (Mixing Lemma) Fix any $A$ ,$B$, if $$h:\{0,1\}^m\rightarrow \{0,1\}^m$$ is chosen randomly from a pairwise independent hashing family $H$, then  

$$\Pr_{h\in H} \left[ \text{h is not } \epsilon \text{-independent} \right]\leq \frac{\alpha\beta/\epsilon^2}{2^m}$$  

**Proof.** 

For any $h$, let $X_i$ be the indicator variable that $h(x_i)\in B$, and $X= \sum_{i\in A}X_i$. $h$ is  $\epsilon$-independent iff $$\lvert\frac{X}{2^m}-\alpha\beta\rvert \leq \epsilon$$. Consider the expectation $E(\frac{X}{2^m}) = \frac{1}{2^m}E(X)$.  By linearity of expectation $ \frac{1}{2^m}E(X) = \frac{1}{2^m}\sum_{i\in A} E[X_i] =  \frac{1}{2^m}\cdot \beta \lvert A\rvert = \alpha\beta$. Since $X_i$'s are pairwise independent, by Chebyshev inequality we have:  

$$\Pr\left[ \lvert\frac{X}{2^m}-\alpha\beta\rvert \geq \epsilon \right ]\leq \frac{Var[X]}{2^{2m}\cdot \epsilon^2} \leq\frac{\sum_{i\in A}Var[X_i]}{2^{2m}\cdot \epsilon^2} \leq \frac{\lvert A\rvert\cdot \beta(1-\beta)}{2^{2m}\cdot \epsilon^2}\leq \frac{\alpha\beta}{2^m\cdot \epsilon^2}$$  

