---
layout: default
permalink: /glthm/
title: Goldreich Levin Theorem
tags: Cryptography
---

In this blog we study and present the proof of Goldreich-Levin theorem. The main material we use is the  course notes of Prof. Luca Trevisan (see references at the end).  

The Goldreich-Levin Theorem has several interpretations. We will first explain and prove it from cryptographic view. Then we will see in fact it can also be viewed as list-decoding of Hadamard code.

## Background
In this section we present necessary definitions and math inequalities we will be using. The first theorem is a variant of Markov inequality.  

**Theorem 1 (Markov Inequality, Variant)** Suppose $X$ is a random variable in $[0, 1]$ and $0<t<E[X]$, then  

$$Pr[X\geq t] \geq \frac{E[X]-t}{1-t}$$

**Proof:**  
 
Let $S$ denote the set of $x$ such that $X(x)\geq t$, then

$$\begin{align*} 
E[X] &= \sum_{x\in S}Pr(x)X(x) + \sum_{x\not\in S}Pr(x)X(x)\\ 
 &\leq \sum_{x\in S}Pr(x)\cdot 1 + \sum_{x\not\in S}Pr(x)\cdot t\\
 &= Pr(S) + t\cdot (1-Pr(S))\\
 &= (1-t)\cdot Pr(S) + t
\end{align*}$$ 

**Theorem 2 (Chernoff Bound)**  Suppose $X_1, \ldots, X_k$ are 0, 1 *i.i.d* random variables and $ X = \sum_i^{k} X_i$, then for any $0<\epsilon<1$:  

$$Pr[\ X>(1+\epsilon)E[X]\ ]<e^{-\frac{\epsilon^2}{3}\cdot E[X]}$$  

$$Pr[\ X<(1-\epsilon)E[X]\ ]<e^{-\frac{\epsilon^2}{3}\cdot E[X]}$$

**Definition 1 (Hard-Core Predicate)** A boolean function $$P: \{0, 1 \}^n \rightarrow \{0, 1\}$$ is $(t, \epsilon)$ - hard core for a permutation $$f: \{0,1\}^n \rightarrow \{0,1\}$$ if for every algorithm $A$ of complexity $t$  

$$\Pr_{x\sim \{0, 1\}^n}[A(f(x)) = P(x)]\leq \frac{1}{2}+\epsilon$$

Note: we use $$x\sim \{0, 1\}^n$$ to denote uniform distribution over $$\{0, 1\}^n$$


## Goldreich-Levin Theorem

We denote inner product modulo 2 using the following notation:  

$$\langle x,r\rangle = \sum_i x_i\cdot r_i\ mod\ 2$$


The Goldreich-Levin theorem says that a random XOR is hard-core for every one-way permutation. This means that if there is an efficient algorithm to predict $\langle x, r\rangle$, given $f(x)$ and $r$, then there is also an efficient algorithm to compute a pre-image of $f(x)$, so if $f(x)$ is a one-way function, we can invert it with noticeable probability.   

**Theorem 1 (Goldreich-Levin Theorem)** *Suppose $A$ is an algorithm of complexity $t$ such that*  

$$\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle] \geq \frac{1}{2} + \epsilon$$  

*Then there is an algorithm $A^\prime$ of complexity at most $O(t\epsilon^{-2}n^{O(1)})$ such that* 

$$\Pr_x[A^\prime(f(x)) = x] \geq \Omega(\epsilon)$$

We will first prove a weak Goldreich-Levin algorithm, which will later be used in our proof of GL theorem.

**Theorem 2 (Goldreich-Levin Algorithm Weak Version)** *Suppose there is a function $H$ such that, for some $x$*  

$$\Pr_r[H(r)\ =\ \langle x,r\rangle]\ \geq \frac{3}{4} + \epsilon$$

*Then there is an algorithm $GLW$ that can output $x$ with high probability ( $\geq 1-\frac{1}{n}$ ).*  

Let's first see how an easy algorithm can find $x$ when the probability on the right side is 1 in the above (so that $H(r)$ outputs $\langle x,r\rangle$ correctly).  

Let $e_i$ denote the vector in which the $i$th bit is 1 and 0 otherwise. $x_i = H(e_i) = \langle x, e_i \rangle$. Then we can output $x$ by enumerating all the $e_i$.   

Now to prove Theorem 2, note that $H(r)$ fails with small probability, we'll use the majority vote method. Intuitively,  we randomly sample several points $r_1 \ldots r_k$. Let $H^\prime_{r_1 \ldots r_k}(e_i)$ be the function that take the majority of  $H(r_i+e_i) - H(r_i)$ on these $r_i$s. The observation is that if $H(r)$ computes $\langle x, r\rangle$ correctly, then we have:

$$ \langle x, r_i+e_i \rangle - \langle x, r_i \rangle = \langle x, e_i\rangle$$

We now bound the probability that $H(r_i + e_i) - H(r_i)$ fails.  

$$\begin{align*}
  Pr[H(r_i + e_i) - H(r_i) \neq  \langle x, e_i\rangle] &= Pr[H(r_i + e_i) \neq \langle x, r_i+e_i \rangle\ \cup H(r_i) \neq \langle x, r_i \rangle]\\
  &\leq Pr[H(r_i + e_i) \neq \langle x, r_i+e_i \rangle]\ + Pr[H(r_i) \neq \langle x, r_i \rangle]\\
  &\leq \frac{1}{4} - \epsilon + \frac{1}{4} - \epsilon\\
  &\leq \frac{1}{2} - 2\epsilon
 \end{align*}$$
 
The GLW algorithm is as follows:
 
 * GLW Algorithm:
 * for $i= 1, 2, \ldots , n$:  
     * for $j = 1, 2, \ldots , 4\log n/ \epsilon^2$:  
     * randomly sample $$r_j\in \{0, 1\}^n$$
     * compute $H(r_j + e_i) - H(r_j)$
 * compute $$x_i = H^\prime_{r_1 \ldots r_k}(e_i) = Majority\{H(r_j + e_i) - H(r_j): r_j = 1, 2, \ldots , 4\log n/ \epsilon^2 \}$$  
 * return $x$  

Now we analyze this algorithm. Since each $H(r_j + e_i) - H(r_j)$ outputs the correct answer with probability at least $1/2 + 2\epsilon$. By the Chernoff Bound (Theorem 2 in Background), the probability that the majority function $H^\prime_{r_1 \ldots r_k}(e_i)$ fails to output $x_i$ is at most $e^{-2\log n} = O(\frac{1}{n^2})$. Then the union bound gives us that GLW outputs $x$ correctly with probability at least $1-1/n$. The algorithm runs in time $O(\frac{n^2 \log n}{\epsilon^2})$, makes $O(\frac{n\log n}{\epsilon^2})$ queries to $H$.

**Discussion.** The above framework only works for cases where $\Pr_r[H(r) = \langle x, r \rangle]$ greater than $\frac{3}{4}$. For the case in Goldreich-Levin theorem where $H(r)$ makes more error,the union bound gives probability less than $1/2$ and the Chernoff Bound won't work. In fact, it is possible to construct a function $H$ such that $Pr[H(r) = \langle x, r\rangle] = 3/4$ and $Pr[H(r) = \langle x^\prime, r\rangle] = 3/4$ where $x \neq x^\prime$. So no algorithm can guarantee to find the correct $x$ when given such $H$, since $x$ is not uniquely defined by $H$. However, the Goldreich-Levin theorem tells us how to find a list of possible candidates of $x$ with good probability. We present their algorithm below.    

We proceed to prove the Goldreich-Levin Theorem.

Given (as in the Goldreich-Levin theorem) $\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle] \geq \frac{1}{2} + \epsilon$, we first show that there is a certain fraction of $x$,  $\Pr_r[A(f(x), r) = \langle x, r \rangle] \geq \frac{1}{2} + \frac{1}{2}\epsilon$ holds, and we call such $x$ "good" :   

$$\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle]$$ can be viewed as $$\sum_x Pr(x)\Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]$$,  which is equal to the expectation $$ E_x[\ \Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]\ ]$$ ( so we view $$\Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]$$ as a random variable of $x$). Now we have the expectation is greater than $1/2 + \epsilon$, by the Markov ineuality (Theorem 1 in background) we have:  

$$\Pr_x[\ Pr[A(f(x),r)\ =\ \langle x ,r\rangle]\geq \frac{1}{2} + \frac{1}{2}\epsilon\ ]\geq \frac{\frac{1}{2}\epsilon}{\frac{1}{2}-\frac{1}{2}\epsilon}\geq \frac{\epsilon}{2}$$

So at least $\frac{\epsilon}{2}$ of $x$ are good. On these good $x$, we prove the GL algorithm:  

**Theorem 3 (Goldreich-Levin Algorithm)**
*Suppose there is a function $H$ such that, for some $x$:*

$$\Pr_r[H(r) = \langle x, r \rangle] \geq \frac{1}{2} + \epsilon$$

*Then there is an algorithm GL outputs a list $L$ of candidates of $x$, such that $L$ is of size $O(\epsilon^{-2})$ and $x\in L$ with probability  at least $1/2$. The GL algorithm runs in time $O(n^2 \epsilon^{-4} \log n)$, makes $O(n\epsilon^{-4}\log n)$ queries to $H$.*


## References
Lecture Notes 11, 12 of Luca Trevisan's Spring 2009 Cryptography Courses, see this [Link](https://people.eecs.berkeley.edu/~luca/cs276/#notes).