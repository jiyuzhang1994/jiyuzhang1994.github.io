---
layout: default
permalink: /glthm/
title: Goldreich Levin Theorem
tags: Cryptography
---

In this blog we study and present the proof of Goldreich-Levin theorem. The main material we use is the  course notes of Prof. Luca Trevisan (see references at the end).  

The Goldreich-Levin Theorem has several interpretations. We will first explain and prove it from cryptographic view. Then we will see in fact it can also be viewed as list-decoding of Hadamard code.

## Background
In this section we present necessary definitions and math inequalities we will be using. The first theorem is a variant of Markov inequality.  

**Theorem 1 (Markov Inequality, Variant)** Suppose $X$ is a random variable in $[0, 1]$ and $0<t<E[X]$, then  

$$Pr[X\geq t] \geq \frac{E[X]-t}{1-t}$$

**Proof:**  
 
Let $S$ denote the set of $x$ such that $X(x)\geq t$, then

$$\begin{align*} 
E[X] &= \sum_{x\in S}Pr(x)X(x) + \sum_{x\not\in S}Pr(x)X(x)\\ 
 &\leq \sum_{x\in S}Pr(x)\cdot 1 + \sum_{x\not\in S}Pr(x)\cdot t\\
 &= Pr(S) + t\cdot (1-Pr(S))\\
 &= (1-t)\cdot Pr(S) + t
\end{align*}$$ 

**Theorem 2 (Chernoff Bound)**  Suppose $X_1, \ldots, X_k$ are $0, 1$ *i.i.d* random variables and $ X = \sum_i^{k} X_i$, then for any $0<\epsilon<1$:  

$$Pr[\ X>(1+\epsilon)E[X]\ ]<e^{-\frac{\epsilon^2}{3}\cdot E[X]}$$  

$$Pr[\ X<(1-\epsilon)E[X]\ ]<e^{-\frac{\epsilon^2}{3}\cdot E[X]}$$


**Theorem 3 (Chebyshev Inequality)**  
Suppose $X= X_1 + \cdots + X_k$ where $X_1, \ldots ,X_k$ are $0, 1$ pairwise independent variables and $t>0$, then  

$$Pr[\mid X-E[X]\mid \geq t] \leq \frac{Var(X_1) + \cdots + Var(X_k)}{t^2}$$

In particular, if for each $X_i$, $Pr[X_i = 1] \geq 1/2 + \epsilon$, then $Var(X_i)<\frac{1}{4}$, therefore 

$$Pr[\mid X-E[X]\mid \geq t] \leq \frac{Var(X_1) + \cdots + Var(X_k)}{t^2}\leq \frac{k}{4t^2}$$

**Proof** 

Let $S$ be the set of $x$ such that $\mid X(x)-E[X]\mid \geq t$

$$\begin{align*} 
Var[X] &= \sum_x (X(x)-E[X])^2 \cdot Pr(x)\\
 &= \sum_{x\in S}(X(x)-E[X])^2 \cdot Pr(x) + \sum_{x\not\in S}(X(x)-E[X])^2 \cdot Pr(x)\\
 &\geq Pr(S)\cdot t^2 + 0
\end{align*}$$

So 

$$Pr(S) \leq \frac{Var[X]}{t^2}$$  

and

$$\begin{align*} 
Var[X_1+X_2] &= E[(X_1+X_2)^2] - E(X_1+X_2)^2\\
&= E[X_1^2] + 2E[X_1X_2] + E[X_2]^2 - E[X_1]^2 -2E[X_1]E[X_2] - E[X_2]^2\\
&= Var[X_1]+Var[X_2]
\end{align*}$$

this can be easily generalized to show $Var[X_1 + \cdots + X_k]= Var[X_1] + \cdots + Var[X_k]$

**Definition 1 (Hard-Core Predicate)** A boolean function $$P: \{0, 1 \}^n \rightarrow \{0, 1\}$$ is $(t, \epsilon)$ - hard core for a permutation $$f: \{0,1\}^n \rightarrow \{0,1\}$$ if for every algorithm $A$ of complexity $t$  

$$\Pr_{x\sim \{0, 1\}^n}[A(f(x)) = P(x)]\leq \frac{1}{2}+\epsilon$$

Note: we use $$x\sim \{0, 1\}^n$$ to denote uniform distribution over $$\{0, 1\}^n$$


## Goldreich-Levin Theorem

We denote inner product modulo 2 using the following notation:  

$$\langle x,r\rangle = \sum_i x_i\cdot r_i\ mod\ 2$$


The Goldreich-Levin theorem says that a random XOR is hard-core for every one-way permutation. This means that if there is an efficient algorithm to predict $\langle x, r\rangle$, given $f(x)$ and $r$, then there is also an efficient algorithm to compute a pre-image of $f(x)$, so if $f(x)$ is a one-way function, we can invert it with noticeable probability.   

**Theorem 1 (Goldreich-Levin Theorem)** *Suppose $A$ is an algorithm of complexity $t$ such that*  

$$\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle] \geq \frac{1}{2} + \epsilon$$  

*Then there is an algorithm $A^\prime$ of complexity at most $O(t\epsilon^{-2}n^{O(1)})$ such that* 

$$\Pr_x[A^\prime(f(x)) = x] \geq \Omega(\epsilon)$$

We will first prove a weak Goldreich-Levin algorithm, which will later be used in our proof of GL theorem.

**Theorem 2 (Goldreich-Levin Algorithm Weak Version)** *Suppose there is a function $H$ such that, for some $x$*  

$$\Pr_r[H(r)\ =\ \langle x,r\rangle]\ \geq \frac{3}{4} + \epsilon$$

*Then there is an algorithm $GLW$ that can output $x$ with high probability ( $\geq 1-\frac{1}{n}$ ).*  

Let's first see how an easy algorithm can find $x$ when the probability on the right side is 1 in the above (so that $H(r)$ outputs $\langle x,r\rangle$ correctly).  

Let $e_i$ denote the vector in which the $i$th bit is 1 and 0 otherwise. $x_i = H(e_i) = \langle x, e_i \rangle$. Then we can output $x$ by enumerating all the $e_i$.   

Now to prove Theorem 2, note that $H(r)$ fails with small probability, we'll use the majority vote method. Intuitively,  we randomly sample several points $r_1 \ldots r_k$. Let $H^\prime_{r_1 \ldots r_k}(e_i)$ be the function that take the majority of  $H(r_i+e_i) - H(r_i)$ on these $r_i$s. The observation is that if $H(r)$ computes $\langle x, r\rangle$ correctly, then we have:

$$ \langle x, r_i+e_i \rangle - \langle x, r_i \rangle = \langle x, e_i\rangle$$

We now bound the probability that $H(r_i + e_i) - H(r_i)$ fails.  

$$\begin{align*}
  Pr[H(r_i + e_i) - H(r_i) \neq  \langle x, e_i\rangle] &= Pr[H(r_i + e_i) \neq \langle x, r_i+e_i \rangle\ \cup H(r_i) \neq \langle x, r_i \rangle]\\
  &\leq Pr[H(r_i + e_i) \neq \langle x, r_i+e_i \rangle]\ + Pr[H(r_i) \neq \langle x, r_i \rangle]\\
  &\leq \frac{1}{4} - \epsilon + \frac{1}{4} - \epsilon\\
  &\leq \frac{1}{2} - 2\epsilon
 \end{align*}$$
 
The GLW algorithm is as follows:
 
 * GLW Algorithm:
 * for $i= 1, 2, \ldots , n$:  
     * for $j = 1, 2, \ldots , 4\log n/ \epsilon^2$:  
         * randomly sample $$r_j\in \{0, 1\}^n$$
         * compute $H(r_j + e_i) - H(r_j)$
     * compute $$x_i = H^\prime_{r_1 \ldots r_k}(e_i) = Majority\{H(r_j + e_i) - H(r_j): j = 1, 2, \ldots , 4\log n/ \epsilon^2 \}$$  
 * return $x$  

Now we analyze this algorithm. Since each $H(r_j + e_i) - H(r_j)$ outputs the correct answer with probability at least $1/2 + 2\epsilon$. By the Chernoff Bound (Theorem 2 in Background), the probability that the majority function $H^\prime_{r_1 \ldots r_k}(e_i)$ fails to output $x_i$ is at most $e^{-2\log n} = O(\frac{1}{n^2})$. Then the union bound gives us that GLW outputs $x$ correctly with probability at least $1-1/n$. The algorithm runs in time $O(\frac{n^2 \log n}{\epsilon^2})$, makes $O(\frac{n\log n}{\epsilon^2})$ queries to $H$.

**Discussion.** The above framework only works for cases where $\Pr_r[H(r) = \langle x, r \rangle]$ greater than $\frac{3}{4}$. For the case in Goldreich-Levin theorem where $H(r)$ makes more error,the union bound gives probability less than $1/2$ and the Chernoff Bound won't work. In fact, it is possible to construct a function $H$ such that $Pr[H(r) = \langle x, r\rangle] = 3/4$ and $Pr[H(r) = \langle x^\prime, r\rangle] = 3/4$ where $x \neq x^\prime$. So no algorithm can guarantee to find the correct $x$ when given such $H$, since $x$ is not uniquely defined by $H$. However, the Goldreich-Levin theorem tells us how to find a list of possible candidates of $x$ with good probability. We present their algorithm below.    

We proceed to prove the Goldreich-Levin Theorem.

Given (as in the Goldreich-Levin theorem) $\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle] \geq \frac{1}{2} + \epsilon$, we first show that there is a certain fraction of $x$,  $\Pr_r[A(f(x), r) = \langle x, r \rangle] \geq \frac{1}{2} + \frac{1}{2}\epsilon$ holds, and we call such $x$ "good" :   

$$\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle]$$ can be viewed as $$\sum_x Pr(x)\Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]$$,  which is equal to the expectation $$ E_x[\ \Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]\ ]$$ ( so we view $$\Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]$$ as a random variable of $x$). Now we have the expectation is greater than $1/2 + \epsilon$, by the Markov ineuality (Theorem 1 in background) we have:  

$$\Pr_x[\ Pr[A(f(x),r)\ =\ \langle x ,r\rangle]\geq \frac{1}{2} + \frac{1}{2}\epsilon\ ]\geq \frac{\frac{1}{2}\epsilon}{\frac{1}{2}-\frac{1}{2}\epsilon}\geq \frac{\epsilon}{2}$$

So at least $\frac{\epsilon}{2}$ of $x$ are good. On these good $x$, we prove the GL algorithm:  

**Theorem 3 (Goldreich-Levin Algorithm)**
*Suppose there is a function $H$ such that, for some $x$:*

$$\Pr_r[H(r) = \langle x, r \rangle] \geq \frac{1}{2} + \epsilon$$

*Then there is an algorithm GL outputs a list $L$ of candidates of $x$, such that $L$ is of size $O(\epsilon^{-2})$ and $x\in L$ with probability  at least $1/2$. The GL algorithm runs in time $O(n^2 \epsilon^{-4} \log n)$, makes $O(n\epsilon^{-4}\log n)$ queries to $H$.*  

As discussed above, we now don't have $H(r)$ that predicts $\langle x, r \rangle$ well, so $H(r+r_j) - H(r_j)$ fails to predict $\langle x, r \rangle$ with high probability ($ > 1/2$). However, let's make an assumption that, instead of $H(r_j)$, we get the correct $\langle x, r_j \rangle$ for every $j$, so now $H(r+r_j) - \langle x, r_j \rangle$ succeeds with probability at least $1/2 + \epsilon$. We do an error reduction by taking $k= O(\frac{1}{\epsilon^2})$ samples of $r_j$, then apply the majority vote method. Thus we can reduce the probability to match that in Theorem 2 and apply the GLW algorithm to find $x$.     
Moreover, since we dont' know the correct $\langle x, r_j \rangle$, we just enumerate all the possible values of it, which gives the following (inefficient) algorithm:

* Inefficient GL algorithm:
* randomly sample $$r_1, \ldots, r_k \in \{0, 1\}^n$$ where $k =  O(\frac{1}{\epsilon^2})$
* for $$b_1, \ldots, b_k \in \{0, 1\}$$ (each $b_j$ corresponds to a possible value of $\langle x, r_j \rangle$) :  
    * let $$H^\prime_{r_1 \ldots r_k}(r) :=  Majority\{H(r_j + r) - b_j: j = 1, 2, \ldots, k \}$$
    * run GLW algorithm in Theorem 2, using $H^\prime_{r_1 \ldots r_k}(r)$ instead of $H(r)$, outputs $x$
    * add $x$ to list $L$
* return $L$

*Analysis:*  By the Chernoff Bound we have  

$$\Pr_{r,r_1,\ldots, r_k}[H^\prime_{r_1 \ldots r_k}(r) = \langle x, r\rangle]\geq 1-o(1)\geq 1-\frac{1}{32}$$  

It follow by the Markov Inequality that  

$$\Pr_r[\Pr_{r_1, \ldots, r_k}[H^\prime_{r_1 \ldots r_k}(r) = \langle x, r\rangle] > \frac{3}{4}]> \frac{31}{32}$$

By the GLW algorithm, with probability at least $\frac{31}{32} - \frac{1}{n} > \frac{1}{2}$, $x$ is in the list.

**Discussion.** The above algorithm is inefficient in that the size of $L$ is $2^k$, which is exponential in $k$, the GL algorithm below reduces this using pair-wise independence. 

* GL algorithm:
* Randomly sample $$r_1, \ldots, r_k \in \{0, 1\}^n$$ where $k =  O(\log \frac{1}{\epsilon^2})$
* for each subset $$S\subseteq \{r_1, \ldots, r_k\}$$, define $r_S = \sum_{i\in S} r_i$ 
* for all $$b_1 \ldots, b_k \in \{0, 1\}$$:  
    * define $b_S = \sum_{i\in S} b_i$
    * define $$H^\prime(r) :=  Majority\{H(r_S + r) - b_S: S\subseteq \{r_1, \ldots, r_k\} \}$$
    * run GLW algorithm in Theorem 2, using $H^\prime(r)$ instead of $H(r)$, outputs $x$
    * add $x$ to list $L$
* return $L$

Now we analyze this GL algorithm. Since $r_1, \ldots, r_k$ are randomly picked, for any two sets $S$ and $T$, $r_S$ and $r_S$ are pairwise independent, therefore $H(r_S+r) - b_S$ and $H(r_T+r) - b_T$ are pairwise independent. In addition, on the correct $b_S$, $H(r_S+r) - b_S = \langle x, r \rangle$ with probability at least $1/2 + \epsilon$. By the Chebyshev Ineuqality (Theorem 3 in background), $H^\prime(r)$ outputs the wrong value with probability at most $\frac{1}{4\epsilon^2 \cdot 2^{k}}$ where $k=O(\log \frac{8}{\epsilon^2})$, so the probability is at most $\frac{1}{32}$. Given such $H^\prime(r)$ the GLW algorithm will work well.   
The running time of GL algorithm is $O(n^2 \log n \cdot \epsilon^{-4})$, it makes$O(n^2\log n\cdot \epsilon^{-4})$ queries to $H$ and outputs a list $L$ of size $O(\frac{1}{\epsilon^2})$. $x$ is in $L$ with probability greater than $1/2$.


## References
Lecture Notes 11, 12 of Luca Trevisan's Spring 2009 Cryptography Courses, see this [Link](https://people.eecs.berkeley.edu/~luca/cs276/#notes).