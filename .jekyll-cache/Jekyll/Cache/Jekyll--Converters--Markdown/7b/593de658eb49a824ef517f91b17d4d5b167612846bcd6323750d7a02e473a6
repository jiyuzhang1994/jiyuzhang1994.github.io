I"¸<ul>
  <li><a href="#algo">An Algorithm for Estimating Success Probability via Nisan‚Äôs PRG</a></li>
  <li><a href="#sakszhou">Saks and Zhou‚Äôs Algorithm</a></li>
</ul>

<p>Given a ROBP of our interest, there is a trivial algorithm to compute the $(i,j)$-th entry of one layer‚Äôs transition matrix $M$: for input $(i,j)$, simply enumerates all transition strings <script type="math/tex">\{0,1\}^m</script> and counts the number of strings that lead to $j$, then divide this number by $2^m$. This algorithm uses space $O(d+m)$.</p>

<p>To derandomize $BPL$, it‚Äôs enough to show a deterministic algorithm that runs in small space and estimates the $(i, j)$-th  entry with error $1/d$ in the matrix $M^{2^r}$. We say this algorithm approximates $M^{2^r}$.</p>

<p>For reasons above, from now on we phrase everything in terms of matrix exponentiation.</p>

<p><strong>Fact.</strong> The repeated squaring algorithm computes exact $M^{2^r}$ with space $O(rd)$.</p>

<p>From Nisan‚Äôs generator, we should see that it implicitly gives an algorithm for approximating matrix exponentiation. Let‚Äôs show the algorithm below.</p>

<h2 id="-1-an-algorithm-for-approximating-matrix-exponentiation-via-nisans-prg"><a name="algo"></a> 1. An Algorithm for Approximating Matrix Exponentiation via Nisan‚Äôs PRG</h2>

<p>The algorithm starts by randomly picking a set of hash functions and store these functions, we call the space we need for this purpose the <em>random bit complexity</em>. We think of this step as the outer layer of this algorithm. It then starts to feed the pseudorandom bits, produced by computing these hash functions, to the ROBP and computes using the trivial counting algorithm. We call the space for this purpose the <em>processing space complexity</em>. We think of this step as the inner layer (or subroutine) of the algorithm. The following algorithm from [1] captures the subroutine for processing matrices. Saks and Zhou‚Äôs algorithm is a variant of this subroutine.</p>

<p><strong>Lemma 4.1</strong> (Saks and Zhou; Nisan) For a $d\times d$ matrix $M$ and integers $r, m$, there is an algorithm PRS(M, r, m; h) (meaning <em>Pseudorandom Repeated Squaring</em>) that takes a random string <script type="math/tex">h\in \{0,1\}^{2m\cdot r}</script>, runs in space $O(m+r+\log d)$ and, if $m= O(\log d)$, approximates the matrix $M^{2^r}$ with error $O(1/d)$.</p>

<p>Therefore, if we use Nisan‚Äôs generator and apply the algorithm in Lemma 4.1 in a straightforward way, we have an algorithm for deterministically simulating $BPL$ in space $O(\log n^2)$ (for random bit complexity) + $O(\log n)$ (for processing space complexity) = $O(\log n^2)$. For comparison, the recursive repeated squaring algorithm has processing space complexity $O(\log n^2)$.</p>

<p>The main idea of Saks and Zhou for proving $BPL\subseteq L^{3/2}$ is , by combining these two algorithms (<em>PRS + Recursive Repeated Squaring</em>) in a sophisticated way, so the final random bit complexity falls down to $O(\log n^{3/2})$ and the processing space complexity becomes $O(\log n^{3/2})$.</p>

<h2 id="2--saks-and-zhous-algorithm">2. <a name="sakszhou"></a> Saks and Zhou‚Äôs Algorithm</h2>

<p>As mentioned in the last section, we gonna combine PRS and the repeated squaring algorithm in a ‚Äúsophisticated‚Äù way. To illustrate this way of combining, we first define some operators for real numbers in $[0,1]$ and similarly, for matrix.</p>

<p><strong>Definition 2.1</strong> (<em>Perturbation Operator</em>) a perturbation opertaor $\Sigma_\delta$ is a function mapping nonnegative real number $z\in [0,1]$ to <script type="math/tex">\Sigma_\delta = \max \{z-\delta, 0\}</script>. The operator applies to matrices by applying it entry by entry to the matrix.</p>

<p><strong>Definition 2.2</strong> (<em>Truncation Operator</em>) for a positive integer $t$, a truncation operator $\lfloor\  \rfloor_t$ is a function that, for a nonnegative real number $z$, truncating the binary expansion of $z$ after $t$ binary digits. That is, $\lfloor z\rfloor _t = 2^{-t} \lfloor 2^t z\rfloor$. Again, the operator applies to matrices by applying it entry by entry to the matrix.</p>

<p>Some facts for applying these operators on matrices will be useful for our purpose:</p>

<p><strong>Proposition 2.3</strong> For $M, N \in R^d\times R^d$,</p>

<ol>
  <li>$\lvert\lvert M - \lfloor M \rfloor_t \rvert\rvert \leq d\cdot 2^{-t}$</li>
  <li>$\lvert\lvert M - \Sigma_\delta M  \rvert\rvert \leq d\cdot \delta$</li>
  <li>$\lvert\lvert \Sigma_\delta M - \Sigma_\delta N\rvert\rvert \leq \lvert\lvert M - N\rvert\rvert$</li>
</ol>

<p>We now start to give an overview of Saks and Zhou‚Äôs algorithm. The idea is, to apply PRS recursively.</p>

<p>Let $A^r(M)$ be the matrix obtained by repeatedly squaring $M$ for $r$ times, i.e. $A^r(M) = M^{2^r}$. Consider $A^r(M) = A^{r_1\cdot r_2}(M) = A^{r_2}(A^{r_1}(M))$ where $r_1 \cdot r_2 = r$, if we apply PRS recursively, that is, we repeatedly compute $A^{r_1}$ for $r_2$ times. Now the random bit complexity will be $r_2\cdot r_1 m = O(rd)$ and the processing space complexity is $r_2\cdot O(d)$. Therefore the random bit complexity has not been improved, and we paid additional cost for processing space complexity.</p>

<p>The additional idea is to feed the same random bits for each level of these PRSs, so we can reduce the random bit complexity, and if necessary we are willing to pay more processing space complexity. However it‚Äôs hard to prove that there exists such a sequence of random bits that works for every level. To present how Saks and Zhou get avoid of this obstacle, we will introduce some objects which are useful both for presenting and for analyzing.</p>

<p>We define the <strong>exact repeated squaring matrices sequence</strong></p>

<p>$s_1:N_0=M_0, N_1 \ldots N_{r_2}$</p>

<p>where $N_i$ is the matrix obtained by computing $A^{r_1}(N_{i-1})= N^{2^{r_1}}_{i-1}$. The last matrix in the sequence is the one we would like to approximate.  We also define a <strong>pseudorandom matrices sequence</strong></p>

<p>$s_2:M_0, M_1, \ldots, M_{r_2}$</p>

<p>where $M_i = PRS(M_{i-1}, m, r_1; h)$. We will not directly show that $s_2$ approximates $s_1$ well because‚Ä¶ it‚Äôs not known how to prove it. Instead we want to show a modified version of $s_2$ that is good for our purpose.</p>

<p>We define the third sequence, which we call  <strong>pseudorandom matrices sequence SZ ver.</strong> (pseudorandom matrices sequence under Saks and Zhou‚Äôs manipulation)</p>

<p>$s_3:M_0, M^P_1, M^\Sigma_1, M_1, \ldots, M^P_{r_2}, M^\Sigma_{r_2}, M_{r_2}$</p>

<p>where $M^P_i = PRS(M_{i-1}, r_1, m; h)$; $M^\Sigma_i = \Sigma_{\delta_i}M^P_i$; $M_i = \lfloor M^\Sigma_i \rfloor_t$.</p>

<p>Saks and Zhou‚Äôs algorithm recursively computes $M_i$ in $s_3$.</p>

<p>We now present Saks and Zhou‚Äôs algorithm which we denote as SZ for computing sequence $M_{r_2}$ in $s_3$, and will prove its correctness in the next section.</p>

<hr />

<p>Algorithm SZ:</p>

<p><strong>Input</strong><br />
a $d\times d$ substochastic matrix $M$, intgers $r$ and $a$, indices $u, v\in [d]$.</p>

<p>Then the parameters $m, D, K, t= K-D$ are computed.</p>

<p><strong>Randomn input (stored in the outer layer as random bit complexity)</strong><br />
$h \in ({0,1}^{2m})^{r_2}$ and $q\in ({0,1}^{D})^{r_2}$</p>

<p>The algorithm SZ then recursively computes $M_i[u,v]$ in $s_3$</p>

<p><strong>Output</strong><br />
$M_{r_2}[u,v]$</p>

<hr />

<p>For the rest of this post we‚Äôll prove the following theorem</p>

<p><strong>Theorem 2.4</strong>  The algorithm SZ approximates $A^r(M)$ with error $K-D-2r-\log d$ except with probability $\frac{2^{r+ 2\log d}}{2^m(2^D + 2^{2K+4r+5\log d})}$</p>

<h2 id="references">References</h2>

<p>[1] Michael Saks and Shiyu Zhou, ‚ÄúBPHSPACE(S) ‚äÜ DSPACE(S^3/2)‚Äù. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.225.8850&amp;rep=rep1&amp;type=pdf</p>

:ET