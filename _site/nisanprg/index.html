<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Nisan's Pseudorandom Generator for Space-Bounded Computation &middot; Jiyu Zhang
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
   <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         processEscapes: true
       }
     });
   </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>


  <body class="theme-base-0d">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 style="font-size: 43px">
        <a href="/">
          Jiyu Zhang
        </a>
      </h1>
      <p class="lead">Contact: <br /> zjy9462 at gmail dot com</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item active" href="/">Home</a>
      <a class="sidebar-nav-item" href="/blog/">Blog</a>
      <a class="sidebar-nav-item" href="/Notes/">Notes</a>
      <a class="sidebar-nav-item" href="/teaching/">Teaching</a>
    </nav>

    <p>&copy; 2020. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <ul>
  <li><a href="#robp">ROBP and Matrix Exponentiation</a></li>
  <li><a href="#pre">Preliminaries</a></li>
  <li><a href="#prg">Nisan’s Pseudorandom Generator</a></li>
</ul>

<p>In this post we present Nisan’s pseudorandom generator (PRG). In the next post, we will see how Saks and Zhou use this PRG to prove that $BPL\subseteq L^{3/2}$.</p>

<h2 id="-1-robp-and-matrix-exponentiation"><a name="robp"></a> 1. ROBP and Matrix Exponentiation</h2>

<p>Recall that we are interested in the ROBP where each layer of it has the same set of nodes, and the set is of size $2^s$. Each node in a layer has $2^m$ out edges each labeled with a binary string of length $m$ and maps to a node in the next layer. We can visualize the computation on a ROBP as following:  we associate the transition from the $i$th layer to the $i+1$th layer with a $2^s\times 2^s$ transition matrix $M$ which is substochastic. Therefore each time the ROBP makes a transition, the previous product is multiplied with the same $M$. After $r$ times transition the resulting matrix is denoted as $M^r$. The entry $M^r_{i, j}$ is the probability that the ROBP starts at node $i$ and ends at node $j$ after $r$ transitions when we choose input uniformly at random. This view of the distribution as matrix will also be useful for analyzing.</p>

<p>Let’s examine some more properties about the matrix $M$. For each entry in $M$, say $M_{i,j}$, we can associate it with a subset of strings in <script type="math/tex">\{0,1\}^m</script>, which means that the the $i$th node given one of this set of strings will transit to the $j$th node in the next layer. Moreover, the same string does not appear twice in the a row. and the set of all strings in a row of $M$ is exactly the set of all strings <script type="math/tex">\{0,1\}^m</script>.</p>

<p>Recall our goal is to construct a pseudorandom generator for the above ROBP. This is equivalent to we want that the final matrix $M_{prg}^r$ obtained by feeding the ROBP with pseudorandom bits is “close” to the final matrix $M^r$ obtained by feeding with uniformly random bits.</p>

<p>With this in mind we now present some techniques we need in order to analyze Nisan’s PRG (which we will present later).</p>

<h2 id="2--preliminaries">2. <a name="pre"></a> Preliminaries</h2>

<p>For convenience, we set $d = 2^s$. We then introduce more notations. Let $[d]$ be the set of all nodes in a layer and <script type="math/tex">Q: [d]\times \{0,1\}^m\rightarrow [d]</script> be the transition function for two consecutive layers’s computation. <script type="math/tex">Q^r:[d]\times \{0,1\}^{m\cdot r}\rightarrow [d]</script> is the transition function for $r$ consecutive layers. Then we use $Q[x]$ to represent the behavior of $Q$ on <script type="math/tex">x \in \{0,1\}^m</script>, and the corresponding matrix is denoted as $M[x]$.</p>

<p>Let $Q_{i,j}$ be the subset of strings in <script type="math/tex">\{0,1\}^m</script> that maps node $i$ to node $j$ in the next layer. Similarly, let $Q^r_{i,j}$ be the subset of strings in <script type="math/tex">\{0,1\}^{m\cdot r}</script> which can be viewed as paths from node $i$ in the first layer to node $j$ in the last layer. Clearly, $M^r_{i,j}=\frac{\lvert Q^r_{i,j} \rvert}{2^{mr}}$.</p>

<p><strong>Definition 2.1</strong> (Matrix Norm) Let $M$ be a $d \times d$ matrix, we use the matrix norm <script type="math/tex">\lvert \lvert M\rvert \rvert</script> which is the largest row sum of $M$:</p>

<script type="math/tex; mode=display">\lvert \lvert M\rvert \rvert = \max_{i} \sum_j \lvert M_{i, j} \rvert</script>

<p>The following are some standard facts:</p>

<p><strong>Proposition 2.2</strong></p>

<p>For matrices $M, N \in R^{d}\times R^d$,</p>

<script type="math/tex; mode=display">\lvert \lvert M+N\rvert \rvert \leq \lvert \lvert M\rvert \rvert  + \lvert \lvert N\rvert \rvert</script>

<script type="math/tex; mode=display">\lvert \lvert MN\rvert \rvert \leq \lvert \lvert M\rvert \rvert  \cdot  \lvert \lvert N\rvert \rvert</script>

<p><strong>Proposition 2.3</strong></p>

<p>If $M, M^\prime, N, N^\prime$ are substochastic matrices, then</p>

<script type="math/tex; mode=display">\lvert\lvert MM^\prime - NN^\prime \rvert\rvert \leq \lvert\lvert M-N\rvert\rvert + \lvert\lvert M^\prime- N^\prime\rvert\rvert</script>

<p><strong>Definition 2.4</strong> (Pairwise Independent Hashing Family) A family of hashing functions <script type="math/tex">H:\{0,1\}^m\rightarrow \{0,1\}^n</script> is called pairwise independent if for any <script type="math/tex">x_1, x_2 \in \{0,1\}^m</script> and $x_1 \neq x_2$, <script type="math/tex">y_1, y_2\in \{0,1\}^n</script> , when <script type="math/tex">h:\{0,1\}^m\rightarrow \{0,1\}^n</script> is chosen randomly from $H$, then</p>

<script type="math/tex; mode=display">\Pr_{h\in H}\left [\ h(x_1) = y_1 \land h(x_2) = y_2 \right ] = \frac{1}{2^{2n}}</script>

<p>An explicit construction of such a hashing family is known to exist and can be encoded using $m+n$ bits. Moreover, it can be computed using $O(m)$ space.</p>

<p><strong>Definition 2.5</strong> Let $A, B\subseteq {0,1}^m$, $h:{0,1}^m \rightarrow {0,1}^m$. We say a hash function is <em>$\epsilon$-independent for $A$, $B$</em> if</p>

<script type="math/tex; mode=display">% <![CDATA[
\left \lvert \Pr_x \left[ x\in A \land h(x)\in B \right] - \frac{\lvert A\rvert \lvert B \rvert}{2^{2m}} \right \rvert < \epsilon %]]></script>

<p>From now on we denote $\frac{\lvert A\rvert}{2^m}$ as $\alpha$ and $\frac{\lvert B\rvert}{2^m}$ as $\beta$.</p>

<p>We now prove a mixing lemma which roughtly says that the functions from a pairwise independent hashing family are almost all $\epsilon$-independent.</p>

<p><strong>Theorem 2.6</strong> (Mixing Lemma) Fix any $A$ ,$B$, if <script type="math/tex">h:\{0,1\}^m\rightarrow \{0,1\}^m</script> is chosen randomly from a pairwise independent hashing family $H$, then</p>

<script type="math/tex; mode=display">\Pr_{h\in H} \left[ \text{h is not } \epsilon \text{-independent} \right]\leq \frac{\alpha\beta/\epsilon^2}{2^m}</script>

<p><strong>Proof.</strong></p>

<p>For any $h$, let $X_i$ be the indicator variable that $h(x_i)\in B$, and $X= \sum_{i\in A}X_i$. $h$ is  $\epsilon$-independent iff <script type="math/tex">% <![CDATA[
\lvert\frac{X}{2^m}-\alpha\beta\rvert < \epsilon %]]></script>. Consider the expectation $E[\frac{X}{2^m}] = \frac{1}{2^m}E[X]$.  By linearity of expectation, $ E[\frac{X}{2^m}] = \frac{1}{2^m}\sum_{i\in A} E[X_i] =  \frac{1}{2^m}\cdot \beta \lvert A\rvert = \alpha\beta$. Since $X_i$’s are pairwise independent, by Chebyshev inequality we have:</p>

<script type="math/tex; mode=display">\Pr\left[ \lvert\frac{X}{2^m}-\alpha\beta\rvert \geq \epsilon \right ]\leq \frac{Var[X]}{2^{2m}\cdot \epsilon^2} \leq\frac{\sum_{i\in A}Var[X_i]}{2^{2m}\cdot \epsilon^2} \leq \frac{\lvert A\rvert\cdot \beta(1-\beta)}{2^{2m}\cdot \epsilon^2}\leq \frac{\alpha\beta}{2^m\cdot \epsilon^2}</script>

<h2 id="-3-nisans-pseudorandom-generator"><a name="prg"></a> 3. Nisan’s Pseudorandom Generator</h2>

<p>Nisan’s generator is constructed by using random hash functions from pairwise independent hash family. <br />
Consider the matrix $M^2[x_1,x_2]$ of $Q^2[x_1, x_2]$ where $x_1x_2$ is chosen uniformly at random, we first show that if we choose $h_1$ randomly from $H$, then $M^2(x_1, h_1(x_1)$ of is close to $M^2(x_1,x_2)$.</p>

<p><strong>Lemma 3.1</strong></p>

<script type="math/tex; mode=display">\Pr \left[ \lvert\lvert M^2[x_1, h_1(x_1)] -M^2[x_1,x_2]\rvert\rvert \geq \epsilon \right ] \leq \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}</script>

<p><strong>Proof.</strong></p>

<p>Consider the $(i, j)$-th entry of $M^2[x_1, h_1(x_1)] -M^2(x_1,x_2)$,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
  \left\lvert M^2[x_1, h_1(x_1)]_{i,j} -M^2[x_1,x_2]_{i,j}\right\rvert &= \lvert \sum_p (M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - M[x_1]_{i,p}\cdot M[x_2]_{p, j})\rvert\\
  &\leq \sum_p \left\lvert M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - \alpha\beta \right\rvert\\
 \end{align*} %]]></script>

<p>Inparticular, if $h$ is $\epsilon/d^2$-independent for all $Q_{i,p}$ and $Q_{p, j}$, then</p>

<script type="math/tex; mode=display">% <![CDATA[
\sum_p \left\lvert M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - \alpha\beta \right\rvert < \epsilon/d %]]></script>

<p>and</p>

<script type="math/tex; mode=display">% <![CDATA[
\lvert\lvert M^2[x_1, h_1(x_1)] -M^2[x_1,x_2]\rvert\rvert <\epsilon %]]></script>

<p>With Lemma 2.6, if we want this holds for all triples $(i, p, j)$, a union bound gives us that</p>

<script type="math/tex; mode=display">\Pr \left[ \lvert\lvert M^2[x_1, h_1(x_1)] -M^2(x_1,x_2)\rvert\rvert \geq \epsilon \right ] \leq \frac{\alpha\beta\cdot d^4 \cdot d^3}{2^m \epsilon^2} = \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}</script>

<p>Now we have a lemma for the case $r=2$, we can do it by repeatedly squaring to prove a lemma for the case $r$. For the next steps, every time we pick a new hash function from the above family, we will generate our pseudorandom bits by compositing these picked hash functions. For example, when we pick $h_2$, then new pseudorandom bits are generated by concatenating $h_2(x)$ and $h_2(h_1(x))$. From now on we will denote the new matrix as $M^{2^r}[x; h_1,..\ldots h_r]$ and the matrix we feed with uniformly random bits $x_1, \ldots x_{2^r}$ is denoted simply as $M^{2^r}$. Let’s do some more steps:</p>

<p>when $r=4$,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
  \lvert\lvert M^4[x, h_1, h_2]_{i,j} -M^4_{i,j}\rvert\rvert &= \lvert\lvert M^4[x, h_1, h_2]_{i,j}-(M^2[x, h_1])^2 + (M^2[x, h_1])^2 - M^4\rvert\rvert\\
  &\leq \lvert\lvert M^4[x, h_1, h_2]_{i,j}-(M^2[x, h_1])^2\rvert\rvert + \lvert\lvert (M^2[x, h_1])^2 - M^4\rvert\rvert\\
  &\leq \epsilon + 2\cdot \epsilon\\
  &\leq 3\epsilon
 \end{align*} %]]></script>

<p>when $r=8$,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
  \lvert\lvert M^8[x, h_1, h_2, h_3]_{i,j} -M^8_{i,j}\rvert\rvert &= \lvert\lvert M^8[x, h_1, h_2, h_3]_{i,j}-(M^4[x, h_1, h_2])^2 + (M^4[x, h_1, h_2])^2 - M^8\rvert\rvert\\
  &\leq \epsilon + 2\cdot 3\epsilon\\
  &\leq 7\epsilon
 \end{align*} %]]></script>

<p>The recursive relation for error is $\epsilon(r)=\epsilon + 2\cdot \epsilon(\log r)$.  In general, we have</p>

<script type="math/tex; mode=display">\epsilon(r) = (r-1)\epsilon</script>

<p><strong>Theorem 3.2</strong> If we pick $h_1,\ldots ,h_{\log r}$ uniformly random from the pairwise independent hash family, then</p>

<script type="math/tex; mode=display">\Pr_{h_1,\ldots ,h_{\log r}}\left[\lvert\lvert M^r[h_1,\ldots ,h_{\log r}] - M^r\rvert\rvert \geq (r-1)\epsilon \right]\leq (\log r) \cdot \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}</script>

<p>With these we’re ready to present Nisan’s pseudorandom generator.</p>

<p><strong>Definition 3.3</strong> (<em>Pseudorandom Generator for Matirx Exponentiation</em>) A pseudorandom generator $G$ is $\epsilon$-pseudorandom for $M^r$ if</p>

<script type="math/tex; mode=display">\lvert M^r[G(x)]_{i,j} - M^r[x_1 \ldots x_r]_{i,j}\rvert \leq \epsilon</script>

<p>Nisan’s generator is thus constructed by compositing these randomly chosen hash functions as we described above. For <script type="math/tex">x\in \{0,1\}^m</script>,</p>

<script type="math/tex; mode=display">G_{h_1,\ldots ,h_{\log r}}(x) = x,h_1(x), h_2(x), h_2(h_1(x)), \ldots, h_{\log r}(x),\ldots h_{\log r}(h_{\log r-1}(\ldots h_1(x)))</script>

<p><strong>Theorem 3.4</strong> (Nisan) For $d, m$ and $r&lt;d$, there is a pseudorandom generator $G$ that is $1/d$-pseudorandom for $M^r$. Moreover, the generator takes a seed of length $O(m\log r)$ and produces a sequence of pseudorandom bits block by block for a total length of $m\cdot r$. Finally, the outputs can be computed in space $O(m+\log r)$.</p>

<p><strong>Proof.</strong></p>

<p>In Theorem 3.2, let’s set $\epsilon = 1/2dr$ and $m = \Theta(\log d) = C\log d$ for some large $C$. Therefore</p>

<script type="math/tex; mode=display">\lvert\lvert M^r[h_1,\ldots ,h_{\log r}] - M^r\rvert\rvert \leq 1/2d</script>

<p>except with probability at most $\frac{d^9 r^2 \log r}{d^C} = O(1/d)$ for large $C$. While in the later case, the difference is bounded to be at most $1\cdot O(1/d) = 1/2d$.</p>

<p>Finally, we have</p>

<script type="math/tex; mode=display">\left \lvert\Pr_{x, h_1, \ldots ,h_{\log r}}\left[ Q(i; G(x)) = j\right ]- \Pr_{x_1, \ldots , x_{2^r}}\left[ Q(i; x_1, \ldots , x^{2^r}) = j\right ]\right \lvert \leq 1/d</script>


    </div>

  </body>
</html>
