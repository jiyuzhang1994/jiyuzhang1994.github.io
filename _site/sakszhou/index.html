<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Derandomize BPL:BPL is in L^{3/2} &middot; Jiyu Zhang
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
   <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         processEscapes: true
       }
     });
   </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>


  <body class="theme-base-0g">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 style="font-size: 43px">
        <a href="/">
          Jiyu Zhang
        </a>
      </h1>
      <p class="lead">Contact: <br /> zjy9462 at gmail dot com</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item active" href="/">Home</a>
      <a class="sidebar-nav-item" href="/blog/">Blog</a>
      <a class="sidebar-nav-item" href="/Notes/">Notes</a>
      <a class="sidebar-nav-item" href="/teaching/">Teaching</a>
    </nav>

    <p>&copy; 2020. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <ul>
  <li><a href="#algo">An Algorithm for Estimating Success Probability via Nisan’s PRG</a></li>
  <li><a href="#sakszhou">Saks and Zhou’s Algorithm</a></li>
  <li><a href="#correctness">Correctness Proof</a></li>
</ul>

<p>Given a ROBP of width $d$ of our interest, it is naturally associated with a $d\times d$ transition matrix $M$, and there is a trivial algorithm to compute the $(i,j)$-th entry of $M$: for input $(i,j)$, simply enumerates all transition strings <script type="math/tex">\{0,1\}^m</script> and counts the number of strings that lead to $j$, then divide this number by $2^m$. This algorithm uses space $O(m+\log d)$. We say this algorithm computes exact  $M$.</p>

<p>To derandomize $BPL$, it’s enough to show a deterministic algorithm that, given as input a matrix $M$, a positive integer $r$ and a pair of indices $(i, j)$, runs in small space and estimates the $(i, j)$-th  entry with error $1/d$ in the matrix $M^{2^r}$. We say this algorithm approximates $M^{2^r}$.</p>

<p>For reasons above, from now on we phrase everything in terms of computing/approximating matrix exponentiation.</p>

<p><strong>Fact.</strong> Given a $d\times d$ matrix $M$, a positive integer $r$, and a pair of indices $(i, j)$, the repeated squaring algorithm computes exact $M^{2^r}$ using space $O(r \log d)$.</p>

<p>From Nisan’s generator, we should see that it implicitly gives an algorithm for approximating matrix exponentiation. Let’s show the algorithm below.</p>

<h2 id="-1-an-algorithm-for-approximating-matrix-exponentiation-via-nisans-prg"><a name="algo"></a> 1. An Algorithm for Approximating Matrix Exponentiation via Nisan’s PRG</h2>

<p>The algorithm starts by randomly picking a set of hash functions and store these functions, we call the space we need for this purpose the <em>random bit complexity</em>. We think of this step as the outer layer of this algorithm. It then starts to feed the pseudorandom bits, produced by computing these hash functions, to the ROBP and computes using the trivial counting algorithm. We call the space for this purpose the <em>processing space complexity</em>. We think of this step as the inner layer (or subroutine) of the algorithm. The following algorithm from [1] captures the subroutine for processing matrices. Saks and Zhou’s algorithm is a variant of this subroutine.</p>

<p><strong>Lemma 1.1</strong> (Saks and Zhou; Nisan) For a $d\times d$ matrix $M$ and integers $r, m$, there is an algorithm PRS(M, r, m; h) (meaning Pseudorandom Repeated Squaring) that takes a random string <script type="math/tex">h\in \{0,1\}^{2m\cdot r}</script>, runs in space $O(m+r+\log d)$ and, if $m= O(\log d)$, approximates the matrix $M^{2^r}$ with error $O(1/d)$ except with probability $O(1/d)$.</p>

<p>Therefore, if we use Nisan’s generator and apply the algorithm in Lemma 4.1 in a straightforward way, we have an algorithm for deterministically simulating $BPL$ in space $O(\log^2 n)$ (for random bit complexity) + $O(\log n)$ (for processing space complexity) = $O(\log^2 n)$. For comparison, the recursive repeated squaring algorithm has processing space complexity $O(\log^2 n)$.</p>

<p>The main idea of Saks and Zhou for proving $BPL\subseteq L^{3/2}$ is , by combining these two algorithms (<em>PRS + Recursive Repeated Squaring</em>) in a sophisticated way, so the final random bit complexity falls down to $O(\log^{3/2} n)$ and the processing space complexity becomes $O(\log^{3/2} n)$.</p>

<h2 id="2--saks-and-zhous-algorithm">2. <a name="sakszhou"></a> Saks and Zhou’s Algorithm</h2>

<p>As mentioned in the last section, we gonna combine PRS and the repeated squaring algorithm in a “sophisticated” way. To illustrate this way of combining, we first define some operators for real numbers in $[0,1]$ and similarly, for matrix.</p>

<p><strong>Definition 2.1</strong> (<em>Perturbation Operator</em>) a perturbation opertaor $\Sigma_\delta$ is a function mapping nonnegative real number $z\in [0,1]$ to <script type="math/tex">\Sigma_\delta = \max \{z-\delta, 0\}</script>. The operator applies to matrices by applying it entry by entry to the matrix.</p>

<p><strong>Definition 2.2</strong> (<em>Truncation Operator</em>) for a positive integer $t$, a truncation operator $\lfloor\  \rfloor_t$ is a function that, for a nonnegative real number $z$, truncating the binary expansion of $z$ after $t$ binary digits. That is, $\lfloor z\rfloor _t = 2^{-t} \lfloor 2^t z\rfloor$. Again, the operator applies to matrices by applying it entry by entry to the matrix.</p>

<p>Some facts for applying these operators on matrices will be useful for our purpose:</p>

<p><strong>Proposition 2.3</strong> For $M, N \in R^d\times R^d$,</p>

<ol>
  <li>$\lvert\lvert M - \lfloor M \rfloor_t \rvert\rvert \leq d\cdot 2^{-t}$</li>
  <li>$\lvert\lvert M - \Sigma_\delta M  \rvert\rvert \leq d\cdot \delta$</li>
  <li>$\lvert\lvert \Sigma_\delta M - \Sigma_\delta N\rvert\rvert \leq \lvert\lvert M - N\rvert\rvert$</li>
</ol>

<p>We now start to give an overview of Saks and Zhou’s algorithm. The idea is to apply PRS recursively, but not in the direct way.</p>

<p>Let $A^r(M)$ be the matrix obtained by repeatedly squaring $M$ for $r$ times, i.e. $A^r(M) = M^{2^r}$. Consider $A^r(M) = A^{r_1\cdot r_2}(M) = A^{r_2}(A^{r_1}(M))$ where $r_1 \cdot r_2 = r$,  we apply PRS recursively, that is, we repeatedly compute $A^{r_1}$ for $r_2$ times. Now the random bit complexity will be $r_2\cdot r_1 m = O(rd)$ and the processing space complexity is $r_2\cdot O(d)$. Therefore the random bit complexity has not been improved, and we paid additional cost for processing space complexity.</p>

<p>The additional idea is to feed the same random bits for each level of these PRSs, so we can reduce the random bit complexity, and if necessary we are willing to pay more processing space complexity. However it’s hard to prove that there exists such a sequence of random bits that works for every level. To present how Saks and Zhou get avoid of this obstacle, we will introduce some objects which are useful both for presenting and for analyzing.</p>

<p>We define the <strong>exact repeated squaring matrices sequence</strong></p>

<p>$s_1:N_0=M_0, N_1 \ldots N_{r_2}$</p>

<p>where $N_i$ is the matrix obtained by computing $A^{r_1}(N_{i-1})= N^{2^{r_1}}_{i-1}$. The last matrix in the sequence is the one we would like to approximate.  We also define a <strong>pseudorandom matrices sequence</strong></p>

<p>$s_2:M_0, M_1, \ldots, M_{r_2}$</p>

<p>where $M_i = PRS(M_{i-1}, m, r_1; h)$. We will not directly show that $s_2$ approximates $s_1$ well because… it’s not known how to prove it. Instead we want to show a modified version of $s_2$ that is good for our purpose.</p>

<p>We define the third sequence, modified from $s_2$, which we call  <strong>pseudorandom matrices sequence SZ</strong> (pseudorandom matrices sequence under Saks and Zhou’s manipulation)</p>

<p>$s_3:M_0, M^P_1, M^\Sigma_1, M_1, \ldots, M^P_{r_2}, M^\Sigma_{r_2}, M_{r_2}$</p>

<p>where $M^P_i = PRS(M_{i-1}, r_1, m; h)$; $M^\Sigma_i = \Sigma_{\delta_i}M^P_i$; $M_i = \lfloor M^\Sigma_i \rfloor_t$.</p>

<p>Saks and Zhou’s algorithm recursively computes $M_i$ in $s_3$.</p>

<p>We now present Saks and Zhou’s algorithm which we denote as SZ for computing $M_{r_2}$ in $s_3$, and will prove its correctness in the next section.</p>

<hr />

<p>Algorithm SZ:</p>

<p><strong>Input</strong><br />
a $d\times d$ substochastic matrix $M$, intgers $r$ and $a$, indices $u, v\in [d]$.</p>

<p>Then the parameters $m, D, K, t= K-D$ are computed.</p>

<p><strong>Randomn input (stored in the outer layer as random bit complexity)</strong><br />
<script type="math/tex">h \in (\{0,1\}^{2m})^{r_1}</script> and <script type="math/tex">q\in (\{0,1\}^{D})^{r_2}</script></p>

<p>The algorithm SZ then recursively computes $M_i[u,v]$ in $s_3$</p>

<p><strong>Output</strong><br />
$M_{r_2}[u,v]$</p>

<hr />

<p>The random bit complexity is $O(r_1 m) + O(r_2 D) = O\left ( (r_1+r_2)d \right)$ when $m, D = \theta(d)$.</p>

<p>The processing space complexity is $O(r_2 d)$.</p>

<h2 id="-3-correctness-proof"><a name="correctness"></a> 3. Correctness Proof</h2>

<p>For the rest of this post we’ll prove the following theorem</p>

<p><strong>Theorem 3.1</strong>  The algorithm SZ approximates $A^r(M)$ with error $K-D-2r-\log d$ except with probability $\frac{2^{r+ 2\log d}}{2^m\cdot (2^D + 2^{2K+4r+5\log d})}$</p>

<p>We define the fourth sequence, modified from $s_1$, which we call <strong>exact repeated squaring sequence SZ</strong>. This sequence will act as a bridge in our analysis.</p>

<p>$s_4: N_0 = M_0, N^P_1, N^\Sigma_1, N_1, \ldots, N^P_{r_2}, N^\Sigma_{r_2}, N_{r_2}$</p>

<p>We will show three lemmas through which we can see Theorem 3.1 follows.</p>

<p><strong>Lemma 3.2</strong>  For any <script type="math/tex">q\in (\{0,1\}^{D})^{r_2}</script>, the sequence $s_4$ has the property that the item $N_{r_2}$ approximates $A^r(M)$ with error $O(2^{K-D-2r-\log d})$.</p>

<p>With this lemma, we would like to show that $M_{r_2}$ in $s_3$ is close to $N_{r_2}$ in $s_4$. We can actually show that $M_{r_2} = N_{r_2}$ for most of $q$ and $h$.</p>

<p><strong>Definition 3.3</strong> Let $M$ be a substochastic matrix. For integers $r,a,m$, we say a string <script type="math/tex">h \in (\{0,1\}^{2m})^{r_1}</script> is $a$-pseudorandom for $M$ if $PRS(W, r, m;h)$ approximates $A^r(M)$ with error $a$.</p>

<p><strong>Definition 3.4</strong> A nonnegative real number $r$ is $(b,t)$-dangerous for positive integers $b&gt;t$, if $r$ can be written as $2^{-t} I +\rho$, where $I$ is positive and $\rho\in [-2^{-b}, 2^{-b})$, otherwise we say $r$ is $(b,t)$-safe. A substochastic matrix $M$ is $(b,t)$-safe/dangerous if all of its entries are $(b,t)$-safe/dangerous.</p>

<p><strong>Lemma 3.5</strong> For a fixed <script type="math/tex">q\in (\{0,1\}^{D})^{r_2}</script>, if all the matrices in $s_4$ are $(K,t)$-safe, and <script type="math/tex">h \in (\{0,1\}^{2m})^{r_1}</script> is $K$-pseudorandom for $N_i$’s for $0\leq i\leq r_2$. Then $M_{i}=N_{i}$ for all $i$. In particualr, $M_{r_2}=N_{r_2}$.</p>

<p><strong>Lemma 3.6</strong> For a randomly chosen <script type="math/tex">q\in (\{0,1\}^{D})^{r_2}</script>, the probability that all of the matrices $N^\Sigma_i$ in $s_4$ are $(K,t)$-safe is at least $1-2^{-D+r+2\log d}$.</p>

<p><strong>Proof of Theorem 3.1</strong><br />
<script type="math/tex">% <![CDATA[
\begin{align*}
  \Pr [M_{r_2} \neq N_{r_2}] &\leq \Pr[\text{some of the }N_i\text{'s in } s_4 \text{ are }(K,t) \text{-dangerous}] + \Pr [h \text{ is not }K\text{-pseudorandom for each } N_i\in s_4]\\
  &\leq 2^{-D+r+2\log d} + 2^{-m+2K+4r+5\log d}\\
 \end{align*} %]]></script></p>

<p>Proofs for Lemma 3.2, Lemma 3.5, Lemma 3.6 to be completed.</p>

<h2 id="references">References</h2>

<p>[1] Michael Saks and Shiyu Zhou, “BPHSPACE(S) ⊆ DSPACE(S^3/2)”. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.225.8850&amp;rep=rep1&amp;type=pdf</p>


    </div>

  </body>
</html>
