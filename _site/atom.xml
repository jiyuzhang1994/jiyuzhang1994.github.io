<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Jiyu Zhang</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2020-10-02T07:08:55+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Mark Otto</name>
   <email></email>
 </author>

 
 <entry>
   <title>入学前一些零散的想法...</title>
   <link href="http://localhost:4000/prephd/"/>
   <updated>2020-09-24T00:00:00+08:00</updated>
   <id>http://localhost:4000/prephd</id>
   <content type="html">&lt;p&gt;目标：学好（？）quantum + cryptography&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Quantum Resources</title>
   <link href="http://localhost:4000/quantum/"/>
   <updated>2020-07-07T00:00:00+08:00</updated>
   <id>http://localhost:4000/quantum</id>
   <content type="html">&lt;p&gt;Starting Quantum Computation (no review/comments yet)&lt;/p&gt;

&lt;p&gt;Book:&lt;/p&gt;

&lt;p&gt;Quantum Computation and Quantum Information, by Michael Nielsen and Isaac Chuang.&lt;/p&gt;

&lt;p&gt;Supplementary Lecture notes:&lt;/p&gt;

&lt;p&gt;Aaronson: &lt;a href=&quot;https://www.scottaaronson.com/blog/?p=4805&quot;&gt;18fall notes&lt;/a&gt;, &lt;a href=&quot;https://www.scottaaronson.com/blog/?p=3943&quot;&gt;problem sets&lt;/a&gt; ;&lt;/p&gt;

&lt;p&gt;O’Donnell: &lt;a href=&quot;http://www.cs.cmu.edu/~odonnell/quantum15/&quot;&gt;15fall course&lt;/a&gt; ; &lt;a href=&quot;https://www.youtube.com/playlist?list=PLm3J0oaFux3YL5qLskC6xQ24JpMwOAeJz&quot;&gt;18fall videos&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;Watrous: &lt;a href=&quot;https://cs.uwaterloo.ca/~watrous/LectureNotes.html&quot;&gt;lecture notes&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;de Wolf: &lt;a href=&quot;https://arxiv.org/abs/1907.09415&quot;&gt;lecture notes&lt;/a&gt; .&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Hardness Amplification for weak O.W.Fs</title>
   <link href="http://localhost:4000/wowftosowf/"/>
   <updated>2020-06-15T00:00:00+08:00</updated>
   <id>http://localhost:4000/wowftosowf</id>
   <content type="html">&lt;p&gt;We prove that if there exists a weak one-way function, then there exists a strong one-way function, in the sense that the weak o.w.f can be converted into a strong o.w.f. We call this transformation as &lt;strong&gt;Hardness Amplification&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1.&lt;/strong&gt; For any weak one-way function &lt;script type=&quot;math/tex&quot;&gt;f:\{0,1\}^*\to \{0,1\}^*&lt;/script&gt;, &lt;strong&gt;there exists a polynomial&lt;/strong&gt; $m(\cdot)$ such that &lt;script type=&quot;math/tex&quot;&gt;f^\prime (x_1,\ldots ,x_{m(n)})= f(x_1), \ldots, f(x_{m(n)})&lt;/script&gt; is a strong one-way function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose $f$ is $q(n)$-weak, we show that there exists a polynomial $m(\cdot)$ such that if $f^\prime$ as above is not a strong o.w.f, we can invert $f(x)$ with probability greater than $2q(n)$ for random input $x$.&lt;/p&gt;

&lt;p&gt;Assume $f^\prime$ is not a strong o.w.f, there exists $A^\prime$ s.t&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y})= \vec{x} \right] \geq 1/p(mn)&lt;/script&gt;

&lt;p&gt;for some polynomial polynomial $p(mn)$. In fact, we’ll show that $m= 2n q(n)$ suffices for our purpose.&lt;/p&gt;

&lt;p&gt;We next present algorithms for inverting $f(x)$.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;$A_0(y, f, A^\prime)$:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Choose $i\in [m]$ uniformly at random.&lt;/li&gt;
  &lt;li&gt;Choose &lt;script type=&quot;math/tex&quot;&gt;x_j\in \{0,1\}^n&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;j\in[m] \backslash \{i\}&lt;/script&gt; at random.&lt;/li&gt;
  &lt;li&gt;$y_i \leftarrow y$, $y_j \leftarrow f(x_j)$, we get $\vec{y}$. Then compute $A^\prime (\vec{y})$.&lt;/li&gt;
  &lt;li&gt;Output $x_i$ if $f(x_i) = y$. Otherwise output fail.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;We also would like to amplify our probability for success by repeating $A_0$ as many times as possible  (as long as it is in p.p.t). We define $A$ which repeats $A_0$ for $2nm^2 p(mn)$ times.  We’ll show that $A$ inverts $f$ with good probability.&lt;/p&gt;

&lt;p&gt;For our analysis of $A$, we say $x$’s are “good” if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr \left[y\leftarrow f(x) : A_0(1^n, y) = x  \right] \geq \frac{1}{2m^2p(mn)}&lt;/script&gt;

&lt;p&gt;We next show that&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;On those good $x$’s, $A$ can invert $f(x)$ with good probability.&lt;/li&gt;
  &lt;li&gt;There are sufficiently many good $x$’s.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally we can show it contradicts the fact that $f$ is $q(n)$-weak one-way.&lt;/p&gt;

&lt;p&gt;On these good $x$’s, we know that the probability that $A$ fails to invert $f(x)$ is at most&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1-\frac{1}{2m^2p(mn)})^{2nm^2 p(mn)} \leq e^{-n}&lt;/script&gt;

&lt;p&gt;which is negligible.&lt;/p&gt;

&lt;p&gt;We’ll now show there are at least $2^n(1-\frac{1}{2q(n)})$ good $x$’s. We prove this by contradiction.&lt;/p&gt;

&lt;p&gt;Assume there are at most $2^n(1-\frac{1}{2q(n)})$ good $x$’s, we show (1) does not hold.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y})= \vec{x} \right] &amp;\leq \Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y}) \text{ succeeds} \land \text{all } x_i \text{'s are good} \right]\\  
  &amp;+ \Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y}) \text{ succeeds} \land \text{some } x_i \text{'s are bad} \right]\\   
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;for each $j\in [m]$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y}) \text{ succeeds} \land x_j \text{ is bad} \right] &amp;\leq \Pr \left[ \vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y}) = \vec{x} \mid x_j \text{ is bad} \right]\\  
  &amp;\leq m\cdot \Pr \left[ \vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}), y_j=f(x_j): A_0(1^n, y_j) = x_j \mid x_j \text{ is bad} \right]\\
  &amp;\leq m\cdot \frac{1}{2m^2p(mn)} \leq \frac{1}{2mp(mn)}   
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The second inequality holds because, to invert $f^\prime$, we need to invert $x_i$ for each $i\in[m]$. So the probability is less than that we take a union bound on inverting each $x_i$ using $A_0$, conditioned on that $x_j$ is bad.&lt;/p&gt;

&lt;p&gt;Now to bound &lt;script type=&quot;math/tex&quot;&gt;\Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y}) \text{ succeeds} \land \text{some } x_i \text{'s are bad} \right]&lt;/script&gt;, we again take a union bound over all the $j\in[m]$.  Therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y}) \text{ succeeds} \land \text{some } x_i \text{'s are bad} \right]\leq m\cdot \frac{1}{2mp(mn)}\leq \frac{1}{2p(mn)}&lt;/script&gt;

&lt;p&gt;Also,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y}) \text{ succeeds} \land \text{all } x_i \text{'s are good} \right] &amp;\leq \Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}: \text{all } x_i \text{'s are good} \right] \\
&amp;\leq (1-\frac{1}{2q(n)})^m \\
&amp;\leq e^{-n}\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is negligible.&lt;/p&gt;

&lt;p&gt;Thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\Pr \left[\vec{x} \leftarrow \{0,1\}^{m\cdot n}, \vec{y}=f^\prime(\vec{x}): A^\prime(1^{mn}, \vec{y})= \vec{x} \right]\leq \frac{1}{2p(mn)} + e^{-n} &lt; \frac{1}{p(mn)} %]]&gt;&lt;/script&gt;

&lt;p&gt;Contradicts (1).&lt;/p&gt;

&lt;p&gt;Now we can show the probability that $A$ fails to inverts $f$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}  
\Pr\left[ A(y, f, A^\prime) \text{fails} \right] &amp;\leq e^{-n}\cdot \Pr[x \text{ is good}] + 1\cdot \Pr[x \text { is bad}]\\
&amp;\leq e^{-n} + \frac{1}{2q(n)}\\ 
&amp;&lt; \frac{1}{q(n)}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So $f$ is not $q(n)$-weak.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] R.Pass &amp;amp; A.Shelat. A Course in Cryptography.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>negligible, non-negligible, noticeable</title>
   <link href="http://localhost:4000/negligible/"/>
   <updated>2020-06-13T00:00:00+08:00</updated>
   <id>http://localhost:4000/negligible</id>
   <content type="html">&lt;p&gt;Figure examples for negligible, non-negligible, noticeable functions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/negligible.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Some Notes on Zero Knowledge Proofs</title>
   <link href="http://localhost:4000/zknotes/"/>
   <updated>2020-06-03T00:00:00+08:00</updated>
   <id>http://localhost:4000/zknotes</id>
   <content type="html">&lt;p&gt;I’m reposting this to clearify questions I had when study these notes. I’ll be using the conventional notations in [1].&lt;/p&gt;

&lt;p&gt;There are two properties that Interactive Proofs must satisfy. Informally, a good interactive proof system for a language $L$ consists of a pair of prover and verifier $(P, V)$ that satisfies the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;if $x\in L$, then there is a very large probability that $V$ accepts. (the &lt;strong&gt;completeness&lt;/strong&gt;)&lt;/li&gt;
  &lt;li&gt;if $x\not\in L$, then a malicious prover has very tiny chance to convince $V$ that $x\in L$. ( the &lt;strong&gt;soundness&lt;/strong&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that, in completeness, we consider both honest prover and honest verifier. The honest prover may be either computationally bounded or computationally unrestricted depending on the model we concern. In soundness, we consider malicious prover who can arbitrarily deviate from the protocol, and is computationally unrestricted. The verifier is considered to be honest in this case.&lt;/p&gt;

&lt;p&gt;Zero knowledge proofs are, interactive proofs with an additional property called the zero knowledge property. The property says that in the interaction the prover doesn’t convey any knowledge but that the statement is true. In order to achieve this, we need to carefully define what is “knowledge”.&lt;/p&gt;

&lt;p&gt;As [1] suggests, “A conversation therefore conveys knowledge when the conversation allows the recipient to complete a “new” task that the recipient could not complete before”.&lt;/p&gt;

&lt;p&gt;In the computational world, “The amount of knowledge conveyed in a message can be quantified by considering the running time and size of a Turing machine that generates the message”. Therefore, the zero knowledge can mean that what you see doesn’t give you extra power to compute something. That is, the verifier can’t compute anything new given the messages communicated between the two parties.&lt;/p&gt;

&lt;p&gt;It turns out that this (zero knowledge) is modeled by showing that, for any adversarial p.p.t. verifier &lt;script type=&quot;math/tex&quot;&gt;V^*&lt;/script&gt; who tries to extract knowledge from the interaction, there exists a p.p.t. simulator &lt;script type=&quot;math/tex&quot;&gt;S^*&lt;/script&gt; such that the following distributions are computationally indistinguishable:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$View[P(x) \leftrightarrow V^*(x)]$&lt;/li&gt;
  &lt;li&gt;$S^*(x)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Informally, &lt;strong&gt;whatever Alice learns in the process can be produced by herself&lt;/strong&gt;. So Alice can essentially simulate the interaction independently.&lt;/p&gt;

&lt;p&gt;The following are some important notes I’d like to make.&lt;/p&gt;

&lt;h2 id=&quot;requiring-s-to-ouputcompute-vs-randomness&quot;&gt;Requiring &lt;script type=&quot;math/tex&quot;&gt;S^*&lt;/script&gt; to ouput/compute &lt;script type=&quot;math/tex&quot;&gt;V^*&lt;/script&gt;’s randomness&lt;/h2&gt;

&lt;p&gt;One thing [1] doesn’t specify is that the simulator necessarily needs to output the randomness the verifier is using. This is justified in [2] by giving an example of an interactive proof for graph isomorphism. The example shows the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;the protocol enables the verifier to learn the isomorphism (the witness) the prover holds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;there exists a perfect simulator if we don’t require the simulator to output the randomness of the verifier.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Readers should refer to page 7-6 of [2] to see the example. We only explained what properties the protocol has.&lt;/p&gt;

&lt;h2 id=&quot;the-power-of-simulator&quot;&gt;The power of simulator&lt;/h2&gt;

&lt;p&gt;Another question we might have is: since a language equiped with a zk proof has a simulator, then it should be possible for a malicious prover to utilize this simulator to cheat the verifier, which seems to immediately contradict the soundness. The answer is that, the simulator is usually allowed to do more than what the prover can do. Specifically, the simulator can &lt;strong&gt;rewind&lt;/strong&gt; the verifier.&lt;/p&gt;

&lt;p&gt;Rewinding means that the simulator may essentially query the verifier from any previous internal states of it. In contrast, the real prover is defined not to be able to do this. &lt;strong&gt;&lt;em&gt;What we can ask&lt;/em&gt;&lt;/strong&gt; is why this model is of interest. An example is that you can consider the situation when you (a real prover) are entering a system and you’re asked to answer a sequence of questions. If in some steps you say:”I want to go back to one of the previous questions, forget what I said just now.” Then this immediately causes the doubt of the verifier and it may simply halt and reject it.&lt;/p&gt;

&lt;p&gt;To remark, the simulator lives in the same world as the verifier does. They have the same auxiliary input. The simulator can rewind the verifier, and can be quantum if the verifier is quantum.&lt;/p&gt;

&lt;p&gt;Some remarks that might be helpful for understanding can be found at page 18 of [3]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;br /&gt;
[1] R.Pass &amp;amp; A.Shelat. A Course in Cryptography.&lt;br /&gt;
[2] Rafail Ostrovsky. Foundations of Cryptography Draft Lecture Notes.&lt;br /&gt;
[3] Yehuda Lindell. How To Simulate It – A Tutorial on the Simulation Proof Technique. https://eprint.iacr.org/2016/046.pdf&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Barrington's Theorem</title>
   <link href="http://localhost:4000/barrington/"/>
   <updated>2020-02-20T00:00:00+08:00</updated>
   <id>http://localhost:4000/barrington</id>
   <content type="html">&lt;p&gt;In this post we prove Barrintong’s theorem.&lt;/p&gt;

&lt;p&gt;Barrington’s theorem implies that the complexity class NC1 is equal to the class of constant width branching programs. Since we know that constant width branching program can be computed by NC1 circuits.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fact.&lt;/strong&gt; Constant width branching programs can be computed by general fan-in 2 circuits with $O(\log n)$ depth.&lt;/p&gt;

&lt;p&gt;The idea is to compute recursively using divide and conquer (divide the branching program into the first half and the second half).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1.&lt;/strong&gt; (Barrington) For any general circuit of fan-in 2 and depth $d$, there is a constant width (actually, width 5) branching program of length $4^d$ that simulates it. Especially when $d=O(\log n)$, then the branching program is of length at most $n^{O(1)}$.&lt;/p&gt;

&lt;p&gt;Recall that a width $w$ branching program has layers of the same vertices. Each vertex in the $i$-th layer has two out edges (labeled 0 and 1) mapping to two vertices in the next layer, depending on the input &lt;script type=&quot;math/tex&quot;&gt;x_i\in \{0,1\}&lt;/script&gt;. So essentially between every two consecutive layers there are two mappings $f, g : [w] \rightarrow [w]$.  We consider a special branching program called &lt;strong&gt;Permutation Branching Program (PBP)&lt;/strong&gt; where $f,g$ are permutations.&lt;/p&gt;

&lt;p&gt;Note that the computing process of PBP can be viewed as the composition of these permutations.  We say a permutation in $S_5$ is a 5-cycle if it is exactly a cycle of length 5. For example, $1\rightarrow 3\rightarrow 5\rightarrow 4 \rightarrow 2 \rightarrow 1$. We denote it as $(13542)$ for convenience.&lt;/p&gt;

&lt;p&gt;we define that the PBP $B$ &lt;em&gt;$\sigma$-computes&lt;/em&gt; a language $L$ if  there exists a 5-cycle $\sigma$ such that  $B(x)=\sigma\Rightarrow x\in L$ and $B(x) = e \Rightarrow x\not\in L$ where $e\in S_5$ is the identity.&lt;/p&gt;

&lt;p&gt;We proceed to prove a few lemmas, then we will prove that $O(\log n)$-depth circuit can be recognized by a 5-cycle PBP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 1.&lt;/strong&gt; Let $\sigma, \tau$ be any two 5-cycle. If there is a PBP $B_1$ $\sigma$-computes a language $L$, then there is a PBP $B_2$ of the same length that $\tau$-computes $L$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let $\sigma = (\sigma_1\sigma_2\sigma_3\sigma_4\sigma_5)$ and $\tau = (\tau_1\tau_2\tau_3\tau_4\tau_5)$. Consider the permutation $\theta = (\tau_1\rightarrow \sigma_1)$, then $\theta^{-1} \sigma \theta$ = $\tau$. To see why this holds, consider $\tau_i$,&lt;/p&gt;

&lt;p&gt;$\theta^{-1} \sigma \theta(\tau_i): \tau_i\rightarrow \sigma_i \rightarrow \sigma_{i+1}\rightarrow\tau_{i+1}$&lt;/p&gt;

&lt;p&gt;Therefore $B_1(x) = \sigma \Rightarrow B_2(x) = \tau$ and $B_1(x) = e \Rightarrow B_2(x) = e$.&lt;/p&gt;

&lt;p&gt;So basically to construct $B_2$ all we need is to “embed” a $\theta^{-1}$ at the front and a $\theta$ at the end.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 2.&lt;/strong&gt; If a language $L$ can be $\sigma$-computed  by a PBP for a 5-cycle $\sigma$, then its complement can be $\tau$-computed by a PBP of the same length for a 5-cycle $\tau$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We just embed a $\sigma^{-1}$ at the end of the PBP, so that it outputs $e$ if the original PBP outputs $\sigma$ and it outputs $\sigma^{-1}$ if the original PBP outputs $e$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 3.&lt;/strong&gt; There exists 5-cycles $\sigma, \tau$ such that $\sigma \tau \sigma^{-1} \tau^{-1}$ is also a 5-cycle.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A concrete example is $\sigma = (12345)$ and $\tau=(13542)$. Then $(12345)(13542)(54321)(24531) = (13254)$.&lt;/p&gt;

&lt;p&gt;Note that, Lemma 3 basically gives us an $AND$ gate. We can concatenate the four PBPs $B_1, B_2, B_3, B_4$ recognizes $\sigma,\tau,\sigma^{-1},\tau^{-1}$ respectively. If either $B_1(x)=e$ or $B_2(x)=e$, then $B_1B_2B_3B_4(x) = e$.  If both $B_1(x)=e$ and $B_2(x)=e$, then it outputs $e$. Otherwise it outputs $\sigma \tau \sigma^{-1} \tau^{-1}$. Combining with Lemma 2, we can also get $OR$ gate, by negating the previous outputs and applying $AND$ on them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of Theorem 1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The proof goes by induction:&lt;/p&gt;

&lt;p&gt;If the depth of the circuit is 0 then there is a length-1 PBP. Assume the depth of the circuit is $d-1$ and has length $4^{d-1}$ PBP and, assume WLOG, that at the top is an AND gate. The inputs of it are computed by $B_1$ and $B_2$. Then we concatenate $B_1B_2B_1^{-1}B_2^{-1}$, and now the theorem follows by lemma 3. The depth is $4^{d-1} \cdot 4 = 4^d$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] David A. Barrington. “Bounded-width polynomial-size branching programs recognize exactly those languages in NC^1”. https://people.cs.umass.edu/~barring/publications/bwbp.pdf&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Derandomize BPL:BPL is in L^{3/2}</title>
   <link href="http://localhost:4000/sakszhou/"/>
   <updated>2019-11-20T00:00:00+08:00</updated>
   <id>http://localhost:4000/sakszhou</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#algo&quot;&gt;An Algorithm for Estimating Success Probability via Nisan’s PRG&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sakszhou&quot;&gt;Saks and Zhou’s Algorithm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#correctness&quot;&gt;Correctness Proof&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given a ROBP of width $d$ of our interest, it is naturally associated with a $d\times d$ transition matrix $M$, and there is a trivial algorithm to compute the $(i,j)$-th entry of $M$: for input $(i,j)$, simply enumerates all transition strings &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^m&lt;/script&gt; and counts the number of strings that lead to $j$, then divide this number by $2^m$. This algorithm uses space $O(m+\log d)$. We say this algorithm computes exact  $M$.&lt;/p&gt;

&lt;p&gt;To derandomize $BPL$, it’s enough to show a deterministic algorithm that, given as input a matrix $M$, a positive integer $r$ and a pair of indices $(i, j)$, runs in small space and estimates the $(i, j)$-th  entry with error $1/d$ in the matrix $M^{2^r}$. We say this algorithm approximates $M^{2^r}$.&lt;/p&gt;

&lt;p&gt;For reasons above, from now on we phrase everything in terms of computing/approximating matrix exponentiation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fact.&lt;/strong&gt; Given a $d\times d$ matrix $M$, a positive integer $r$, and a pair of indices $(i, j)$, the repeated squaring algorithm computes exact $M^{2^r}$ using space $O(r \log d)$.&lt;/p&gt;

&lt;p&gt;From Nisan’s generator, we should see that it implicitly gives an algorithm for approximating matrix exponentiation. Let’s show the algorithm below.&lt;/p&gt;

&lt;h2 id=&quot;-1-an-algorithm-for-approximating-matrix-exponentiation-via-nisans-prg&quot;&gt;&lt;a name=&quot;algo&quot;&gt;&lt;/a&gt; 1. An Algorithm for Approximating Matrix Exponentiation via Nisan’s PRG&lt;/h2&gt;

&lt;p&gt;The algorithm starts by randomly picking a set of hash functions and store these functions, we call the space we need for this purpose the &lt;em&gt;random bit complexity&lt;/em&gt;. We think of this step as the outer layer of this algorithm. It then starts to feed the pseudorandom bits, produced by computing these hash functions, to the ROBP and computes using the trivial counting algorithm. We call the space for this purpose the &lt;em&gt;processing space complexity&lt;/em&gt;. We think of this step as the inner layer (or subroutine) of the algorithm. The following algorithm from [1] captures the subroutine for processing matrices. Saks and Zhou’s algorithm is a variant of this subroutine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 1.1&lt;/strong&gt; (Saks and Zhou; Nisan) For a $d\times d$ matrix $M$ and integers $r, m$, there is an algorithm PRS(M, r, m; h) (meaning Pseudorandom Repeated Squaring) that takes a random string &lt;script type=&quot;math/tex&quot;&gt;h\in \{0,1\}^{2m\cdot r}&lt;/script&gt;, runs in space $O(m+r+\log d)$ and, if $m= O(\log d)$, approximates the matrix $M^{2^r}$ with error $O(1/d)$ except with probability $O(1/d)$.&lt;/p&gt;

&lt;p&gt;Therefore, if we use Nisan’s generator and apply the algorithm in Lemma 4.1 in a straightforward way, we have an algorithm for deterministically simulating $BPL$ in space $O(\log^2 n)$ (for random bit complexity) + $O(\log n)$ (for processing space complexity) = $O(\log^2 n)$. For comparison, the recursive repeated squaring algorithm has processing space complexity $O(\log^2 n)$.&lt;/p&gt;

&lt;p&gt;The main idea of Saks and Zhou for proving $BPL\subseteq L^{3/2}$ is , by combining these two algorithms (&lt;em&gt;PRS + Recursive Repeated Squaring&lt;/em&gt;) in a sophisticated way, so the final random bit complexity falls down to $O(\log^{3/2} n)$ and the processing space complexity becomes $O(\log^{3/2} n)$.&lt;/p&gt;

&lt;h2 id=&quot;2--saks-and-zhous-algorithm&quot;&gt;2. &lt;a name=&quot;sakszhou&quot;&gt;&lt;/a&gt; Saks and Zhou’s Algorithm&lt;/h2&gt;

&lt;p&gt;As mentioned in the last section, we gonna combine PRS and the repeated squaring algorithm in a “sophisticated” way. To illustrate this way of combining, we first define some operators for real numbers in $[0,1]$ and similarly, for matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.1&lt;/strong&gt; (&lt;em&gt;Perturbation Operator&lt;/em&gt;) a perturbation opertaor $\Sigma_\delta$ is a function mapping nonnegative real number $z\in [0,1]$ to &lt;script type=&quot;math/tex&quot;&gt;\Sigma_\delta = \max \{z-\delta, 0\}&lt;/script&gt;. The operator applies to matrices by applying it entry by entry to the matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.2&lt;/strong&gt; (&lt;em&gt;Truncation Operator&lt;/em&gt;) for a positive integer $t$, a truncation operator $\lfloor\  \rfloor_t$ is a function that, for a nonnegative real number $z$, truncating the binary expansion of $z$ after $t$ binary digits. That is, $\lfloor z\rfloor _t = 2^{-t} \lfloor 2^t z\rfloor$. Again, the operator applies to matrices by applying it entry by entry to the matrix.&lt;/p&gt;

&lt;p&gt;Some facts for applying these operators on matrices will be useful for our purpose:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition 2.3&lt;/strong&gt; For $M, N \in R^d\times R^d$,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\lvert\lvert M - \lfloor M \rfloor_t \rvert\rvert \leq d\cdot 2^{-t}$&lt;/li&gt;
  &lt;li&gt;$\lvert\lvert M - \Sigma_\delta M  \rvert\rvert \leq d\cdot \delta$&lt;/li&gt;
  &lt;li&gt;$\lvert\lvert \Sigma_\delta M - \Sigma_\delta N\rvert\rvert \leq \lvert\lvert M - N\rvert\rvert$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We now start to give an overview of Saks and Zhou’s algorithm. The idea is to apply PRS recursively, but not in the direct way.&lt;/p&gt;

&lt;p&gt;Let $A^r(M)$ be the matrix obtained by repeatedly squaring $M$ for $r$ times, i.e. $A^r(M) = M^{2^r}$. Consider $A^r(M) = A^{r_1\cdot r_2}(M) = A^{r_2}(A^{r_1}(M))$ where $r_1 \cdot r_2 = r$,  we apply PRS recursively, that is, we repeatedly compute $A^{r_1}$ for $r_2$ times. Now the random bit complexity will be $r_2\cdot r_1 m = O(rd)$ and the processing space complexity is $r_2\cdot O(d)$. Therefore the random bit complexity has not been improved, and we paid additional cost for processing space complexity.&lt;/p&gt;

&lt;p&gt;The additional idea is to feed the same random bits for each level of these PRSs, so we can reduce the random bit complexity, and if necessary we are willing to pay more processing space complexity. However it’s hard to prove that there exists such a sequence of random bits that works for every level. To present how Saks and Zhou get avoid of this obstacle, we will introduce some objects which are useful both for presenting and for analyzing.&lt;/p&gt;

&lt;p&gt;We define the &lt;strong&gt;exact repeated squaring matrices sequence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$s_1:N_0=M_0, N_1 \ldots N_{r_2}$&lt;/p&gt;

&lt;p&gt;where $N_i$ is the matrix obtained by computing $A^{r_1}(N_{i-1})= N^{2^{r_1}}_{i-1}$. The last matrix in the sequence is the one we would like to approximate.  We also define a &lt;strong&gt;pseudorandom matrices sequence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$s_2:M_0, M_1, \ldots, M_{r_2}$&lt;/p&gt;

&lt;p&gt;where $M_i = PRS(M_{i-1}, m, r_1; h)$. We will not directly show that $s_2$ approximates $s_1$ well because… it’s not known how to prove it. Instead we want to show a modified version of $s_2$ that is good for our purpose.&lt;/p&gt;

&lt;p&gt;We define the third sequence, modified from $s_2$, which we call  &lt;strong&gt;pseudorandom matrices sequence SZ&lt;/strong&gt; (pseudorandom matrices sequence under Saks and Zhou’s manipulation)&lt;/p&gt;

&lt;p&gt;$s_3:M_0, M^P_1, M^\Sigma_1, M_1, \ldots, M^P_{r_2}, M^\Sigma_{r_2}, M_{r_2}$&lt;/p&gt;

&lt;p&gt;where $M^P_i = PRS(M_{i-1}, r_1, m; h)$; $M^\Sigma_i = \Sigma_{\delta_i}M^P_i$; $M_i = \lfloor M^\Sigma_i \rfloor_t$.&lt;/p&gt;

&lt;p&gt;Saks and Zhou’s algorithm recursively computes $M_i$ in $s_3$.&lt;/p&gt;

&lt;p&gt;We now present Saks and Zhou’s algorithm which we denote as SZ for computing $M_{r_2}$ in $s_3$, and will prove its correctness in the next section.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Algorithm SZ:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;br /&gt;
a $d\times d$ substochastic matrix $M$, intgers $r$ and $a$, indices $u, v\in [d]$.&lt;/p&gt;

&lt;p&gt;Then the parameters $m, D, K, t= K-D$ are computed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Randomn input (stored in the outer layer as random bit complexity)&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;h \in (\{0,1\}^{2m})^{r_1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;q\in (\{0,1\}^{D})^{r_2}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The algorithm SZ then recursively computes $M_i[u,v]$ in $s_3$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;br /&gt;
$M_{r_2}[u,v]$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The random bit complexity is $O(r_1 m) + O(r_2 D) = O\left ( (r_1+r_2)d \right)$ when $m, D = \theta(d)$.&lt;/p&gt;

&lt;p&gt;The processing space complexity is $O(r_2 d)$.&lt;/p&gt;

&lt;h2 id=&quot;-3-correctness-proof&quot;&gt;&lt;a name=&quot;correctness&quot;&gt;&lt;/a&gt; 3. Correctness Proof&lt;/h2&gt;

&lt;p&gt;For the rest of this post we’ll prove the following theorem&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3.1&lt;/strong&gt;  The algorithm SZ approximates $A^r(M)$ with error $K-D-2r-\log d$ except with probability $\frac{2^{r+ 2\log d}}{2^m\cdot (2^D + 2^{2K+4r+5\log d})}$&lt;/p&gt;

&lt;p&gt;We define the fourth sequence, modified from $s_1$, which we call &lt;strong&gt;exact repeated squaring sequence SZ&lt;/strong&gt;. This sequence will act as a bridge in our analysis.&lt;/p&gt;

&lt;p&gt;$s_4: N_0 = M_0, N^P_1, N^\Sigma_1, N_1, \ldots, N^P_{r_2}, N^\Sigma_{r_2}, N_{r_2}$&lt;/p&gt;

&lt;p&gt;We will show three lemmas through which we can see Theorem 3.1 follows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 3.2&lt;/strong&gt;  For any &lt;script type=&quot;math/tex&quot;&gt;q\in (\{0,1\}^{D})^{r_2}&lt;/script&gt;, the sequence $s_4$ has the property that the item $N_{r_2}$ approximates $A^r(M)$ with error $O(2^{K-D-2r-\log d})$.&lt;/p&gt;

&lt;p&gt;With this lemma, we would like to show that $M_{r_2}$ in $s_3$ is close to $N_{r_2}$ in $s_4$. We can actually show that $M_{r_2} = N_{r_2}$ for most of $q$ and $h$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 3.3&lt;/strong&gt; Let $M$ be a substochastic matrix. For integers $r,a,m$, we say a string &lt;script type=&quot;math/tex&quot;&gt;h \in (\{0,1\}^{2m})^{r_1}&lt;/script&gt; is $a$-pseudorandom for $M$ if $PRS(W, r, m;h)$ approximates $A^r(M)$ with error $a$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 3.4&lt;/strong&gt; A nonnegative real number $r$ is $(b,t)$-dangerous for positive integers $b&amp;gt;t$, if $r$ can be written as $2^{-t} I +\rho$, where $I$ is positive and $\rho\in [-2^{-b}, 2^{-b})$, otherwise we say $r$ is $(b,t)$-safe. A substochastic matrix $M$ is $(b,t)$-safe/dangerous if all of its entries are $(b,t)$-safe/dangerous.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 3.5&lt;/strong&gt; For a fixed &lt;script type=&quot;math/tex&quot;&gt;q\in (\{0,1\}^{D})^{r_2}&lt;/script&gt;, if all the matrices in $s_4$ are $(K,t)$-safe, and &lt;script type=&quot;math/tex&quot;&gt;h \in (\{0,1\}^{2m})^{r_1}&lt;/script&gt; is $K$-pseudorandom for $N_i$’s for $0\leq i\leq r_2$. Then $M_{i}=N_{i}$ for all $i$. In particualr, $M_{r_2}=N_{r_2}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 3.6&lt;/strong&gt; For a randomly chosen &lt;script type=&quot;math/tex&quot;&gt;q\in (\{0,1\}^{D})^{r_2}&lt;/script&gt;, the probability that all of the matrices $N^\Sigma_i$ in $s_4$ are $(K,t)$-safe is at least $1-2^{-D+r+2\log d}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of Theorem 3.1&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \Pr [M_{r_2} \neq N_{r_2}] &amp;\leq \Pr[\text{some of the }N_i\text{'s in } s_4 \text{ are }(K,t) \text{-dangerous}] + \Pr [h \text{ is not }K\text{-pseudorandom for each } N_i\in s_4]\\
  &amp;\leq 2^{-D+r+2\log d} + 2^{-m+2K+4r+5\log d}\\
 \end{align*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Proofs for Lemma 3.2, Lemma 3.5, Lemma 3.6 to be completed.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Michael Saks and Shiyu Zhou, “BPHSPACE(S) ⊆ DSPACE(S^3/2)”. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.225.8850&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Nisan's Pseudorandom Generator for Space-Bounded Computation</title>
   <link href="http://localhost:4000/nisanprg/"/>
   <updated>2019-11-06T00:00:00+08:00</updated>
   <id>http://localhost:4000/nisanprg</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#robp&quot;&gt;ROBP and Matrix Exponentiation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pre&quot;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#prg&quot;&gt;Nisan’s Pseudorandom Generator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post we present Nisan’s pseudorandom generator (PRG). In the next post, we will see how Saks and Zhou use this PRG to prove that $BPL\subseteq L^{3/2}$.&lt;/p&gt;

&lt;h2 id=&quot;-1-robp-and-matrix-exponentiation&quot;&gt;&lt;a name=&quot;robp&quot;&gt;&lt;/a&gt; 1. ROBP and Matrix Exponentiation&lt;/h2&gt;

&lt;p&gt;Recall that we are interested in the ROBP where each layer of it has the same set of nodes, and the set is of size $2^s$. Each node in a layer has $2^m$ out edges each labeled with a binary string of length $m$ and maps to a node in the next layer. We can visualize the computation on a ROBP as following:  we associate the transition from the $i$th layer to the $i+1$th layer with a $2^s\times 2^s$ transition matrix $M$ which is substochastic. Therefore each time the ROBP makes a transition, the previous product is multiplied with the same $M$. After $r$ times transition the resulting matrix is denoted as $M^r$. The entry $M^r_{i, j}$ is the probability that the ROBP starts at node $i$ and ends at node $j$ after $r$ transitions when we choose input uniformly at random. This view of the distribution as matrix will also be useful for analyzing.&lt;/p&gt;

&lt;p&gt;Let’s examine some more properties about the matrix $M$. For each entry in $M$, say $M_{i,j}$, we can associate it with a subset of strings in &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^m&lt;/script&gt;, which means that the the $i$th node given one of this set of strings will transit to the $j$th node in the next layer. Moreover, the same string does not appear twice in the a row. and the set of all strings in a row of $M$ is exactly the set of all strings &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^m&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Recall our goal is to construct a pseudorandom generator for the above ROBP. This is equivalent to we want that the final matrix $M_{prg}^r$ obtained by feeding the ROBP with pseudorandom bits is “close” to the final matrix $M^r$ obtained by feeding with uniformly random bits.&lt;/p&gt;

&lt;p&gt;With this in mind we now present some techniques we need in order to analyze Nisan’s PRG (which we will present later).&lt;/p&gt;

&lt;h2 id=&quot;2--preliminaries&quot;&gt;2. &lt;a name=&quot;pre&quot;&gt;&lt;/a&gt; Preliminaries&lt;/h2&gt;

&lt;p&gt;For convenience, we set $d = 2^s$. We then introduce more notations. Let $[d]$ be the set of all nodes in a layer and &lt;script type=&quot;math/tex&quot;&gt;Q: [d]\times \{0,1\}^m\rightarrow [d]&lt;/script&gt; be the transition function for two consecutive layers’s computation. &lt;script type=&quot;math/tex&quot;&gt;Q^r:[d]\times \{0,1\}^{m\cdot r}\rightarrow [d]&lt;/script&gt; is the transition function for $r$ consecutive layers. Then we use $Q[x]$ to represent the behavior of $Q$ on &lt;script type=&quot;math/tex&quot;&gt;x \in \{0,1\}^m&lt;/script&gt;, and the corresponding matrix is denoted as $M[x]$.&lt;/p&gt;

&lt;p&gt;Let $Q_{i,j}$ be the subset of strings in &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^m&lt;/script&gt; that maps node $i$ to node $j$ in the next layer. Similarly, let $Q^r_{i,j}$ be the subset of strings in &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^{m\cdot r}&lt;/script&gt; which can be viewed as paths from node $i$ in the first layer to node $j$ in the last layer. Clearly, $M^r_{i,j}=\frac{\lvert Q^r_{i,j} \rvert}{2^{mr}}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.1&lt;/strong&gt; (Matrix Norm) Let $M$ be a $d \times d$ matrix, we use the matrix norm &lt;script type=&quot;math/tex&quot;&gt;\lvert \lvert M\rvert \rvert&lt;/script&gt; which is the largest row sum of $M$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert \lvert M\rvert \rvert = \max_{i} \sum_j \lvert M_{i, j} \rvert&lt;/script&gt;

&lt;p&gt;The following are some standard facts:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition 2.2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For matrices $M, N \in R^{d}\times R^d$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert \lvert M+N\rvert \rvert \leq \lvert \lvert M\rvert \rvert  + \lvert \lvert N\rvert \rvert&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert \lvert MN\rvert \rvert \leq \lvert \lvert M\rvert \rvert  \cdot  \lvert \lvert N\rvert \rvert&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proposition 2.3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If $M, M^\prime, N, N^\prime$ are substochastic matrices, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert\lvert MM^\prime - NN^\prime \rvert\rvert \leq \lvert\lvert M-N\rvert\rvert + \lvert\lvert M^\prime- N^\prime\rvert\rvert&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.4&lt;/strong&gt; (Pairwise Independent Hashing Family) A family of hashing functions &lt;script type=&quot;math/tex&quot;&gt;H:\{0,1\}^m\rightarrow \{0,1\}^n&lt;/script&gt; is called pairwise independent if for any &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2 \in \{0,1\}^m&lt;/script&gt; and $x_1 \neq x_2$, &lt;script type=&quot;math/tex&quot;&gt;y_1, y_2\in \{0,1\}^n&lt;/script&gt; , when &lt;script type=&quot;math/tex&quot;&gt;h:\{0,1\}^m\rightarrow \{0,1\}^n&lt;/script&gt; is chosen randomly from $H$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{h\in H}\left [\ h(x_1) = y_1 \land h(x_2) = y_2 \right ] = \frac{1}{2^{2n}}&lt;/script&gt;

&lt;p&gt;An explicit construction of such a hashing family is known to exist and can be encoded using $m+n$ bits. Moreover, it can be computed using $O(m)$ space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.5&lt;/strong&gt; Let $A, B\subseteq {0,1}^m$, $h:{0,1}^m \rightarrow {0,1}^m$. We say a hash function is &lt;em&gt;$\epsilon$-independent for $A$, $B$&lt;/em&gt; if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left \lvert \Pr_x \left[ x\in A \land h(x)\in B \right] - \frac{\lvert A\rvert \lvert B \rvert}{2^{2m}} \right \rvert &lt; \epsilon %]]&gt;&lt;/script&gt;

&lt;p&gt;From now on we denote $\frac{\lvert A\rvert}{2^m}$ as $\alpha$ and $\frac{\lvert B\rvert}{2^m}$ as $\beta$.&lt;/p&gt;

&lt;p&gt;We now prove a mixing lemma which roughtly says that the functions from a pairwise independent hashing family are almost all $\epsilon$-independent.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2.6&lt;/strong&gt; (Mixing Lemma) Fix any $A$ ,$B$, if &lt;script type=&quot;math/tex&quot;&gt;h:\{0,1\}^m\rightarrow \{0,1\}^m&lt;/script&gt; is chosen randomly from a pairwise independent hashing family $H$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{h\in H} \left[ \text{h is not } \epsilon \text{-independent} \right]\leq \frac{\alpha\beta/\epsilon^2}{2^m}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For any $h$, let $X_i$ be the indicator variable that $h(x_i)\in B$, and $X= \sum_{i\in A}X_i$. $h$ is  $\epsilon$-independent iff &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\lvert\frac{X}{2^m}-\alpha\beta\rvert &lt; \epsilon %]]&gt;&lt;/script&gt;. Consider the expectation $E[\frac{X}{2^m}] = \frac{1}{2^m}E[X]$.  By linearity of expectation, $ E[\frac{X}{2^m}] = \frac{1}{2^m}\sum_{i\in A} E[X_i] =  \frac{1}{2^m}\cdot \beta \lvert A\rvert = \alpha\beta$. Since $X_i$’s are pairwise independent, by Chebyshev inequality we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr\left[ \lvert\frac{X}{2^m}-\alpha\beta\rvert \geq \epsilon \right ]\leq \frac{Var[X]}{2^{2m}\cdot \epsilon^2} \leq\frac{\sum_{i\in A}Var[X_i]}{2^{2m}\cdot \epsilon^2} \leq \frac{\lvert A\rvert\cdot \beta(1-\beta)}{2^{2m}\cdot \epsilon^2}\leq \frac{\alpha\beta}{2^m\cdot \epsilon^2}&lt;/script&gt;

&lt;h2 id=&quot;-3-nisans-pseudorandom-generator&quot;&gt;&lt;a name=&quot;prg&quot;&gt;&lt;/a&gt; 3. Nisan’s Pseudorandom Generator&lt;/h2&gt;

&lt;p&gt;Nisan’s generator is constructed by using random hash functions from pairwise independent hash family. &lt;br /&gt;
Consider the matrix $M^2[x_1,x_2]$ of $Q^2[x_1, x_2]$ where $x_1x_2$ is chosen uniformly at random, we first show that if we choose $h_1$ randomly from $H$, then $M^2(x_1, h_1(x_1)$ of is close to $M^2(x_1,x_2)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 3.1&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr \left[ \lvert\lvert M^2[x_1, h_1(x_1)] -M^2[x_1,x_2]\rvert\rvert \geq \epsilon \right ] \leq \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Consider the $(i, j)$-th entry of $M^2[x_1, h_1(x_1)] -M^2(x_1,x_2)$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \left\lvert M^2[x_1, h_1(x_1)]_{i,j} -M^2[x_1,x_2]_{i,j}\right\rvert &amp;= \lvert \sum_p (M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - M[x_1]_{i,p}\cdot M[x_2]_{p, j})\rvert\\
  &amp;\leq \sum_p \left\lvert M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - \alpha\beta \right\rvert\\
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Inparticular, if $h$ is $\epsilon/d^2$-independent for all $Q_{i,p}$ and $Q_{p, j}$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\sum_p \left\lvert M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - \alpha\beta \right\rvert &lt; \epsilon/d %]]&gt;&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\lvert\lvert M^2[x_1, h_1(x_1)] -M^2[x_1,x_2]\rvert\rvert &lt;\epsilon %]]&gt;&lt;/script&gt;

&lt;p&gt;With Lemma 2.6, if we want this holds for all triples $(i, p, j)$, a union bound gives us that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr \left[ \lvert\lvert M^2[x_1, h_1(x_1)] -M^2(x_1,x_2)\rvert\rvert \geq \epsilon \right ] \leq \frac{\alpha\beta\cdot d^4 \cdot d^3}{2^m \epsilon^2} = \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}&lt;/script&gt;

&lt;p&gt;Now we have a lemma for the case $r=2$, we can do it by repeatedly squaring to prove a lemma for the case $r$. For the next steps, every time we pick a new hash function from the above family, we will generate our pseudorandom bits by compositing these picked hash functions. For example, when we pick $h_2$, then new pseudorandom bits are generated by concatenating $h_2(x)$ and $h_2(h_1(x))$. From now on we will denote the new matrix as $M^{2^r}[x; h_1,..\ldots h_r]$ and the matrix we feed with uniformly random bits $x_1, \ldots x_{2^r}$ is denoted simply as $M^{2^r}$. Let’s do some more steps:&lt;/p&gt;

&lt;p&gt;when $r=4$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \lvert\lvert M^4[x, h_1, h_2]_{i,j} -M^4_{i,j}\rvert\rvert &amp;= \lvert\lvert M^4[x, h_1, h_2]_{i,j}-(M^2[x, h_1])^2 + (M^2[x, h_1])^2 - M^4\rvert\rvert\\
  &amp;\leq \lvert\lvert M^4[x, h_1, h_2]_{i,j}-(M^2[x, h_1])^2\rvert\rvert + \lvert\lvert (M^2[x, h_1])^2 - M^4\rvert\rvert\\
  &amp;\leq \epsilon + 2\cdot \epsilon\\
  &amp;\leq 3\epsilon
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;when $r=8$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \lvert\lvert M^8[x, h_1, h_2, h_3]_{i,j} -M^8_{i,j}\rvert\rvert &amp;= \lvert\lvert M^8[x, h_1, h_2, h_3]_{i,j}-(M^4[x, h_1, h_2])^2 + (M^4[x, h_1, h_2])^2 - M^8\rvert\rvert\\
  &amp;\leq \epsilon + 2\cdot 3\epsilon\\
  &amp;\leq 7\epsilon
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The recursive relation for error is $\epsilon(r)=\epsilon + 2\cdot \epsilon(\log r)$.  In general, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon(r) = (r-1)\epsilon&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3.2&lt;/strong&gt; If we pick $h_1,\ldots ,h_{\log r}$ uniformly random from the pairwise independent hash family, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{h_1,\ldots ,h_{\log r}}\left[\lvert\lvert M^r[h_1,\ldots ,h_{\log r}] - M^r\rvert\rvert \geq (r-1)\epsilon \right]\leq (\log r) \cdot \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}&lt;/script&gt;

&lt;p&gt;With these we’re ready to present Nisan’s pseudorandom generator.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 3.3&lt;/strong&gt; (&lt;em&gt;Pseudorandom Generator for Matirx Exponentiation&lt;/em&gt;) A pseudorandom generator $G$ is $\epsilon$-pseudorandom for $M^r$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert M^r[G(x)]_{i,j} - M^r[x_1 \ldots x_r]_{i,j}\rvert \leq \epsilon&lt;/script&gt;

&lt;p&gt;Nisan’s generator is thus constructed by compositing these randomly chosen hash functions as we described above. For &lt;script type=&quot;math/tex&quot;&gt;x\in \{0,1\}^m&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{h_1,\ldots ,h_{\log r}}(x) = x,h_1(x), h_2(x), h_2(h_1(x)), \ldots, h_{\log r}(x),\ldots h_{\log r}(h_{\log r-1}(\ldots h_1(x)))&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3.4&lt;/strong&gt; (Nisan) For $d, m$ and $r&amp;lt;d$, there is a pseudorandom generator $G$ that is $1/d$-pseudorandom for $M^r$. Moreover, the generator takes a seed of length $O(m\log r)$ and produces a sequence of pseudorandom bits block by block for a total length of $m\cdot r$. Finally, the outputs can be computed in space $O(m+\log r)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Theorem 3.2, let’s set $\epsilon = 1/2dr$ and $m = \Theta(\log d) = C\log d$ for some large $C$. Therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert\lvert M^r[h_1,\ldots ,h_{\log r}] - M^r\rvert\rvert \leq 1/2d&lt;/script&gt;

&lt;p&gt;except with probability at most $\frac{d^9 r^2 \log r}{d^C} = O(1/d)$ for large $C$. While in the later case, the difference is bounded to be at most $1\cdot O(1/d) = 1/2d$.&lt;/p&gt;

&lt;p&gt;Finally, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left \lvert\Pr_{x, h_1, \ldots ,h_{\log r}}\left[ Q(i; G(x)) = j\right ]- \Pr_{x_1, \ldots , x_{2^r}}\left[ Q(i; x_1, \ldots , x^{2^r}) = j\right ]\right \lvert \leq 1/d&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>BPL and Fooling BPL</title>
   <link href="http://localhost:4000/derandbpl/"/>
   <updated>2019-11-03T00:00:00+08:00</updated>
   <id>http://localhost:4000/derandlogspace</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#bpl&quot;&gt;Log-space computation and BPL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#robp&quot;&gt;ROBP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fool&quot;&gt;Fooling BPL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Starting from this post I will discuss about the $BPL\ vs.\ L$ problem and write notes about some known, important result. This post is an introduction to the model.&lt;/p&gt;

&lt;h2 id=&quot;-1-log-space-computation-and-the-class-bpl&quot;&gt;&lt;a name=&quot;bpl&quot;&gt;&lt;/a&gt; 1. Log-space Computation and the class BPL&lt;/h2&gt;

&lt;p&gt;We are interested in space-bounded probabilistic computation, specifically, on Turing machines that have logarithmic working space. We start by describing such machines.&lt;/p&gt;
&lt;h3 id=&quot;log-space-probabilistic-turing-machine&quot;&gt;Log-space Probabilistic Turing Machine&lt;/h3&gt;

&lt;p&gt;A log-space probabilistic turing machine(PTM) consists of the following parts:  a two-way read-only input tape where we assume that the input is of size $n$; a normal read-write working tape of size $O(\log n)$, a finite states control that has a finite number of states, a output tape that is write-only. In addition, the machine has access to infinite randomness. That is, the machine is allowed to flip coins to decide the next step. We model this by giving it access to an infinite one-way random tape, it reads the next random bit when it is needed. Note that it must write the random bit down (on the work tape) if it wants to retrieve it later.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/L-PTM.jpg&quot; alt=&quot;log-space PTM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the execution of this TM, a &lt;em&gt;configuration&lt;/em&gt; consists of the position of read head on the input tape plus the contents on it, the position of read-write head on the working tape plus the contents, the specific state on the finite states control. We should see that the total number of configurations is $O(\log n) \cdot 2^{O(\log n)}$(# of possible work tape configuration) $\cdot \lvert \Delta \rvert$(# of states which is finite) $\cdot n$ (# of possible input tape configuration) $=2^{O(\log n)}$.&lt;/p&gt;

&lt;h3 id=&quot;bpl&quot;&gt;BPL&lt;/h3&gt;

&lt;p&gt;$BPL$ can be written as $BP_HL$ or $BP_HSPACE(\log n)$. It refers to &lt;em&gt;Bounded-error Probabilistic Log-space that halts absolutely&lt;/em&gt; (hence the sub “H”). Formally, we say a language $L$ is in $BPL$ if there is a log-space PTM recognizes it and satisfies the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;it always halts.&lt;/li&gt;
  &lt;li&gt;if $x\in L$, it accepts with probability at least 2/3.&lt;/li&gt;
  &lt;li&gt;if $x\not\in L$, it rejects with probability at least 2/3.&lt;/li&gt;
  &lt;li&gt;the probability is taken over the coin flips of the PTM.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We emphasize that in the definition of $BPL$ the PTM halts absolutely, which is the case we’re interested in. For space-bounded computation there are other PTMs(e.g. PTM that halts almost surely) which we will not discuss here. Interested reader should refer to the survey [1]. We now state some observations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The PTM runs in time at most $2^{O(\log n)}$, which is the total number of configurations. Otherwise the PTM falls in loop and may never halt.&lt;/li&gt;
  &lt;li&gt;Since it runs in time at most $2^{O(\log n)}$, it uses at most $2^{O(\log n)}$ random bits.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In fact, restricting a PTM to use at most $2^{O(\log n)}$ randomness is “equivalent” to it halts absolutely. Consider the following: the PTM uses space $O(\log n)$ to maintain a counter, every time the PTM flips a coin, the counter is reset to 0, if the counter reaches $2^{O(\log n)}$, then it halts and rejects. It can be seen that this halting PTM recognizes the same language.&lt;/p&gt;

&lt;h2 id=&quot;-2-read-once-branching-program&quot;&gt;&lt;a name=&quot;robp&quot;&gt;&lt;/a&gt; 2. Read-once Branching Program&lt;/h2&gt;

&lt;p&gt;In this section we introduce a (non-uniform) computation model that simulates space-bounded computation. A branching program (in our interest) is a layed acyclic graph, each layer has a total number of $2^{O(\log n)}$ nodes. For our purpose, you should think of each of these nodes as a configuration in the execution of the above BPL machine on a given input, so each layer has the same nodes and, since it’s halting, the same node won’t be entered twice. For each input, there is a starting node in the first layer that represents the starting configuration. In the last layer there are only 2 nodes, one represents the “accept” state and the other represents the “reject” state. Between the $i$th and $i+1$th layer there are transition edges mapping from nodes in the $i$th layer to the nodes in the $i+1$th layer. Each of these edges is labeled a string &lt;script type=&quot;math/tex&quot;&gt;r_i \in\{0,1\}^m&lt;/script&gt;. You should think of this string as the $m$ coin flips tossed by the machine to decide which configuration to enter in the next step, and it’s an exercise to see that this is equivalent to repeating $m$ times the process of tossing one coin then perform some deterministic computation . A computation on a branching program with input $r_n$ is just a path from a node in the first layer to the accepting/rejecting node in the last layer. Among the intermediate layers it reads $r_i$s and chooses the edges to traverse layer by layer. We say a branching program is a ROBP (Read Once Branching Program) if it reads its input in a one-way fashion.&lt;/p&gt;

&lt;p&gt;For a computation on the BPL machine with input $x$, you can think of a ROBP simulates the computation by:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;pick a node in the first layer as starting node according to the input $x$;&lt;/li&gt;
  &lt;li&gt;read the sequence of random bits (at most $2^{O(\log n)}$) on the original random tape as input and traverses the graph accordingly.&lt;/li&gt;
  &lt;li&gt;note that it reads the input in a streaming (one-way) fashion.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The picture below represents a branching program using $r_n$ randomness and has $2^{O(\log n)}$ nodes in each layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ROBP1.jpg&quot; alt=&quot;ROBP&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For convenience, we denote the &lt;em&gt;width&lt;/em&gt; of the (RO)BP as the maximum number of nodes in a layer of it. We denote the &lt;em&gt;length&lt;/em&gt; of the (RO)BP as the number of layers in it.&lt;/p&gt;

&lt;h2 id=&quot;-3-fooling-bpl&quot;&gt;&lt;a name=&quot;fool&quot;&gt;&lt;/a&gt; 3. Fooling BPL&lt;/h2&gt;

&lt;p&gt;From the previous sections, we can see that to fool every language in BPL, it suffices to fool all ROBPs that have width at most $2^{O(\log n)}$ and length at most $2^{O(\log n)}$. We formally define what we mean by fooling a ROBP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 3.1&lt;/strong&gt; For a ROBP &lt;script type=&quot;math/tex&quot;&gt;Q: \{0,1\}^{n} \rightarrow \{0,1\}&lt;/script&gt; and $r&amp;lt;n = 2^{O(\log n)}$, we say a pseudorandom generator &lt;script type=&quot;math/tex&quot;&gt;G: \{0,1\}^{r} \rightarrow \{0,1\}^{n}&lt;/script&gt; $\epsilon$-fools $B$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\lvert \Pr \left[ Q(G(U_{r}))=1\right] - \Pr\left[Q(U_n)=1 \right]\rvert &lt; \epsilon %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] Michael Saks, “Randomization and Derandomization in Space-Bounded
Computation” http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.1185&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Goldreich Levin Theorem</title>
   <link href="http://localhost:4000/glthm/"/>
   <updated>2019-10-04T00:00:00+08:00</updated>
   <id>http://localhost:4000/glthm</id>
   <content type="html">&lt;p&gt;In this blog we study and present the proof of Goldreich-Levin theorem. The main material we use is the  course notes of Prof. Luca Trevisan (see references at the end).&lt;/p&gt;

&lt;p&gt;The Goldreich-Levin Theorem has several interpretations. We will first explain and prove it from cryptographic view. Then we will see in fact it can also be viewed as list-decoding of Hadamard code.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;In this section we present necessary definitions and math inequalities we will be using. The first theorem is a variant of Markov inequality.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1 (Markov Inequality, Variant)&lt;/strong&gt; Suppose $X$ is a random variable in $[0, 1]$ and $0&amp;lt;t&amp;lt;E[X]$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr[X\geq t] \geq \frac{E[X]-t}{1-t}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let $S$ denote the set of $x$ such that $X(x)\geq t$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} 
E[X] &amp;= \sum_{x\in S}Pr(x)X(x) + \sum_{x\not\in S}Pr(x)X(x)\\ 
 &amp;\leq \sum_{x\in S}Pr(x)\cdot 1 + \sum_{x\not\in S}Pr(x)\cdot t\\
 &amp;= Pr(S) + t\cdot (1-Pr(S))\\
 &amp;= (1-t)\cdot Pr(S) + t
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2 (Chernoff Bound)&lt;/strong&gt;  Suppose $X_1, \ldots, X_k$ are $0, 1$ &lt;em&gt;i.i.d&lt;/em&gt; random variables and $ X = \sum_i^{k} X_i$, then for any $0&amp;lt;\epsilon&amp;lt;1$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Pr[\ X&gt;(1+\epsilon)E[X]\ ]&lt;e^{-\frac{\epsilon^2}{3}\cdot E[X]} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Pr[\ X&lt;(1-\epsilon)E[X]\ ]&lt;e^{-\frac{\epsilon^2}{3}\cdot E[X]} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3 (Chebyshev Inequality)&lt;/strong&gt;&lt;br /&gt;
Suppose $X= X_1 + \cdots + X_k$ where $X_1, \ldots ,X_k$ are $0, 1$ pairwise independent variables and $t&amp;gt;0$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr[\mid X-E[X]\mid \geq t] \leq \frac{Var(X_1) + \cdots + Var(X_k)}{t^2}&lt;/script&gt;

&lt;p&gt;In particular, if for each $X_i$, $Pr[X_i = 1] \geq 1/2 + \epsilon$, then $Var(X_i)&amp;lt;\frac{1}{4}$, therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr[\mid X-E[X]\mid \geq t] \leq \frac{Var(X_1) + \cdots + Var(X_k)}{t^2}\leq \frac{k}{4t^2}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let $S$ be the set of $x$ such that $\mid X(x)-E[X]\mid \geq t$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} 
Var[X] &amp;= \sum_x (X(x)-E[X])^2 \cdot Pr(x)\\
 &amp;= \sum_{x\in S}(X(x)-E[X])^2 \cdot Pr(x) + \sum_{x\not\in S}(X(x)-E[X])^2 \cdot Pr(x)\\
 &amp;\geq Pr(S)\cdot t^2 + 0
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr(S) \leq \frac{Var[X]}{t^2}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} 
Var[X_1+X_2] &amp;= E[(X_1+X_2)^2] - E(X_1+X_2)^2\\
&amp;= E[X_1^2] + 2E[X_1X_2] + E[X_2]^2 - E[X_1]^2 -2E[X_1]E[X_2] - E[X_2]^2\\
&amp;= Var[X_1]+Var[X_2]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;this can be easily generalized to show $Var[X_1 + \cdots + X_k]= Var[X_1] + \cdots + Var[X_k]$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 1 (Hard-Core Predicate)&lt;/strong&gt; A boolean function &lt;script type=&quot;math/tex&quot;&gt;P: \{0, 1 \}^n \rightarrow \{0, 1\}&lt;/script&gt; is $(t, \epsilon)$ - hard core for a permutation &lt;script type=&quot;math/tex&quot;&gt;f: \{0,1\}^n \rightarrow \{0,1\}^n&lt;/script&gt; if for every algorithm $A$ of complexity $t$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{x\sim \{0, 1\}^n}[A(f(x)) = P(x)]\leq \frac{1}{2}+\epsilon&lt;/script&gt;

&lt;p&gt;Note: we use &lt;script type=&quot;math/tex&quot;&gt;x\sim \{0, 1\}^n&lt;/script&gt; to denote uniform distribution over &lt;script type=&quot;math/tex&quot;&gt;\{0, 1\}^n&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;goldreich-levin-theorem&quot;&gt;Goldreich-Levin Theorem&lt;/h2&gt;

&lt;p&gt;We denote inner product modulo 2 using the following notation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle x,r\rangle = \sum_i x_i\cdot r_i\ mod\ 2&lt;/script&gt;

&lt;p&gt;The Goldreich-Levin theorem says that a random XOR is hard-core for every one-way permutation. This means that if there is an efficient algorithm to predict $\langle x, r\rangle$, given $f(x)$ and $r$, then there is also an efficient algorithm to compute a pre-image of $f(x)$, so if $f(x)$ is a one-way function, we can invert it with noticeable probability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1 (Goldreich-Levin Theorem)&lt;/strong&gt; &lt;em&gt;Suppose $A$ is an algorithm of complexity $t$ such that&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle] \geq \frac{1}{2} + \epsilon&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Then there is an algorithm $A^\prime$ of complexity at most $O(t\epsilon^{-2}n^{O(1)})$ such that&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_x[A^\prime(f(x)) = x] \geq \Omega(\epsilon)&lt;/script&gt;

&lt;p&gt;We will first prove a weak Goldreich-Levin algorithm, which will later be used in our proof of GL theorem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2 (Goldreich-Levin Algorithm Weak Version)&lt;/strong&gt; &lt;em&gt;Suppose there is a function $H$ such that, for some $x$&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_r[H(r)\ =\ \langle x,r\rangle]\ \geq \frac{3}{4} + \epsilon&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Then there is an algorithm $GLW$ that can output $x$ with high probability ( $\geq 1-\frac{1}{n}$ ).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let’s first see how an easy algorithm can find $x$ when the probability on the right side is 1 in the above (so that $H(r)$ outputs $\langle x,r\rangle$ correctly).&lt;/p&gt;

&lt;p&gt;Let $e_i$ denote the vector in which the $i$th bit is 1 and 0 otherwise. $x_i = H(e_i) = \langle x, e_i \rangle$. Then we can output $x$ by enumerating all the $e_i$.&lt;/p&gt;

&lt;p&gt;Now to prove Theorem 2, note that $H(r)$ fails with small probability, we’ll use the majority vote method. Intuitively,  we randomly sample several points $r_1 \ldots r_k$. Let $H^\prime_{r_1 \ldots r_k}(e_i)$ be the function that take the majority of  $H(r_j+e_i) - H(r_j)$ on these $r_j$s. The observation is that if $H(r)$ computes $\langle x, r\rangle$ correctly, then we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle x, r_j+e_i \rangle - \langle x, r_j \rangle = \langle x, e_i\rangle&lt;/script&gt;

&lt;p&gt;We now bound the probability that $H(r_j + e_i) - H(r_j)$ fails.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  Pr[H(r_j + e_i) - H(r_i) \neq  \langle x, e_i\rangle] &amp;= Pr[H(r_j + e_i) \neq \langle x, r_j+e_i \rangle\ \cup H(r_j) \neq \langle x, r_j \rangle]\\
  &amp;\leq Pr[H(r_j + e_i) \neq \langle x, r_j+e_i \rangle]\ + Pr[H(r_j) \neq \langle x, r_j \rangle]\\
  &amp;\leq \frac{1}{4} - \epsilon + \frac{1}{4} - \epsilon\\
  &amp;\leq \frac{1}{2} - 2\epsilon
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The GLW algorithm is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GLW Algorithm:&lt;/li&gt;
  &lt;li&gt;for $i= 1, 2, \ldots , n$:
    &lt;ul&gt;
      &lt;li&gt;for $j = 1, 2, \ldots , 4\log n/ \epsilon^2$:
        &lt;ul&gt;
          &lt;li&gt;randomly sample &lt;script type=&quot;math/tex&quot;&gt;r_j\in \{0, 1\}^n&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;compute $H(r_j + e_i) - H(r_j)$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;compute &lt;script type=&quot;math/tex&quot;&gt;x_i = H^\prime_{r_1 \ldots r_k}(e_i) = Majority\{H(r_j + e_i) - H(r_j): j = 1, 2, \ldots , 4\log n/ \epsilon^2 \}&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return $x$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now we analyze this algorithm. Since each $H(r_j + e_i) - H(r_j)$ outputs the correct answer with probability at least $1/2 + 2\epsilon$. By the Chernoff Bound (Theorem 2 in Background), the probability that the majority function $H^\prime_{r_1 \ldots r_k}(e_i)$ fails to output $x_i$ is at most $e^{-2\log n} = O(\frac{1}{n^2})$. Then the union bound gives us that GLW outputs $x$ correctly with probability at least $1-1/n$. The algorithm runs in time $O(\frac{n^2 \log n}{\epsilon^2})$, makes $O(\frac{n\log n}{\epsilon^2})$ queries to $H$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion.&lt;/strong&gt; The above framework only works for cases where $\Pr_r[H(r) = \langle x, r \rangle]$ greater than $\frac{3}{4}$. For the case in Goldreich-Levin theorem where $H(r)$ makes more error,the union bound gives probability less than $1/2$ and the Chernoff Bound won’t work. In fact, it is possible to construct a function $H$ such that $Pr[H(r) = \langle x, r\rangle] = 3/4$ and $Pr[H(r) = \langle x^\prime, r\rangle] = 3/4$ where $x \neq x^\prime$. So no algorithm can guarantee to find the correct $x$ when given such $H$, since $x$ is not uniquely defined by $H$. However, the Goldreich-Levin theorem tells us how to find a list of possible candidates of $x$ with good probability. We present their algorithm below.&lt;/p&gt;

&lt;p&gt;We proceed to prove the Goldreich-Levin Theorem.&lt;/p&gt;

&lt;p&gt;Given (as in the Goldreich-Levin theorem) $\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle] \geq \frac{1}{2} + \epsilon$, we first show that there is a certain fraction of $x$,  $\Pr_r[A(f(x), r) = \langle x, r \rangle] \geq \frac{1}{2} + \frac{1}{2}\epsilon$ holds, and we call such $x$ “good” :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle]&lt;/script&gt; can be viewed as &lt;script type=&quot;math/tex&quot;&gt;\sum_x Pr(x)\Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]&lt;/script&gt;,  which is equal to the expectation &lt;script type=&quot;math/tex&quot;&gt;E_x[\ \Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]\ ]&lt;/script&gt; ( so we view &lt;script type=&quot;math/tex&quot;&gt;\Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]&lt;/script&gt; as a random variable of $x$). Now we have the expectation is greater than $1/2 + \epsilon$, by the Markov ineuality (Theorem 1 in background) we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_x \left[\ \Pr[A(f(x),r)\ =\ \langle x ,r\rangle ]\geq \frac{1}{2} + \frac{1}{2}\epsilon\ \right]\geq \frac{\frac{1}{2}\epsilon}{\frac{1}{2}-\frac{1}{2}\epsilon}\geq \frac{\epsilon}{2}&lt;/script&gt;

&lt;p&gt;So at least $\frac{\epsilon}{2}$ of $x$ are good. On these good $x$, we prove the GL algorithm:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3 (Goldreich-Levin Algorithm)&lt;/strong&gt;
&lt;em&gt;Suppose there is a function $H$ such that, for some $x$:&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_r[H(r) = \langle x, r \rangle] \geq \frac{1}{2} + \epsilon&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Then there is an algorithm GL outputs a list $L$ of candidates of $x$, such that $L$ is of size $O(\epsilon^{-2})$ and $x\in L$ with probability  at least $1/2$. The GL algorithm runs in time $O(n^2 \epsilon^{-4} \log n)$, makes $O(n\epsilon^{-4}\log n)$ queries to $H$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As discussed above, we now don’t have $H(r)$ that predicts $\langle x, r \rangle$ well, so $H(r+r_j) - H(r_j)$ fails to predict $\langle x, r \rangle$ with high probability ($ &amp;gt; 1/2$). However, let’s make an assumption that, instead of $H(r_j)$, we get the correct $\langle x, r_j \rangle$ for every $j$, so now $H(r+r_j) - \langle x, r_j \rangle$ succeeds with probability at least $1/2 + \epsilon$. We do an error reduction by taking $k= O(\frac{1}{\epsilon^2})$ samples of $r_j$, then apply the majority vote method. Thus we can reduce the probability to match that in Theorem 2 and apply the GLW algorithm to find $x$.   &lt;br /&gt;
Moreover, since we dont’ know the correct $\langle x, r_j \rangle$, we just enumerate all the possible values of it, which gives the following (inefficient) algorithm:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inefficient GL algorithm:&lt;/li&gt;
  &lt;li&gt;randomly sample &lt;script type=&quot;math/tex&quot;&gt;r_1, \ldots, r_k \in \{0, 1\}^n&lt;/script&gt; where $k =  O(\frac{1}{\epsilon^2})$&lt;/li&gt;
  &lt;li&gt;for &lt;script type=&quot;math/tex&quot;&gt;b_1, \ldots, b_k \in \{0, 1\}&lt;/script&gt; (each $b_j$ corresponds to a possible value of $\langle x, r_j \rangle$) :
    &lt;ul&gt;
      &lt;li&gt;let &lt;script type=&quot;math/tex&quot;&gt;H^\prime_{r_1 \ldots r_k}(r) :=  Majority\{H(r_j + r) - b_j: j = 1, 2, \ldots, k \}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;run GLW algorithm in Theorem 2, using $H^\prime_{r_1 \ldots r_k}(r)$ instead of $H(r)$, outputs $x$&lt;/li&gt;
      &lt;li&gt;add $x$ to list $L$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return $L$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Analysis:&lt;/em&gt;  By the Chernoff Bound we have, on the correct $b_j$’s,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{r,r_1,\ldots, r_k}[H^\prime_{r_1 \ldots r_k}(r) = \langle x, r\rangle]\geq 1-o(1)\geq 1-\frac{1}{32}&lt;/script&gt;

&lt;p&gt;It follow by the Markov Inequality that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{r_1, \ldots, r_k}[\Pr_r[H^\prime_{r_1 \ldots r_k}(r) = \langle x, r\rangle] &gt; \frac{3}{4}]&gt; \frac{31}{32}&lt;/script&gt;

&lt;p&gt;By the GLW algorithm, with probability at least $\frac{31}{32} - \frac{1}{n} &amp;gt; \frac{1}{2}$, $x$ is in the list.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion.&lt;/strong&gt; The above algorithm is inefficient in that the size of $L$ is $2^k$, which is exponential in $k$, the GL algorithm below reduces this using pairwise independence.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GL algorithm:&lt;/li&gt;
  &lt;li&gt;Randomly sample &lt;script type=&quot;math/tex&quot;&gt;r_1, \ldots, r_k \in \{0, 1\}^n&lt;/script&gt; where $k =  O(\log \frac{1}{\epsilon^2})$&lt;/li&gt;
  &lt;li&gt;for each subset &lt;script type=&quot;math/tex&quot;&gt;S\subseteq \{r_1, \ldots, r_k\}&lt;/script&gt;, define $r_S = \sum_{i\in S} r_i$&lt;/li&gt;
  &lt;li&gt;for all &lt;script type=&quot;math/tex&quot;&gt;b_1 \ldots, b_k \in \{0, 1\}&lt;/script&gt;:
    &lt;ul&gt;
      &lt;li&gt;define $b_S = \sum_{i\in S} b_i$&lt;/li&gt;
      &lt;li&gt;define &lt;script type=&quot;math/tex&quot;&gt;H^\prime(r) :=  Majority\{H(r_S + r) - b_S: S\subseteq \{r_1, \ldots, r_k\} \}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;run GLW algorithm in Theorem 2, using $H^\prime(r)$ instead of $H(r)$, outputs $x$&lt;/li&gt;
      &lt;li&gt;add $x$ to list $L$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return $L$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now we analyze this GL algorithm. Since $r_1, \ldots, r_k$ are randomly picked, for any two different sets $S$ and $T$, $r_S$ and $r_T$ are pairwise independent, therefore $H(r_S+r) - b_S$ and $H(r_T+r) - b_T$ are pairwise independent. In addition, on the correct $b_S$, $H(r_S+r) - b_S = \langle x, r \rangle$ with probability at least $1/2 + \epsilon$. By the Chebyshev Ineuqality (Theorem 3 in background), $H^\prime(r)$ outputs the wrong value with probability at most $\frac{1}{4\epsilon^2 \cdot 2^{k}}$ where $k=O(\log \frac{8}{\epsilon^2})$, so the probability is at most $\frac{1}{32}$. Given such $H^\prime(r)$ the GLW algorithm will work well. &lt;br /&gt;
The running time of GL algorithm is $O(n^2 \log n \cdot \epsilon^{-4})$, it makes$O(n^2\log n\cdot \epsilon^{-4})$ queries to $H$ and outputs a list $L$ of size $O(\frac{1}{\epsilon^2})$. $x$ is in $L$ with probability greater than $1/2$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finalizing the proof.&lt;/strong&gt; Since we have at least $\epsilon/2$ fraction of good $x$, combinging this with the GL algorithm we can conclude that with probability at least $\epsilon /4$ $x$ is in the list $L$. We can check $x$ by evaluating $f(x)$ to see if it equals our input $y$.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;Lecture Notes 11, 12 of Luca Trevisan’s Spring 2009 Cryptography Courses, see this &lt;a href=&quot;https://people.eecs.berkeley.edu/~luca/cs276/#notes&quot;&gt;Link&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 

</feed>
