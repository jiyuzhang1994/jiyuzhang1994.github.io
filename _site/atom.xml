<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Jiyu Zhang</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2019-12-02T16:10:34+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Mark Otto</name>
   <email></email>
 </author>

 
 <entry>
   <title>Derandomize BPL:BPL is in L^{3/2}</title>
   <link href="http://localhost:4000/sakszhou/"/>
   <updated>2019-11-20T00:00:00+08:00</updated>
   <id>http://localhost:4000/sakszhou</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#algo&quot;&gt;An Algorithm for Estimating Success Probability via Nisan’s PRG&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sakszhou&quot;&gt;Saks and Zhou’s Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given a ROBP of our interest, there is a trivial algorithm to compute the $(i,j)$-th entry of one layer’s transition matrix $M$: for input $(i,j)$, simply enumerates all transition strings &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^m&lt;/script&gt; and counts the number of strings that lead to $j$, then divide this number by $2^m$. This algorithm uses space $O(d+m)$.&lt;/p&gt;

&lt;p&gt;To derandomize $BPL$, it’s enough to show a deterministic algorithm that runs in small space and estimates the $(i, j)$-th  entry with error $1/d$ in the matrix $M^{2^r}$. We say this algorithm approximates $M^{2^r}$.&lt;/p&gt;

&lt;p&gt;For reasons above, from now on we phrase everything in terms of matrix exponentiation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fact.&lt;/strong&gt; The repeated squaring algorithm computes exact $M^{2^r}$ with space $O(rd)$.&lt;/p&gt;

&lt;p&gt;From Nisan’s generator, we should see that it implicitly gives an algorithm for approximating matrix exponentiation. Let’s show the algorithm below.&lt;/p&gt;

&lt;h2 id=&quot;-1-an-algorithm-for-approximating-matrix-exponentiation-via-nisans-prg&quot;&gt;&lt;a name=&quot;algo&quot;&gt;&lt;/a&gt; 1. An Algorithm for Approximating Matrix Exponentiation via Nisan’s PRG&lt;/h2&gt;

&lt;p&gt;The algorithm starts by randomly picking a set of hash functions and store these functions, we call the space we need for this purpose the &lt;em&gt;random bit complexity&lt;/em&gt;. We think of this step as the outer layer of this algorithm. It then starts to feed the pseudorandom bits, produced by computing these hash functions, to the ROBP and computes using the trivial counting algorithm. We call the space for this purpose the &lt;em&gt;processing space complexity&lt;/em&gt;. We think of this step as the inner layer (or subroutine) of the algorithm. The following algorithm from [1] captures the subroutine for processing matrices. Saks and Zhou’s algorithm is a variant of this subroutine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 4.1&lt;/strong&gt; (Saks and Zhou; Nisan) For a $d\times d$ matrix $M$ and integers $r, m$, there is an algorithm PRS(M, r, m; h) (meaning &lt;em&gt;Pseudorandom Repeated Squaring&lt;/em&gt;) that takes a random string &lt;script type=&quot;math/tex&quot;&gt;h\in \{0,1\}^{2m\cdot r}&lt;/script&gt;, runs in space $O(m+r+\log d)$ and, if $m= O(\log d)$, approximates the matrix $M^{2^r}$ with error $O(1/d)$.&lt;/p&gt;

&lt;p&gt;Therefore, if we use Nisan’s generator and apply the algorithm in Lemma 4.1 in a straightforward way, we have an algorithm for deterministically simulating $BPL$ in space $O(\log n^2)$ (for random bit complexity) + $O(\log n)$ (for processing space complexity) = $O(\log n^2)$. For comparison, the recursive repeated squaring algorithm has processing space complexity $O(\log n^2)$.&lt;/p&gt;

&lt;p&gt;The main idea of Saks and Zhou for proving $BPL\subseteq L^{3/2}$ is , by combining these two algorithms (&lt;em&gt;PRS + Recursive Repeated Squaring&lt;/em&gt;) in a sophisticated way, so the final random bit complexity falls down to $O(\log n^{3/2})$ and the processing space complexity becomes $O(\log n^{3/2})$.&lt;/p&gt;

&lt;h2 id=&quot;2--saks-and-zhous-algorithm&quot;&gt;2. &lt;a name=&quot;sakszhou&quot;&gt;&lt;/a&gt; Saks and Zhou’s Algorithm&lt;/h2&gt;

&lt;p&gt;As mentioned in the last section, we gonna combine PRS and the repeated squaring algorithm in a “sophisticated” way. To illustrate this way of combining, we first define some operators for real numbers in $[0,1]$ and similarly, for matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.1&lt;/strong&gt; (&lt;em&gt;Perturbation Operator&lt;/em&gt;) a perturbation opertaor $\Sigma_\delta$ is a function mapping nonnegative real number $z\in [0,1]$ to &lt;script type=&quot;math/tex&quot;&gt;\Sigma_\delta = \max \{z-\delta, 0\}&lt;/script&gt;. The operator applies to matrices by applying it entry by entry to the matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.2&lt;/strong&gt; (&lt;em&gt;Truncation Operator&lt;/em&gt;) for a positive integer $t$, a truncation operator $\lfloor\  \rfloor_t$ is a function that, for a nonnegative real number $z$, truncating the binary expansion of $z$ after $t$ binary digits. That is, $\lfloor z\rfloor _t = 2^{-t} \lfloor 2^t z\rfloor$. Again, the operator applies to matrices by applying it entry by entry to the matrix.&lt;/p&gt;

&lt;p&gt;Some facts for applying these operators on matrices will be useful for our purpose:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition 2.3&lt;/strong&gt; For $M, N \in R^d\times R^d$,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\lvert\lvert M - \lfloor M \rfloor_t \rvert\rvert \leq d\cdot 2^{-t}$&lt;/li&gt;
  &lt;li&gt;$\lvert\lvert M - \Sigma_\delta M  \rvert\rvert \leq d\cdot \delta$&lt;/li&gt;
  &lt;li&gt;$\lvert\lvert \Sigma_\delta M - \Sigma_\delta N\rvert\rvert \leq \lvert\lvert M - N\rvert\rvert$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We now start to give an overview of Saks and Zhou’s algorithm. The idea is to apply PRS recursively, but not in the direct way.&lt;/p&gt;

&lt;p&gt;Let $A^r(M)$ be the matrix obtained by repeatedly squaring $M$ for $r$ times, i.e. $A^r(M) = M^{2^r}$. Consider $A^r(M) = A^{r_1\cdot r_2}(M) = A^{r_2}(A^{r_1}(M))$ where $r_1 \cdot r_2 = r$, if we apply PRS recursively, that is, we repeatedly compute $A^{r_1}$ for $r_2$ times. Now the random bit complexity will be $r_2\cdot r_1 m = O(rd)$ and the processing space complexity is $r_2\cdot O(d)$. Therefore the random bit complexity has not been improved, and we paid additional cost for processing space complexity.&lt;/p&gt;

&lt;p&gt;The additional idea is to feed the same random bits for each level of these PRSs, so we can reduce the random bit complexity, and if necessary we are willing to pay more processing space complexity. However it’s hard to prove that there exists such a sequence of random bits that works for every level. To present how Saks and Zhou get avoid of this obstacle, we will introduce some objects which are useful both for presenting and for analyzing.&lt;/p&gt;

&lt;p&gt;We define the &lt;strong&gt;exact repeated squaring matrices sequence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$s_1:N_0=M_0, N_1 \ldots N_{r_2}$&lt;/p&gt;

&lt;p&gt;where $N_i$ is the matrix obtained by computing $A^{r_1}(N_{i-1})= N^{2^{r_1}}_{i-1}$. The last matrix in the sequence is the one we would like to approximate.  We also define a &lt;strong&gt;pseudorandom matrices sequence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$s_2:M_0, M_1, \ldots, M_{r_2}$&lt;/p&gt;

&lt;p&gt;where $M_i = PRS(M_{i-1}, m, r_1; h)$. We will not directly show that $s_2$ approximates $s_1$ well because… it’s not known how to prove it. Instead we want to show a modified version of $s_2$ that is good for our purpose.&lt;/p&gt;

&lt;p&gt;We define the third sequence, which we call  &lt;strong&gt;pseudorandom matrices sequence SZ ver.&lt;/strong&gt; (pseudorandom matrices sequence under Saks and Zhou’s manipulation)&lt;/p&gt;

&lt;p&gt;$s_3:M_0, M^P_1, M^\Sigma_1, M_1, \ldots, M^P_{r_2}, M^\Sigma_{r_2}, M_{r_2}$&lt;/p&gt;

&lt;p&gt;where $M^P_i = PRS(M_{i-1}, r_1, m; h)$; $M^\Sigma_i = \Sigma_{\delta_i}M^P_i$; $M_i = \lfloor M^\Sigma_i \rfloor_t$.&lt;/p&gt;

&lt;p&gt;Saks and Zhou’s algorithm recursively computes $M_i$ in $s_3$.&lt;/p&gt;

&lt;p&gt;We now present Saks and Zhou’s algorithm which we denote as SZ for computing sequence $M_{r_2}$ in $s_3$, and will prove its correctness in the next section.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Algorithm SZ:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;br /&gt;
a $d\times d$ substochastic matrix $M$, intgers $r$ and $a$, indices $u, v\in [d]$.&lt;/p&gt;

&lt;p&gt;Then the parameters $m, D, K, t= K-D$ are computed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Randomn input (stored in the outer layer as random bit complexity)&lt;/strong&gt;&lt;br /&gt;
$h \in ({0,1}^{2m})^{r_2}$ and $q\in ({0,1}^{D})^{r_2}$&lt;/p&gt;

&lt;p&gt;The algorithm SZ then recursively computes $M_i[u,v]$ in $s_3$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;br /&gt;
$M_{r_2}[u,v]$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;For the rest of this post we’ll prove the following theorem&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2.4&lt;/strong&gt;  The algorithm SZ approximates $A^r(M)$ with error $K-D-2r-\log d$ except with probability $\frac{2^{r+ 2\log d}}{2^m(2^D + 2^{2K+4r+5\log d})}$&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Michael Saks and Shiyu Zhou, “BPHSPACE(S) ⊆ DSPACE(S^3/2)”. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.225.8850&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Nisan's Pseudorandom Generator for Space-Bounded Computation</title>
   <link href="http://localhost:4000/nisanprg/"/>
   <updated>2019-11-06T00:00:00+08:00</updated>
   <id>http://localhost:4000/nisanprg</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#robp&quot;&gt;ROBP and Matrix Exponentiation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pre&quot;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nisan&quot;&gt;Nisan’s Pseudorandom Generator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post we present Nisan’s pseudorandom generator (PRG). In the next post, we will see how Saks and Zhou use this PRG to prove that $BPL\subseteq L^{3/2}$.&lt;/p&gt;

&lt;h2 id=&quot;-1-robp-and-matrix-exponentiation&quot;&gt;&lt;a name=&quot;robp&quot;&gt;&lt;/a&gt; 1. ROBP and Matrix Exponentiation&lt;/h2&gt;

&lt;p&gt;Recall that we are interested in the ROBP where each layer of it has the same set of nodes, and the set is of size $2^s$. Each node in a layer has $2^m$ out edges each labeled with a binary string of length $m$ and maps to a node in the next layer. We can visualize the computation on a ROBP as following:  we associate the transition from the $i$th layer to the $i+1$th layer with a $2^s\times 2^s$ transition matrix $M$ which is substochastic. Therefore each time the ROBP makes a transition, the previous product is multiplied with the same $M$. After $r$ times transition the resulting matrix is denoted as $M^r$. The entry $M^r_{i, j}$ is the probability that the ROBP starts at node $i$ and ends at node $j$ after $r$ transitions when we choose input uniformly at random. The randomness is taken over the possible inputs to this ROBP.&lt;/p&gt;

&lt;p&gt;Let’s examine some more properties about the matrix $M$. For each entry in $M$, say $M_{i,j}$, we can associate it with a subset of strings in &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^m&lt;/script&gt;, which means that the the $i$th node given one of this set of strings will transit to the $j$th node in the next layer. Moreover, the same string does not appear twice in the a row. and the set of all strings in a row of $M$ is exactly the set of all strings &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^m&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Recall our goal is to construct a pseudorandom generator for the above ROBP. This is equivalent to we want that the final matrix $M_{prg}^r$ obtained by feeding the ROBP with pseudorandom bits is “close” to the final matrix $M^r$ obtained by feeding with uniformly random bits.&lt;/p&gt;

&lt;p&gt;With this in mind we now present some techniques we need in order to analyze Nisan’s PRG (which we will present later).&lt;/p&gt;

&lt;h2 id=&quot;2--preliminaries&quot;&gt;2. &lt;a name=&quot;pre&quot;&gt;&lt;/a&gt; Preliminaries&lt;/h2&gt;

&lt;p&gt;For convenience, we set $d = 2^s$. We then introduce more notations. Let $[d]$ be the set of all nodes in a layer and &lt;script type=&quot;math/tex&quot;&gt;Q: [d]\times \{0,1\}^m\rightarrow [d]&lt;/script&gt; be the transition function for two consecutive layers’s computation. &lt;script type=&quot;math/tex&quot;&gt;Q^r:[d]\times \{0,1\}^{m\cdot r}\rightarrow [d]&lt;/script&gt; is the transition function for $r$ consecutive layers. Then we use $Q[x]$ to represent the behavior of $Q$ on &lt;script type=&quot;math/tex&quot;&gt;x \in \{0,1\}^m&lt;/script&gt;, and the corresponding matrix is denoted as $M[x]$.&lt;/p&gt;

&lt;p&gt;Let $Q_{i,j}$ be the subset of strings in &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^m&lt;/script&gt; that maps node $i$ to node $j$ in the next layer. Similarly, let $Q^r_{i,j}$ be the subset of strings in &lt;script type=&quot;math/tex&quot;&gt;\{0,1\}^{m\cdot r}&lt;/script&gt; which can be viewed as paths from node $i$ in the first layer to node $j$ in the last layer. Clearly, $M^r_{i,j}=\frac{\lvert Q^r_{i,j} \rvert}{2^{mr}}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.1&lt;/strong&gt; (Matrix Norm) Let $M$ be a $d \times d$ matrix, we use the matrix norm &lt;script type=&quot;math/tex&quot;&gt;\lvert \lvert M\rvert \rvert&lt;/script&gt; which is the largest row sum of $M$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert \lvert M\rvert \rvert = \max_{i} \sum_j \lvert M_{i, j} \rvert&lt;/script&gt;

&lt;p&gt;The following are some standard facts:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition 2.2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For matrices $M, N \in R^{d}\times R^d$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert \lvert M+N\rvert \rvert \leq \lvert \lvert M\rvert \rvert  + \lvert \lvert N\rvert \rvert&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert \lvert MN\rvert \rvert \leq \lvert \lvert M\rvert \rvert  \cdot  \lvert \lvert N\rvert \rvert&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proposition 2.3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If $M, M^\prime, N, N^\prime$ are substochastic matrices, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert\lvert MM^\prime - NN^\prime \rvert\rvert \leq \lvert\lvert M-N\rvert\rvert + \lvert\lvert M^\prime- N^\prime\rvert\rvert&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.4&lt;/strong&gt; (Pairwise Independent Hashing Family) We call a family of hashing functions &lt;script type=&quot;math/tex&quot;&gt;H:\{0,1\}^m\rightarrow \{0,1\}^n&lt;/script&gt; is pairwise independent if for any &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2 \in \{0,1\}^m&lt;/script&gt; and $x_1 \neq x_2$, &lt;script type=&quot;math/tex&quot;&gt;y_1, y_2\in \{0,1\}^n&lt;/script&gt; , when &lt;script type=&quot;math/tex&quot;&gt;h:\{0,1\}^m\rightarrow \{0,1\}^n&lt;/script&gt; is chosen randomly from $H$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{h\in H}\left [\ h(x_1) = y_1 \land h(x_2) = y_2 \right ] = \frac{1}{2^{2n}}&lt;/script&gt;

&lt;p&gt;An explicit construction of such hash family is known to exist and can be encoded using $2m$ bits. Moreover, it can be computed using $O(m)$ space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 2.5&lt;/strong&gt; Let $A, B\subseteq {0,1}^m$, $h:{0,1}^m \rightarrow {0,1}^m$. We say a hash function is &lt;em&gt;$\epsilon$-independent for $A$, $B$&lt;/em&gt; if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left \lvert \Pr_x \left[ x\in A \land h(x)\in B \right] - \frac{\lvert A\rvert \lvert B \rvert}{2^{2m}} \right \rvert &lt; \epsilon %]]&gt;&lt;/script&gt;

&lt;p&gt;From now on we denote $\frac{\lvert A\rvert}{2^m}$ as $\alpha$ and $\frac{\lvert B\rvert}{2^m}$ as $\beta$.&lt;/p&gt;

&lt;p&gt;We now prove a mixing lemma which roughtly says that the functions from a pairwise independent hashing family are almost all $\epsilon$-independent.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2.6&lt;/strong&gt; (Mixing Lemma) Fix any $A$ ,$B$, if &lt;script type=&quot;math/tex&quot;&gt;h:\{0,1\}^m\rightarrow \{0,1\}^m&lt;/script&gt; is chosen randomly from a pairwise independent hashing family $H$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{h\in H} \left[ \text{h is not } \epsilon \text{-independent} \right]\leq \frac{\alpha\beta/\epsilon^2}{2^m}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For any $h$, let $X_i$ be the indicator variable that $h(x_i)\in B$, and $X= \sum_{i\in A}X_i$. $h$ is  $\epsilon$-independent iff &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\lvert\frac{X}{2^m}-\alpha\beta\rvert &lt; \epsilon %]]&gt;&lt;/script&gt;. Consider the expectation $E[\frac{X}{2^m}] = \frac{1}{2^m}E[X]$.  By linearity of expectation, $ E[\frac{X}{2^m}] = \frac{1}{2^m}\sum_{i\in A} E[X_i] =  \frac{1}{2^m}\cdot \beta \lvert A\rvert = \alpha\beta$. Since $X_i$’s are pairwise independent, by Chebyshev inequality we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr\left[ \lvert\frac{X}{2^m}-\alpha\beta\rvert \geq \epsilon \right ]\leq \frac{Var[X]}{2^{2m}\cdot \epsilon^2} \leq\frac{\sum_{i\in A}Var[X_i]}{2^{2m}\cdot \epsilon^2} \leq \frac{\lvert A\rvert\cdot \beta(1-\beta)}{2^{2m}\cdot \epsilon^2}\leq \frac{\alpha\beta}{2^m\cdot \epsilon^2}&lt;/script&gt;

&lt;h2 id=&quot;-3-nisans-pseudorandom-generator&quot;&gt;&lt;a name=&quot;prg&quot;&gt;&lt;/a&gt; 3. Nisan’s Pseudorandom Generator&lt;/h2&gt;

&lt;p&gt;Nisan’s generator is constructed by using random hash functions from pairwise independent hash family. &lt;br /&gt;
Consider the matrix $M^2[x_1,x_2]$ of $Q^2[x_1, x_2]$ where $x_1x_2$ is chosen uniformly at random, we first show that if we choose $h_1$ randomly from $H$, then $M^2(x_1, h_1(x_1)$ of is close to $M^2(x_1,x_2)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 3.1&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr \left[ \lvert\lvert M^2[x_1, h_1(x_1)] -M^2[x_1,x_2]\rvert\rvert \geq \epsilon \right ] \leq \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Consider the $(i, j)$-th entry of $M^2[x_1, h_1(x_1)] -M^2(x_1,x_2)$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \left\lvert M^2[x_1, h_1(x_1)]_{i,j} -M^2[x_1,x_2]_{i,j}\right\rvert &amp;= \lvert \sum_p (M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - M[x_1]_{i,p}\cdot M[x_2]_{p, j})\rvert\\
  &amp;\leq \sum_p \left\lvert M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - \alpha\beta \right\rvert\\
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Inparticular, if $h$ is $\epsilon/d^2$-independent for all $Q_{i,p}$ and $Q_{p, j}$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\sum_p \left\lvert M[x_1]_{i,p}\cdot M[h(x_1)]_{p, j} - \alpha\beta \right\rvert &lt; \epsilon/d %]]&gt;&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\lvert\lvert M^2[x_1, h_1(x_1)] -M^2[x_1,x_2]\rvert\rvert &lt;\epsilon %]]&gt;&lt;/script&gt;

&lt;p&gt;With Lemma 2.6, if we want this holds for all triples $(i, p, j)$, a union bound gives us that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr \left[ \lvert\lvert M^2[x_1, h_1(x_1)] -M^2(x_1,x_2)\rvert\rvert \geq \epsilon \right ] \leq \frac{\alpha\beta\cdot d^4 \cdot d^3}{2^m \epsilon^2} = \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}&lt;/script&gt;

&lt;p&gt;Now we have a lemma for the case $r=2$, we can do it by repeatedly squaring to prove a lemma for the case $r$. For the next steps, every time we pick a new hash function from the above family, we will generate our pseudorandom bits by compositing these picked hash functions. For example, when we pick $h_2$, then new pseudorandom bits are generated by concatenating $h_2(x)$ and $h_2(h_1(x))$. From now on we will denote the new matrix as $M^{2^r}[x; h_1,..\ldots h_r]$ and the matrix we feed with uniformly random bits $x_1, \ldots x_{2^r}$ is denoted simply as $M^{2^r}$. Let’s do some more steps:&lt;/p&gt;

&lt;p&gt;when $r=4$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \lvert\lvert M^4[x, h_1, h_2]_{i,j} -M^4_{i,j}\rvert\rvert &amp;= \lvert\lvert M^4[x, h_1, h_2]_{i,j}-(M^2[x, h_1])^2 + (M^2[x, h_1])^2 - M^4\rvert\rvert\\
  &amp;\leq \lvert\lvert M^4[x, h_1, h_2]_{i,j}-(M^2[x, h_1])^2\rvert\rvert + \lvert\lvert (M^2[x, h_1])^2 - M^4\rvert\rvert\\
  &amp;\leq \epsilon + 2\cdot \epsilon\\
  &amp;\leq 3\epsilon
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;when $r=8$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  \lvert\lvert M^8[x, h_1, h_2, h_3]_{i,j} -M^8_{i,j}\rvert\rvert &amp;= \lvert\lvert M^8[x, h_1, h_2, h_3]_{i,j}-(M^4[x, h_1, h_2])^2 + (M^4[x, h_1, h_2])^2 - M^8\rvert\rvert\\
  &amp;\leq \epsilon + 2\cdot 3\epsilon\\
  &amp;\leq 7\epsilon
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The recursive relation for error is $\epsilon(r)=\epsilon + 2\cdot \epsilon(\log r)$.  In general, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon(r) = (r-1)\epsilon&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3.2&lt;/strong&gt; If we pick $h_1,\ldots ,h_{\log r}$ uniformly random from the pairwise independent hash family, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{h_1,\ldots ,h_{\log r}}\left[\lvert\lvert M^r[h_1,\ldots ,h_{\log r}] - M^r\rvert\rvert \geq (r-1)\epsilon \right]\leq (\log r) \cdot \frac{\alpha\beta\cdot d^7}{2^m \epsilon^2}&lt;/script&gt;

&lt;p&gt;With these we’re ready to present Nisan’s pseudorandom generator.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 3.3&lt;/strong&gt; (&lt;em&gt;Pseudorandom Generator for Matirx Exponentiation&lt;/em&gt;) A pseudorandom generator $G$ is $\epsilon$-pseudorandom for $M^r$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert M^r[G(x)]_{i,j} - M^r[x_1 \ldots x_r]_{i,j}\rvert \leq \epsilon&lt;/script&gt;

&lt;p&gt;Nisan’s generator is thus constructed by compositing these randomly chosen hash functions as we described above. For &lt;script type=&quot;math/tex&quot;&gt;x\in \{0,1\}^m&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{h_1,\ldots ,h_{\log r}}(x) = x,h_1(x), h_2(x), h_2(h_1(x)), \ldots, h_{\log r}(x),\ldots h_{\log r}(h_{\log r-1}(\ldots h_1(x)))&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3.4&lt;/strong&gt; (Nisan) For $d, m$ and $r&amp;lt;d$, there is a pseudorandom generator $G$ that is $1/d$-pseudorandom for $M^r$. Moreover, the generator takes a seed of length $O(m\log r)$ and produces a sequence of pseudorandom bits block by block for a total length of $m\cdot r$. Finally, the outputs can be computed in space $O(m+\log r)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Theorem 3.2, let’s set $\epsilon = 1/2dr$ and $m = \Theta(\log d) = C\log d$ for some large $C$. Therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lvert\lvert M^r[h_1,\ldots ,h_{\log r}] - M^r\rvert\rvert \leq 1/2d&lt;/script&gt;

&lt;p&gt;except with probability at most $\frac{d^9 r^2 \log r}{d^C} = O(1/d)$ for large $C$. While in the later case, the difference is bounded to be at most $1\cdot O(1/d) = 1/2d$.&lt;/p&gt;

&lt;p&gt;Finally, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left \lvert\Pr_{x, h_1, \ldots ,h_{\log r}}\left[ Q(i; G(x)) = j\right ]- \Pr_{x_1, \ldots , x_{2^r}}\left[ Q(i; x_1, \ldots , x^{2^r}) = j\right ]\right \lvert \leq 1/d&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>BPL and Fooling BPL</title>
   <link href="http://localhost:4000/derandbpl/"/>
   <updated>2019-11-03T00:00:00+08:00</updated>
   <id>http://localhost:4000/derandlogspace</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#bpl&quot;&gt;Log-space computation and BPL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#robp&quot;&gt;ROBP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fool&quot;&gt;Fooling BPL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Starting from this post I will discuss about the $BPL\ vs.\ L$ problem and write notes about some known, important result. This post is an introduction to the model.&lt;/p&gt;

&lt;h2 id=&quot;-1-log-space-computation-and-the-class-bpl&quot;&gt;&lt;a name=&quot;bpl&quot;&gt;&lt;/a&gt; 1. Log-space Computation and the class BPL&lt;/h2&gt;

&lt;p&gt;We are interested in space-bounded probabilistic computation, specifically, on Turing machines that have logarithmic working space. We start by describing such machines.&lt;/p&gt;
&lt;h3 id=&quot;log-space-probabilistic-turing-machine&quot;&gt;Log-space Probabilistic Turing Machine&lt;/h3&gt;

&lt;p&gt;A log-space probabilistic turing machine(PTM) consists of the following parts:  a two-way read-only input tape where we assume that the input is of size $n$; a normal read-write working tape of size $O(\log n)$, a finite states control that has a finite number of states, a output tape that is write-only. In addition, the machine has access to infinite randomness. That is, the machine is allowed to flip coins to decide the next step. We model this by giving it access to an infinite one-way random tape, it reads the next random bit when it is needed. Note that it must write the random bit down (on the work tape) if it wants to retrieve it later.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/L-PTM.jpg&quot; alt=&quot;log-space PTM&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the execution of this TM, a &lt;em&gt;configuration&lt;/em&gt; consists of the position of read head on the input tape plus the contents on it, the position of read-write head on the working tape plus the contents, the specific state on the finite states control. We should see that the total number of configurations is $O(\log n) \cdot 2^{O(\log n)}$(# of possible work tape configuration) $\cdot \lvert \Delta \rvert$(# of states which is finite) $\cdot n$ (# of possible input tape configuration) $=2^{O(\log n)}$.&lt;/p&gt;

&lt;h3 id=&quot;bpl&quot;&gt;BPL&lt;/h3&gt;

&lt;p&gt;$BPL$ can be written as $BP_HL$ or $BP_HSPACE(\log n)$. It refers to &lt;em&gt;Bounded-error Probabilistic Log-space that halts absolutely&lt;/em&gt; (hence the sub “H”). Formally, we say a language $L$ is in $BPL$ if there is a log-space PTM recognizes it and satisfies the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;it always halts.&lt;/li&gt;
  &lt;li&gt;if $x\in L$, it accepts with probability at least 2/3.&lt;/li&gt;
  &lt;li&gt;if $x\not\in L$, it rejects with probability at least 2/3.&lt;/li&gt;
  &lt;li&gt;the probability is taken over the coin flips of the PTM.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We emphasize that in the definition of $BPL$ the PTM halts absolutely, which is the case we’re interested in. For space-bounded computation there are other PTMs(e.g. PTM that halts almost surely) which we will not discuss here. Interested reader should refer to the survey [1]. We now state some observations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The PTM runs in time at most $2^{O(\log n)}$, which is the total number of configurations. Otherwise the PTM falls in loop and may never halt.&lt;/li&gt;
  &lt;li&gt;Since it runs in time at most $2^{O(\log n)}$, it uses at most $2^{O(\log n)}$ random bits.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In fact, restricting a PTM to use at most $2^{O(\log n)}$ randomness is “equivalent” to it halts absolutely. Consider the following: the PTM uses space $O(\log n)$ to maintain a counter, every time the PTM flips a coin, the counter is reset to 0, if the counter reaches $2^{O(\log n)}$, then it halts and rejects. It can be seen that this halting PTM recognizes the same language.&lt;/p&gt;

&lt;h2 id=&quot;-2-read-once-branching-program&quot;&gt;&lt;a name=&quot;robp&quot;&gt;&lt;/a&gt; 2. Read-once Branching Program&lt;/h2&gt;

&lt;p&gt;In this section we introduce a (non-uniform) computation model that simulates space-bounded computation. A branching program (in our interest) is a layed acyclic graph, each layer has a total number of $2^{O(\log n)}$ nodes. For our purpose, you should think of each of these nodes as a configuration in the execution of the above BPL machine on a given input, so each layer has the same nodes and, since it’s halting, the same node won’t be entered twice. For each input, there is a starting node in the first layer that represents the starting configuration. In the last layer there are only 2 nodes, one represents the “accept” state and the other represents the “reject” state. Between the $i$th and $i+1$th layer there are transition edges mapping from nodes in the $i$th layer to the nodes in the $i+1$th layer. Each of these edges is labeled a string &lt;script type=&quot;math/tex&quot;&gt;r_i \in\{0,1\}^m&lt;/script&gt;. You should think of this string as the $m$ coin flips tossed by the machine to decide which configuration to enter in the next step, and it’s an exercise to see that this is equivalent to repeating $m$ times the process of tossing one coin then perform some deterministic computation . A computation on a branching program with input $r_n$ is just a path from a node in the first layer to the accepting/rejecting node in the last layer. Among the intermediate layers it reads $r_i$s and chooses the edges to traverse layer by layer. We say a branching program is a ROBP (Read Once Branching Program) if it reads its input in a one-way fashion.&lt;/p&gt;

&lt;p&gt;For a computation on the BPL machine with input $x$, you can think of a ROBP simulates the computation by:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;pick a node in the first layer as starting node according to the input $x$;&lt;/li&gt;
  &lt;li&gt;read the sequence of random bits (at most $2^{O(\log n)}$) on the original random tape as input and traverses the graph accordingly.&lt;/li&gt;
  &lt;li&gt;note that it reads the input in a streaming (one-way) fashion.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The picture below represents a branching program using $r_n$ randomness and has $2^{O(\log n)}$ nodes in each layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ROBP1.jpg&quot; alt=&quot;ROBP&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For convenience, we denote the &lt;em&gt;width&lt;/em&gt; of the (RO)BP as the maximum number of nodes in a layer of it. We denote the &lt;em&gt;length&lt;/em&gt; of the (RO)BP as the number of layers in it.&lt;/p&gt;

&lt;h2 id=&quot;-3-fooling-bpl&quot;&gt;&lt;a name=&quot;fool&quot;&gt;&lt;/a&gt; 3. Fooling BPL&lt;/h2&gt;

&lt;p&gt;From the previous sections, we can see that to fool every language in BPL, it suffices to fool all ROBPs that have width at most $2^{O(\log n)}$ and length at most $2^{O(\log n)}$. We formally define what we mean by fooling a ROBP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 3.1&lt;/strong&gt; For a ROBP &lt;script type=&quot;math/tex&quot;&gt;Q: \{0,1\}^{n} \rightarrow \{0,1\}&lt;/script&gt; and $r&amp;lt;n = 2^{O(\log n)}$, we say a pseudorandom generator &lt;script type=&quot;math/tex&quot;&gt;G: \{0,1\}^{r} \rightarrow \{0,1\}^{n}&lt;/script&gt; $\epsilon$-fools $B$ if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\lvert \Pr \left[ Q(G(U_{r}))=1\right] - \Pr\left[Q(U_n)=1 \right]\rvert &lt; \epsilon %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] Michael Saks, “Randomization and Derandomization in Space-Bounded
Computation” http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.1185&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Goldreich Levin Theorem</title>
   <link href="http://localhost:4000/glthm/"/>
   <updated>2019-10-04T00:00:00+08:00</updated>
   <id>http://localhost:4000/glthm</id>
   <content type="html">&lt;p&gt;In this blog we study and present the proof of Goldreich-Levin theorem. The main material we use is the  course notes of Prof. Luca Trevisan (see references at the end).&lt;/p&gt;

&lt;p&gt;The Goldreich-Levin Theorem has several interpretations. We will first explain and prove it from cryptographic view. Then we will see in fact it can also be viewed as list-decoding of Hadamard code.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;In this section we present necessary definitions and math inequalities we will be using. The first theorem is a variant of Markov inequality.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1 (Markov Inequality, Variant)&lt;/strong&gt; Suppose $X$ is a random variable in $[0, 1]$ and $0&amp;lt;t&amp;lt;E[X]$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr[X\geq t] \geq \frac{E[X]-t}{1-t}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let $S$ denote the set of $x$ such that $X(x)\geq t$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} 
E[X] &amp;= \sum_{x\in S}Pr(x)X(x) + \sum_{x\not\in S}Pr(x)X(x)\\ 
 &amp;\leq \sum_{x\in S}Pr(x)\cdot 1 + \sum_{x\not\in S}Pr(x)\cdot t\\
 &amp;= Pr(S) + t\cdot (1-Pr(S))\\
 &amp;= (1-t)\cdot Pr(S) + t
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2 (Chernoff Bound)&lt;/strong&gt;  Suppose $X_1, \ldots, X_k$ are $0, 1$ &lt;em&gt;i.i.d&lt;/em&gt; random variables and $ X = \sum_i^{k} X_i$, then for any $0&amp;lt;\epsilon&amp;lt;1$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Pr[\ X&gt;(1+\epsilon)E[X]\ ]&lt;e^{-\frac{\epsilon^2}{3}\cdot E[X]} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Pr[\ X&lt;(1-\epsilon)E[X]\ ]&lt;e^{-\frac{\epsilon^2}{3}\cdot E[X]} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3 (Chebyshev Inequality)&lt;/strong&gt;&lt;br /&gt;
Suppose $X= X_1 + \cdots + X_k$ where $X_1, \ldots ,X_k$ are $0, 1$ pairwise independent variables and $t&amp;gt;0$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr[\mid X-E[X]\mid \geq t] \leq \frac{Var(X_1) + \cdots + Var(X_k)}{t^2}&lt;/script&gt;

&lt;p&gt;In particular, if for each $X_i$, $Pr[X_i = 1] \geq 1/2 + \epsilon$, then $Var(X_i)&amp;lt;\frac{1}{4}$, therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr[\mid X-E[X]\mid \geq t] \leq \frac{Var(X_1) + \cdots + Var(X_k)}{t^2}\leq \frac{k}{4t^2}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let $S$ be the set of $x$ such that $\mid X(x)-E[X]\mid \geq t$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} 
Var[X] &amp;= \sum_x (X(x)-E[X])^2 \cdot Pr(x)\\
 &amp;= \sum_{x\in S}(X(x)-E[X])^2 \cdot Pr(x) + \sum_{x\not\in S}(X(x)-E[X])^2 \cdot Pr(x)\\
 &amp;\geq Pr(S)\cdot t^2 + 0
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr(S) \leq \frac{Var[X]}{t^2}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} 
Var[X_1+X_2] &amp;= E[(X_1+X_2)^2] - E(X_1+X_2)^2\\
&amp;= E[X_1^2] + 2E[X_1X_2] + E[X_2]^2 - E[X_1]^2 -2E[X_1]E[X_2] - E[X_2]^2\\
&amp;= Var[X_1]+Var[X_2]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;this can be easily generalized to show $Var[X_1 + \cdots + X_k]= Var[X_1] + \cdots + Var[X_k]$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 1 (Hard-Core Predicate)&lt;/strong&gt; A boolean function &lt;script type=&quot;math/tex&quot;&gt;P: \{0, 1 \}^n \rightarrow \{0, 1\}&lt;/script&gt; is $(t, \epsilon)$ - hard core for a permutation &lt;script type=&quot;math/tex&quot;&gt;f: \{0,1\}^n \rightarrow \{0,1\}^n&lt;/script&gt; if for every algorithm $A$ of complexity $t$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{x\sim \{0, 1\}^n}[A(f(x)) = P(x)]\leq \frac{1}{2}+\epsilon&lt;/script&gt;

&lt;p&gt;Note: we use &lt;script type=&quot;math/tex&quot;&gt;x\sim \{0, 1\}^n&lt;/script&gt; to denote uniform distribution over &lt;script type=&quot;math/tex&quot;&gt;\{0, 1\}^n&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;goldreich-levin-theorem&quot;&gt;Goldreich-Levin Theorem&lt;/h2&gt;

&lt;p&gt;We denote inner product modulo 2 using the following notation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle x,r\rangle = \sum_i x_i\cdot r_i\ mod\ 2&lt;/script&gt;

&lt;p&gt;The Goldreich-Levin theorem says that a random XOR is hard-core for every one-way permutation. This means that if there is an efficient algorithm to predict $\langle x, r\rangle$, given $f(x)$ and $r$, then there is also an efficient algorithm to compute a pre-image of $f(x)$, so if $f(x)$ is a one-way function, we can invert it with noticeable probability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1 (Goldreich-Levin Theorem)&lt;/strong&gt; &lt;em&gt;Suppose $A$ is an algorithm of complexity $t$ such that&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle] \geq \frac{1}{2} + \epsilon&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Then there is an algorithm $A^\prime$ of complexity at most $O(t\epsilon^{-2}n^{O(1)})$ such that&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_x[A^\prime(f(x)) = x] \geq \Omega(\epsilon)&lt;/script&gt;

&lt;p&gt;We will first prove a weak Goldreich-Levin algorithm, which will later be used in our proof of GL theorem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2 (Goldreich-Levin Algorithm Weak Version)&lt;/strong&gt; &lt;em&gt;Suppose there is a function $H$ such that, for some $x$&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_r[H(r)\ =\ \langle x,r\rangle]\ \geq \frac{3}{4} + \epsilon&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Then there is an algorithm $GLW$ that can output $x$ with high probability ( $\geq 1-\frac{1}{n}$ ).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let’s first see how an easy algorithm can find $x$ when the probability on the right side is 1 in the above (so that $H(r)$ outputs $\langle x,r\rangle$ correctly).&lt;/p&gt;

&lt;p&gt;Let $e_i$ denote the vector in which the $i$th bit is 1 and 0 otherwise. $x_i = H(e_i) = \langle x, e_i \rangle$. Then we can output $x$ by enumerating all the $e_i$.&lt;/p&gt;

&lt;p&gt;Now to prove Theorem 2, note that $H(r)$ fails with small probability, we’ll use the majority vote method. Intuitively,  we randomly sample several points $r_1 \ldots r_k$. Let $H^\prime_{r_1 \ldots r_k}(e_i)$ be the function that take the majority of  $H(r_j+e_i) - H(r_j)$ on these $r_j$s. The observation is that if $H(r)$ computes $\langle x, r\rangle$ correctly, then we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle x, r_j+e_i \rangle - \langle x, r_j \rangle = \langle x, e_i\rangle&lt;/script&gt;

&lt;p&gt;We now bound the probability that $H(r_j + e_i) - H(r_j)$ fails.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  Pr[H(r_j + e_i) - H(r_i) \neq  \langle x, e_i\rangle] &amp;= Pr[H(r_j + e_i) \neq \langle x, r_j+e_i \rangle\ \cup H(r_j) \neq \langle x, r_j \rangle]\\
  &amp;\leq Pr[H(r_j + e_i) \neq \langle x, r_j+e_i \rangle]\ + Pr[H(r_j) \neq \langle x, r_j \rangle]\\
  &amp;\leq \frac{1}{4} - \epsilon + \frac{1}{4} - \epsilon\\
  &amp;\leq \frac{1}{2} - 2\epsilon
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The GLW algorithm is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GLW Algorithm:&lt;/li&gt;
  &lt;li&gt;for $i= 1, 2, \ldots , n$:
    &lt;ul&gt;
      &lt;li&gt;for $j = 1, 2, \ldots , 4\log n/ \epsilon^2$:
        &lt;ul&gt;
          &lt;li&gt;randomly sample &lt;script type=&quot;math/tex&quot;&gt;r_j\in \{0, 1\}^n&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;compute $H(r_j + e_i) - H(r_j)$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;compute &lt;script type=&quot;math/tex&quot;&gt;x_i = H^\prime_{r_1 \ldots r_k}(e_i) = Majority\{H(r_j + e_i) - H(r_j): j = 1, 2, \ldots , 4\log n/ \epsilon^2 \}&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return $x$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now we analyze this algorithm. Since each $H(r_j + e_i) - H(r_j)$ outputs the correct answer with probability at least $1/2 + 2\epsilon$. By the Chernoff Bound (Theorem 2 in Background), the probability that the majority function $H^\prime_{r_1 \ldots r_k}(e_i)$ fails to output $x_i$ is at most $e^{-2\log n} = O(\frac{1}{n^2})$. Then the union bound gives us that GLW outputs $x$ correctly with probability at least $1-1/n$. The algorithm runs in time $O(\frac{n^2 \log n}{\epsilon^2})$, makes $O(\frac{n\log n}{\epsilon^2})$ queries to $H$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion.&lt;/strong&gt; The above framework only works for cases where $\Pr_r[H(r) = \langle x, r \rangle]$ greater than $\frac{3}{4}$. For the case in Goldreich-Levin theorem where $H(r)$ makes more error,the union bound gives probability less than $1/2$ and the Chernoff Bound won’t work. In fact, it is possible to construct a function $H$ such that $Pr[H(r) = \langle x, r\rangle] = 3/4$ and $Pr[H(r) = \langle x^\prime, r\rangle] = 3/4$ where $x \neq x^\prime$. So no algorithm can guarantee to find the correct $x$ when given such $H$, since $x$ is not uniquely defined by $H$. However, the Goldreich-Levin theorem tells us how to find a list of possible candidates of $x$ with good probability. We present their algorithm below.&lt;/p&gt;

&lt;p&gt;We proceed to prove the Goldreich-Levin Theorem.&lt;/p&gt;

&lt;p&gt;Given (as in the Goldreich-Levin theorem) $\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle] \geq \frac{1}{2} + \epsilon$, we first show that there is a certain fraction of $x$,  $\Pr_r[A(f(x), r) = \langle x, r \rangle] \geq \frac{1}{2} + \frac{1}{2}\epsilon$ holds, and we call such $x$ “good” :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Pr_{x, r} [A(f(x),r)\ =\ \langle x ,r\rangle]&lt;/script&gt; can be viewed as &lt;script type=&quot;math/tex&quot;&gt;\sum_x Pr(x)\Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]&lt;/script&gt;,  which is equal to the expectation &lt;script type=&quot;math/tex&quot;&gt;E_x[\ \Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]\ ]&lt;/script&gt; ( so we view &lt;script type=&quot;math/tex&quot;&gt;\Pr_r[A(f(x),r)\ =\ \langle x ,r\rangle]&lt;/script&gt; as a random variable of $x$). Now we have the expectation is greater than $1/2 + \epsilon$, by the Markov ineuality (Theorem 1 in background) we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_x \left[\ \Pr[A(f(x),r)\ =\ \langle x ,r\rangle ]\geq \frac{1}{2} + \frac{1}{2}\epsilon\ \right]\geq \frac{\frac{1}{2}\epsilon}{\frac{1}{2}-\frac{1}{2}\epsilon}\geq \frac{\epsilon}{2}&lt;/script&gt;

&lt;p&gt;So at least $\frac{\epsilon}{2}$ of $x$ are good. On these good $x$, we prove the GL algorithm:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3 (Goldreich-Levin Algorithm)&lt;/strong&gt;
&lt;em&gt;Suppose there is a function $H$ such that, for some $x$:&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_r[H(r) = \langle x, r \rangle] \geq \frac{1}{2} + \epsilon&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Then there is an algorithm GL outputs a list $L$ of candidates of $x$, such that $L$ is of size $O(\epsilon^{-2})$ and $x\in L$ with probability  at least $1/2$. The GL algorithm runs in time $O(n^2 \epsilon^{-4} \log n)$, makes $O(n\epsilon^{-4}\log n)$ queries to $H$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As discussed above, we now don’t have $H(r)$ that predicts $\langle x, r \rangle$ well, so $H(r+r_j) - H(r_j)$ fails to predict $\langle x, r \rangle$ with high probability ($ &amp;gt; 1/2$). However, let’s make an assumption that, instead of $H(r_j)$, we get the correct $\langle x, r_j \rangle$ for every $j$, so now $H(r+r_j) - \langle x, r_j \rangle$ succeeds with probability at least $1/2 + \epsilon$. We do an error reduction by taking $k= O(\frac{1}{\epsilon^2})$ samples of $r_j$, then apply the majority vote method. Thus we can reduce the probability to match that in Theorem 2 and apply the GLW algorithm to find $x$.   &lt;br /&gt;
Moreover, since we dont’ know the correct $\langle x, r_j \rangle$, we just enumerate all the possible values of it, which gives the following (inefficient) algorithm:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inefficient GL algorithm:&lt;/li&gt;
  &lt;li&gt;randomly sample &lt;script type=&quot;math/tex&quot;&gt;r_1, \ldots, r_k \in \{0, 1\}^n&lt;/script&gt; where $k =  O(\frac{1}{\epsilon^2})$&lt;/li&gt;
  &lt;li&gt;for &lt;script type=&quot;math/tex&quot;&gt;b_1, \ldots, b_k \in \{0, 1\}&lt;/script&gt; (each $b_j$ corresponds to a possible value of $\langle x, r_j \rangle$) :
    &lt;ul&gt;
      &lt;li&gt;let &lt;script type=&quot;math/tex&quot;&gt;H^\prime_{r_1 \ldots r_k}(r) :=  Majority\{H(r_j + r) - b_j: j = 1, 2, \ldots, k \}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;run GLW algorithm in Theorem 2, using $H^\prime_{r_1 \ldots r_k}(r)$ instead of $H(r)$, outputs $x$&lt;/li&gt;
      &lt;li&gt;add $x$ to list $L$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return $L$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Analysis:&lt;/em&gt;  By the Chernoff Bound we have, on the correct $b_j$’s,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{r,r_1,\ldots, r_k}[H^\prime_{r_1 \ldots r_k}(r) = \langle x, r\rangle]\geq 1-o(1)\geq 1-\frac{1}{32}&lt;/script&gt;

&lt;p&gt;It follow by the Markov Inequality that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr_{r_1, \ldots, r_k}[\Pr_r[H^\prime_{r_1 \ldots r_k}(r) = \langle x, r\rangle] &gt; \frac{3}{4}]&gt; \frac{31}{32}&lt;/script&gt;

&lt;p&gt;By the GLW algorithm, with probability at least $\frac{31}{32} - \frac{1}{n} &amp;gt; \frac{1}{2}$, $x$ is in the list.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion.&lt;/strong&gt; The above algorithm is inefficient in that the size of $L$ is $2^k$, which is exponential in $k$, the GL algorithm below reduces this using pairwise independence.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GL algorithm:&lt;/li&gt;
  &lt;li&gt;Randomly sample &lt;script type=&quot;math/tex&quot;&gt;r_1, \ldots, r_k \in \{0, 1\}^n&lt;/script&gt; where $k =  O(\log \frac{1}{\epsilon^2})$&lt;/li&gt;
  &lt;li&gt;for each subset &lt;script type=&quot;math/tex&quot;&gt;S\subseteq \{r_1, \ldots, r_k\}&lt;/script&gt;, define $r_S = \sum_{i\in S} r_i$&lt;/li&gt;
  &lt;li&gt;for all &lt;script type=&quot;math/tex&quot;&gt;b_1 \ldots, b_k \in \{0, 1\}&lt;/script&gt;:
    &lt;ul&gt;
      &lt;li&gt;define $b_S = \sum_{i\in S} b_i$&lt;/li&gt;
      &lt;li&gt;define &lt;script type=&quot;math/tex&quot;&gt;H^\prime(r) :=  Majority\{H(r_S + r) - b_S: S\subseteq \{r_1, \ldots, r_k\} \}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;run GLW algorithm in Theorem 2, using $H^\prime(r)$ instead of $H(r)$, outputs $x$&lt;/li&gt;
      &lt;li&gt;add $x$ to list $L$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;return $L$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now we analyze this GL algorithm. Since $r_1, \ldots, r_k$ are randomly picked, for any two different sets $S$ and $T$, $r_S$ and $r_T$ are pairwise independent, therefore $H(r_S+r) - b_S$ and $H(r_T+r) - b_T$ are pairwise independent. In addition, on the correct $b_S$, $H(r_S+r) - b_S = \langle x, r \rangle$ with probability at least $1/2 + \epsilon$. By the Chebyshev Ineuqality (Theorem 3 in background), $H^\prime(r)$ outputs the wrong value with probability at most $\frac{1}{4\epsilon^2 \cdot 2^{k}}$ where $k=O(\log \frac{8}{\epsilon^2})$, so the probability is at most $\frac{1}{32}$. Given such $H^\prime(r)$ the GLW algorithm will work well. &lt;br /&gt;
The running time of GL algorithm is $O(n^2 \log n \cdot \epsilon^{-4})$, it makes$O(n^2\log n\cdot \epsilon^{-4})$ queries to $H$ and outputs a list $L$ of size $O(\frac{1}{\epsilon^2})$. $x$ is in $L$ with probability greater than $1/2$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finalizing the proof.&lt;/strong&gt; Since we have at least $\epsilon/2$ fraction of good $x$, combinging this with the GL algorithm we can conclude that with probability at least $\epsilon /4$ $x$ is in the list $L$. We can check $x$ by evaluating $f(x)$ to see if it equals our input $y$.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;Lecture Notes 11, 12 of Luca Trevisan’s Spring 2009 Cryptography Courses, see this &lt;a href=&quot;https://people.eecs.berkeley.edu/~luca/cs276/#notes&quot;&gt;Link&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>差分隐私 Differential Privacy</title>
   <link href="http://localhost:4000/dpsurvey/"/>
   <updated>2019-08-16T00:00:00+08:00</updated>
   <id>http://localhost:4000/DPsurvey</id>
   <content type="html">&lt;p&gt;注：这篇文章的目的在于通过介绍问题背景与实例来使读者对差分隐私这一概念的建立直觉/初步的理解，并对它的数学定义进行讨论。由于时间关系，我们不对前沿的细节做深入的调研，只做简单地了解。&lt;/p&gt;

&lt;h2 id=&quot;一问题背景&quot;&gt;一、问题背景&lt;/h2&gt;

&lt;p&gt;在大数据时代，很多互联网公司将收集的用户使用信息或记录保存于数据库，并通过数据分析手段来提高用户的体验。 在这个过程中不可避免地就会出现隐私泄露的问题。数据集中可能包含有用户的医疗诊断信息，个人消费偏好等信息，个人的此类信息遭遇泄露的话，难免会被有心人利用并导致对个人不利的后果。&lt;/p&gt;

&lt;p&gt;一个比较典型的隐私泄露事件发生在06年的时候，Netflix（美国一家媒体服务提供商） 发起了一个有100万美金奖励的数据挖掘竞赛。 他们发布了一组关于用户浏览记录的数据集， 如果有人能通过这份数据集使他们的推荐系统的效能提高10%就可获得奖金。 当然，为了保护用户隐私，他们声称已经把包含用户ID的信息都删除以实现匿名化。 问题在于，这样是否真的已经将用户的信息隐藏起来？答案是否定的。来自 UT Austin 的 Arvind Narayanan 和 Vitaly Shmatikov 的工作[1]通过把已公布的数据库与 Internet Movie Database (IMDb) 结合起来，实现了一定程度反匿名化，暴露了用户的信息。&lt;/p&gt;

&lt;p&gt;可以举一个简单的例子来从直觉上解释怎么做到反匿名的过程: 假如有一个数据库 $A$，里面包含了一些用户的 性别，出生日期，邮政编码，或是在一段时间内看过某场电影，但没有姓名，身份证信息。而另外一个数据库 $B$ 里包含一些用户的姓名，性别，邮政编码，出生日期，医疗信息，住址。 这样，通过对比这两个公开的数据库（比如相同的邮编和出生日期），我们可以很容易地猜出 $B$ 中的哪些人对应 $A$ 中的哪些信息。这样的攻击我们称为&lt;strong&gt;链接攻击 (Linkage Attack)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;另外一个例子解释攻击者可以通过不同的询问方式来问出私人信息。攻击者第一次进行这样的提问：“在2001年，P大学的大一新生有多少人家庭年收入35万以上？” 他得到准确答案是200。他接着提问：“在2002年， P大学的大二学生有多少人家庭年收入在35万以上？” 这次他得到答案是199。 于是他通过另外一种方式得知一名叫张知秋的同学在大二时退学了（比如查询在校学生表格）。 这样他可以得出结论：张知秋同学的家庭年收入在35万以上。这样的攻击我们称为&lt;strong&gt;差分攻击 (Differencing Attack)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;差分隐私就是为了防止这样的攻击而提出，它的最终目的是使得数据分析者能对数据库进行&lt;strong&gt;隐私保护的数据分析(privacy-preserving data analysis)&lt;/strong&gt;。在差分隐私机制中，我们希望通过对数据库返回的值进行一定的处理，从而使得返回结果具有一定的随机性，但大致保持一定的精确度。这样，就能防止攻击者通过对比不同的查询结果来获取&lt;strong&gt;个体&lt;/strong&gt;的私人隐私。注意我们希望保护的是&lt;strong&gt;特定的某个个体&lt;/strong&gt;的隐私，也就是说，对于任何的个体，他的隐私都不被泄露。&lt;/p&gt;

&lt;h2 id=&quot;二差分隐私机制&quot;&gt;二、差分隐私机制&lt;/h2&gt;

&lt;p&gt;为了解决隐私泄露的问题，差分隐私的概念由Dwork等人提出。从直觉上介绍，一个差分隐私机制掌控有一个数据库，接收用户的查询要求，根据要求返回一个统计数值。比如可以是这个数据库里带有某个性质的人的人数。 但返回精确的数值必定会导致隐私泄露，所以在差分隐私机制中我们引入随机性，使得每次机制返回的值不相同，但大致保持一定的准确度，从而达到隐私保护的作用。&lt;/p&gt;

&lt;p&gt;回到第一部分最后一个例子中，假如对于攻击者的第一次询问的回答是：198。那么攻击者会有这样的结论：大概有200个人年收入在35万以上。假如对于攻击者第二次询问的回答是：201。同样地，攻击者得到的结论是：大概有200个人年收入在35万以上。这样一来，他就没有办法进行差分攻击来获得某个个体的隐私信息。&lt;/p&gt;

&lt;p&gt;具体地说，差分隐私要求对于分析者（或以上的攻击者）的提问，根据数据库$A$概率性地输出一个结果，这个结果在基于数据库包含某个用户$i$的情况和不包含$i$的情况下是相似的。 等价地说，假如有两个数据库 $A$ 和 $A^\prime$ ，其中$A$包含$i$的信息，$A^\prime$不包含$i$的信息，那么我们的差分隐私机制接收提问者的问题，然后分别基于这两个数据库的所输出的结果是大致相似的。更通俗地说，特定的某个$i$在不在分析考虑范围内对差分隐私机制所输出的结果影响不大。接下来我们严格定义这一概念。&lt;/p&gt;

&lt;h2 id=&quot;三定义&quot;&gt;三、定义&lt;/h2&gt;
&lt;p&gt;我们定义数据集 $D$ 和 $D^\prime$ （$D,D^\prime\in \chi^n$， $\chi$为数据空间)为相邻数据集如果它们只在某一个位置不同。 如果它们在$k$个位置不同，我们称它们间的距离为$k$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt; 我们设随机算法为 $M$，数据分析者的查询函数空间为 $Q$，$M$的输出空间为$Y$。我们说 $M$ 是 &lt;strong&gt;$\epsilon-$差分隐私&lt;/strong&gt;的，如果对于任意两个相邻数据集 $D$ 和 $D^\prime$，查询函数$q\in Q$，以及 $Y$ 的子集 $S$，$M$ 满足&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr[M(D)\in S]\leq e^\epsilon\cdot Pr[M(D^\prime)\in S]&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;讨论&lt;/strong&gt; 在定义两个输出相接近时，为什么不使用&lt;em&gt;统计距离（statistical distance）&lt;/em&gt;？ 也就是说，为什么不使用定义 $SD \left( M(D),M(D^\prime)\right)&amp;lt;\epsilon$？&lt;/p&gt;

&lt;p&gt;我们分两种情况讨论使用统计距离的不合理性 —&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;设 $\epsilon&amp;lt; \frac{1}{10n}$，那么对于原数据库 $D$ 以及任意不同的数据库 $D^\prime$（即使$D$与$D^\prime$之间的距离为$n$），$M(D)$ 与 $M(D^\prime)$ 的统计距离最多相差$n\cdot \frac{1}{10n}=\frac{1}{10}$。特别地，$M(D)$与$M(0^n)$的距离最多为$\frac{1}{10}$。 换句话说，基于任意两个数据集的分析结果差距都不大，那我们所收集的$D$的使用价值就不大。&lt;/li&gt;
  &lt;li&gt;设$\epsilon\geq\frac{1}{10n}$。考虑以下机制：以$\frac{1}{10}$的概率随机输出数据库的某行信息。可以验证这样的机制满足我们的定义。但这样的机制是不被允许的 （暴露了输出的那个&lt;strong&gt;个体&lt;/strong&gt;的信息）。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;四一个差分隐私机制的例子&quot;&gt;四、一个差分隐私机制的例子&lt;/h2&gt;

&lt;p&gt;我们考虑计数查询函数&lt;script type=&quot;math/tex&quot;&gt;q\in Q:\chi^n \rightarrow \{0, 1\}&lt;/script&gt; 并定义 $q(x) = \sum_i^{n} q(x_i)\ mod\ 2$， 考虑以下机制：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M(x,q) = q(x) + Lap(1/\epsilon)&lt;/script&gt;

&lt;p&gt;其中 $Lap(\lambda)$ 为拉普拉斯分布，其概率密度函数正比于$e^{-\mid y\mid /\lambda}$。我们证明$M$为&lt;strong&gt;$\epsilon-$差分隐私&lt;/strong&gt;的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;证明&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;可以得到$[M(q,x)=y]$的概率密度函数正比于$e^{-\epsilon\mid y-q(x)\mid}$。 同样地，$[M(q,x^\prime)=y]$正比于$e^{-\epsilon\mid y-q(x^\prime)\mid}$。 由于$q(x)$与$q(x^\prime)$最多相差为1，因此将他们的概率密度函数相除，剩余因数大小为$e^\epsilon$，刚好满足$\epsilon-$差分隐私的要求。&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;[1] Narayanan, Arvind, and Vitaly Shmatikov. “Robust de-anonymization of large sparse datasets.” Security and Privacy, 2008. SP 2008. IEEE Symposium on. IEEE, 2008.&lt;br /&gt;
[2] Dwork “Differential Privacy: A Survey of Results”. https://web.cs.ucdavis.edu/~franklin/ecs289/2010/dwork_2008.pdf&lt;br /&gt;
[3] Dwork and Roth “The Algorithmic Foundations
of Differential Privacy”. https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf&lt;br /&gt;
[4] Vadhan “The Complexity of Differential Privacy” http://privacytools.seas.harvard.edu/files/privacytools/files/complexityprivacy_1.pdf&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Zero-Knowledge Proofs:Definitions and Variants</title>
   <link href="http://localhost:4000/zkdefvar/"/>
   <updated>2019-08-14T00:00:00+08:00</updated>
   <id>http://localhost:4000/zkdefvar</id>
   <content type="html">&lt;p&gt;零知识证明是一种特殊的交互式证明方法（Interactive Proof）。我们知道，在交互式证明中， 我们要求证明或协议（Protocol）满足两个性质：完备性（Completeness）和 可靠性（Soundness），其中可靠性是为了防止恶意的（adversarial）证明者（Prover）。 一个恶意的证明者会想要欺骗验证者（Verifier）从而使验证者相信一个错误的命题，可靠性要求证明者只能有很小的概率能够欺骗验证者。而零知识证明在此之外增设了需满足的第三个性质–零知识性（Zero Knowledge）。不同于可靠性是为了针对恶意的证明者，零知识性是为了防止恶意的验证者获取一些他们想要的信息。&lt;/p&gt;

&lt;p&gt;简单地说，零知识证明要求只保证验证者能确信命题的正确性， 除此以外，在证明过程中验证者不能获取其他任何的信息。比如说，在图同构问题（Graph Isomorphism）中，对于两个同构的图（G, H）这样的输入（a yes instance），零知识证明要求在交互证明结束后验证者 &lt;strong&gt;只能得出 “G和H确实是同构的”&lt;/strong&gt; 这样一个结论，除此以外，他不能获取其他任何信息。比如说验证者不能得到一个排列（Permutation）- π， 使得对 G 的节点施加 π 后得到 H（i.e. π(G)=H）。&lt;/p&gt;

&lt;p&gt;那么应该怎么合理地正式表达（Formulate）这样的要求呢？我们采用以下的方法来阐述这样的要求：任何验证者能从零知识证明过程中获取的信息，都能被与验证者有相同的计算能力的模拟者（Simulator）通过计算得到。模拟者（Simulator）在零知识证明中是一个很重要的概念，根据对模拟者的要求不同，会衍生出一些变体（variants）的定义，这点会在之后介绍。接下来要正式定义零知识：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;定义1.&lt;/strong&gt;  对于一个协议 PV，我们用 View(PV(x)) 来表示在输入为x的情况下，按照 PV 来进行交互证明过程中，验证者视角所看到的信息。具体地说，View(PV(x)) 包含有：
1.整个证明过程中证明者P和验证者V之间来往的讯息
2.验证者V所使用的随机性（Randomness），或者说验证者的用来投掷并决定下一步的硬币（Coins）&lt;/p&gt;

&lt;p&gt;此外，用 [View(PV(x))] 来表示 View(PV(x)) 在P和V所用的随机性下的概率分布（distribution）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;定义2.&lt;/strong&gt;  对于一个语言 L，我们称一个交互式证明或协议 PV 是&lt;strong&gt;零知识&lt;/strong&gt;的，如果&lt;/p&gt;

&lt;div&gt;
$$\forall V^{\prime} \exists S\in PPT\ s.t\ \ \forall x\in L$$  
&lt;/div&gt;
&lt;div&gt;
$$[View(S(x))]\simeq [View(PV^{\prime}(x))]$$
&lt;/div&gt;

&lt;p&gt;也就是说，对于任何验证者，都存在一个模拟者（多项式时间的概率图灵机）使得对于（语言L内）任意的x作为输入时，模拟者与验证者所看到的变量的分布是（几乎）相同的。
更通俗地说， 对于任何验证者视角下交互证明过程中的变量的分布，该验证者都能自身生成（generate）这样的分布。&lt;/p&gt;

&lt;p&gt;需要补充说明的是，上面定义里的量词是针对所有的 V’。特别地，PV’不一定是 L 的交互式证明或协议。也就是说，在上面的定义里，V’的目标只是想尽办法从证明者P那里获取有用的信息。而如果我们要求 PV’ 是一个 L 的交互式证明或协议，那么我们称之为&lt;strong&gt;诚实验证者零知识&lt;/strong&gt;（Honest Verifier Zero Knowledge，i.e. HVZK）。&lt;/p&gt;

&lt;p&gt;另一方面，根据分布的“相同”的不同定义，零知识性质也可分类为：&lt;strong&gt;完全零知识&lt;/strong&gt;（Perfect Zero Knowledge），&lt;strong&gt;统计零知识&lt;/strong&gt;（Statistical Zero Knowledge), 以及 &lt;strong&gt;计算零知识&lt;/strong&gt;（Computational Zero Knowledge）。其中完全零知识是指验证者与模拟者的输出分布完全相等，统计零知识是指两者的输出分布统计上接近（Statistically close），而计算零知识就是说这两个输出分布在多项式时间内是不可区分的。&lt;/p&gt;

&lt;p&gt;在Oded的survey中，他还有提到通常还需要要考虑到敌方（adversary）有辅助信息（auxiliary information）的情况。在这种情况下的零知识，称之为&lt;strong&gt;辅助输入零知识&lt;/strong&gt;（auxiliary-input zero knowledge）。辅助输入零知识需要在定义2里用[View(S(x, z))]和[View((P V’(z))(x))] 来分别替代模拟者的输出分布和验证者的输出分布，其中z代表的是辅助信息。&lt;/p&gt;

&lt;p&gt;除了以上的变体外，Oded的survey里提到了从对模拟者的要求出发衍生的一些变体， 其中一个就是要求有一个&lt;strong&gt;通用模拟者&lt;/strong&gt;（Universal Simulator），这个模拟者拥有验证者B’的程序作为辅助输入。为了这个目的，我们需将定义2里的模拟者 S(x) 修改为 S(x, &amp;lt;B’&amp;gt;)，其中 &amp;lt;B’&amp;gt; 表示B’的程序(program)的描述。更进一步加强定义的话，我们还可以要求把验证者的程序作为一个黑箱（Black Box）或是神谕机（Oracle），称之为&lt;strong&gt;黑箱模拟&lt;/strong&gt;（Black-box Simulation）。似乎最初大家都相信把B’的程序当作输入和把B’的程序作为黑箱使用对于零知识来说是没有区别的，不过Boaz Barak [3] 在2001年的论文给出了它们的区别。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;参考文献(References)&lt;/strong&gt;&lt;br /&gt;
[1] Oded Goldreich. Zero-knowledge twenty years after its invention. Unpublished manuscript. 2002.&lt;br /&gt;
[2] Rafail Ostrovsky. Foundations of Cryptography Draft Lecture Notes.&lt;br /&gt;
[3] Boaz Barak. How to go beyond the black-box simulation barrier. FOCS 2011.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Argument of Correctness of Undo Implementation</title>
   <link href="http://localhost:4000/ProofOfUndo/"/>
   <updated>2019-08-12T00:00:00+08:00</updated>
   <id>http://localhost:4000/ProofOfUndo</id>
   <content type="html">&lt;p&gt;Recently I’ve implemented a simple project of text editor = =. One part of this project is to implement the undo functionality. In case the recuiter might be interested, I’d like to provide a proof here that my idea of implementation can correctly recover deleted characters in its original position, the potential problematic case won’t happen.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Undo is the operation that can undo the user’s last $n$ operation(inserting/deleting characters).&lt;/p&gt;

&lt;p&gt;The difficulty lies in how can one keep track of the original position of the characters deleted. Well, in my implementation, nothing else is needed: just remove the node(which contains a character) and put it into a stack.&lt;/p&gt;

&lt;p&gt;The data strucutre I used for text editor is a doubly linked list, each node has a value contains the specific character, and a &lt;strong&gt;prev&lt;/strong&gt; pointer to the previous character node and a &lt;strong&gt;next&lt;/strong&gt; pointer to the next character node. Visualized as following:&lt;/p&gt;

&lt;div&gt;$$a\leftarrow s \rightarrow b$$  &lt;/div&gt;

&lt;p&gt;To recover the deleted node $s$, simply find its (relative) position by referring to its previous or next node, then insert it in.  While one thing might be problematic: &lt;strong&gt;what if when (the deleted) $s$ is to recover, its previous nodes a or b are not in the data structure, i.e. a and b haven’t been recovered&lt;/strong&gt;. Therefore we can’t find its position. While I argue that this will not happen.&lt;/p&gt;

&lt;h2 id=&quot;the-argument&quot;&gt;The Argument&lt;/h2&gt;
&lt;p&gt;I’d like to show the following: &lt;strong&gt;when $s$ is to recover, its previous or next nodes must exist in the data structure&lt;/strong&gt;, which is a simple argument by contradiction:&lt;br /&gt;
Suppose for the contradiction that $a$ and $b$ are not in the data structure, then $a$ and $b$ must be deleted before $s$ is deleted. However, in this case, by the time $s$ is deleted, its &lt;strong&gt;prev&lt;/strong&gt; and &lt;strong&gt;next&lt;/strong&gt; cannot be $a$ and $b$, which is a contradiction.&lt;/p&gt;

</content>
 </entry>
 

</feed>
